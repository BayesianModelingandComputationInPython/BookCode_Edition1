
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8. Approximate Bayesian Computation &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9. End to End Bayesian Workflows" href="chp_09.html" />
    <link rel="prev" title="7. Bayesian Additive Regression Trees" href="chp_07.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_08.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#life-beyond-likelihood">
   8.1. Life Beyond Likelihood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approximating-the-approximated-posterior">
   8.2. Approximating the Approximated Posterior
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-a-gaussian-the-abc-way">
   8.3. Fitting a Gaussian the ABC-way
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-the-distance-function-epsilon-and-the-summary-statistics">
   8.4. Choosing the Distance Function,
   <span class="math notranslate nohighlight">
    \(\epsilon\)
   </span>
   and the Summary Statistics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-the-distance">
     8.4.1. Choosing the Distance
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-epsilon">
     8.4.2. Choosing
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choosing-summary-statistics">
     8.4.3. Choosing Summary Statistics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#g-and-k-distribution">
   8.5. g-and-k Distribution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#approximating-moving-averages">
   8.6. Approximating Moving Averages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-comparison-in-the-abc-context">
   8.7. Model Comparison in the ABC Context
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marginal-likelihood-and-loo">
     8.7.1. Marginal Likelihood and LOO
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-choice-via-random-forest">
     8.7.2. Model Choice via Random Forest
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-choice-for-ma-model">
     8.7.3. Model Choice for MA Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#choosing-priors-for-abc">
   8.8. Choosing Priors for ABC
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   8.9. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="approximate-bayesian-computation">
<span id="chap8"></span><h1><span class="section-number">8. </span>Approximate Bayesian Computation<a class="headerlink" href="#approximate-bayesian-computation" title="Permalink to this headline">¶</a></h1>
<p>In this chapter we discuss Approximate Bayesian Computation (ABC). The
“approximate” in ABC refers to the lack of explicit likelihood, but not
to the use of numerical methods to approximate the posterior, such as
Markov chain Monte Carlo or Variational Inference. Another common, and
more explicit name, for ABC methods is likelihood-free methods, although
some authors mark a difference between these terms others use them
interchangeably.</p>
<p>ABC methods may be useful when we do not have an explicit expression for
the likelihood, but we have a parameterized <em>simulator</em> capable of
generating synthetic data. The simulator has one or more unknown
parameters and we want to know which set of parameters generates
synthetic data <em>close enough</em> to the observed data. To this extent we
will compute a posterior distribution of those parameters.</p>
<p>ABC methods are becoming increasingly common in the biological sciences,
in particular in sub-fields like systems biology, epidemiology, ecology
and population genetics <span id="id1">[<a class="reference internal" href="references.html#id77">84</a>]</span>. But they are also used in other
domains as they provide a flexible way to solve many practical problems.
This diversity is also reflected in the Python packages available for
ABC <span id="id2">[<a class="reference internal" href="references.html#id56">85</a>, <a class="reference internal" href="references.html#id55">86</a>, <a class="reference internal" href="references.html#id57">87</a>]</span>. Nevertheless, the extra
layer of approximation comes with its own set of difficulties. Mainly
defining what <em>close enough</em> means in the absence of a likelihood and
then being able to actually compute an approximated posterior.</p>
<p>We will discuss these challenges in this chapter from a general
perspective. We highly recommend readers interested into applying ABC
methods to their own problems to complement this chapter with examples
from their own domain knowledge.</p>
<div class="section" id="life-beyond-likelihood">
<span id="id3"></span><h2><span class="section-number">8.1. </span>Life Beyond Likelihood<a class="headerlink" href="#life-beyond-likelihood" title="Permalink to this headline">¶</a></h2>
<p>From Bayes theorem (Equation
<a class="reference internal" href="chp_01.html#equation-eq-posterior-dist">(1.1)</a>), to compute a posterior we
need two basic ingredients, a prior and a likelihood. However, for
particular problems, we may find that we can not express the likelihood
in closed-form, or it is prohibitively costly to compute it. This seems
to be a dead end for our Bayesian enthusiasm. But that is not
necessarily the case as long as we are able to somehow generate
synthetic data. This generator of synthetic data is generally referred
to as a <em>simulator</em>. From the perspective of the ABC method the
simulator is a black-box, we feed parameter values at one side and get
simulated data from the other. The complication we add however, is
uncertainty about which inputs are good enough to generate synthetic
data similar to the observed data.</p>
<p>The basic notion common to all ABC methods is to replace the likelihood
by a <span class="math notranslate nohighlight">\(\delta\)</span> function that computes a distance or more generally some
form of discrepancy between the observed data <span class="math notranslate nohighlight">\(Y\)</span> and the synthetic data
<span class="math notranslate nohighlight">\(\hat Y\)</span> generated by a parameterized simulator <span class="math notranslate nohighlight">\(Sim\)</span>.</p>
<div class="math notranslate nohighlight">
\[\hat Y \sim Sim(\theta)\]</div>
<div class="math notranslate nohighlight">
\[p(\theta \mid Y) 
    \underset{\sim}{\propto} 
    \delta(Y, \hat Y \mid \epsilon)\; p(\boldsymbol{\theta})\]</div>
<p>We aim at using a function <span class="math notranslate nohighlight">\(\delta\)</span> to obtain a <em>practically good
enough</em> approximation to the <em>true</em> likelihood:</p>
<div class="math notranslate nohighlight">
\[\lim_{\epsilon \to 0} \delta(Y, \hat Y \mid \epsilon) = p(Y \mid \boldsymbol{\theta})\]</div>
<p>We introduce a tolerance parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> because the chance of
generating a synthetic data-set <span class="math notranslate nohighlight">\(\hat Y\)</span> being equal to the observed
data <span class="math notranslate nohighlight">\(Y\)</span> is virtually null for most problems <a class="footnote-reference brackets" href="#id54" id="id4">1</a>. The larger the value
of <span class="math notranslate nohighlight">\(\epsilon\)</span> the more tolerant we are about how close <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(\hat Y\)</span>
has to be in order to consider them as <em>close enough</em>. In general and
for a given problem, a larger value of <span class="math notranslate nohighlight">\(\epsilon\)</span> implies a more crude
approximation to the posterior, we will see examples of this later.</p>
<p>In practice, as we increase the sample size (or dimensionality) of the
data it becomes harder and harder to find a small enough values for the
distance function <span class="math notranslate nohighlight">\(\delta\)</span> <a class="footnote-reference brackets" href="#id55" id="id5">2</a>. A naive solution is to increase the
value of <span class="math notranslate nohighlight">\(\epsilon\)</span>, but this means increasing the error of our
approximation. A better solution could be to instead use one or more
summary statistics <span class="math notranslate nohighlight">\(S\)</span> and compute the distance between the data
summaries instead of between the simulated and real datasets.</p>
<div class="math notranslate nohighlight">
\[\delta\left(S(Y), S(\hat Y) \mid \epsilon\right)\]</div>
<p>We must be aware that using a summary statistic introduces an additional
source of error to the ABC approximation, unless the summary statistics
are sufficient with respect to the model parameters <span class="math notranslate nohighlight">\(\theta\)</span>.
Unfortunately, this is not always possible. Nevertheless non-sufficient
summary statistics can still be very useful in practice and they are
often used by practitioners.</p>
<p>In this chapter we will explore a few different distances and summary
statistics focusing on some proven methods. But know that ABC handles so
many different types of simulated data, in so many distinct fields, that
it may be hard to generalize. Moreover the literature is advancing very
quickly so we will focus on building the necessary knowledge, skills and
tools, so you find it easier to generalize to new problems as the ABC
methods continues to evolve.</p>
<div class="admonition-sufficient-statistics admonition">
<p class="admonition-title">Sufficient statistics</p>
<p>A statistic is sufficient with respect to a model
parameter if no other statistic computed from the same sample provides
any additional information about that sample. In other words, that
statistics is <em>sufficient</em> to summarize your samples without losing
information. For example, given a sample of independent values from a
normal distribution with expected value <span class="math notranslate nohighlight">\(\mu\)</span> and known finite variance
the sample mean is a <strong>sufficient statistic</strong> for <span class="math notranslate nohighlight">\(\mu\)</span>. Notice that the
mean says nothing about the dispersion, thus it is only sufficient with
respect to the parameter <span class="math notranslate nohighlight">\(\mu\)</span>. It is known that for iid data the only
distributions with a sufficient statistic with dimension equal to the
dimension of <span class="math notranslate nohighlight">\(\theta\)</span> are the distributions from the Exponential family
<span id="id6">[<a class="reference internal" href="references.html#id153">88</a>, <a class="reference internal" href="references.html#id152">89</a>, <a class="reference internal" href="references.html#id151">90</a>, <a class="reference internal" href="references.html#id150">91</a>]</span>. For other
distribution, the dimension of the sufficient statistic increases with
the sample size.</p>
</div>
</div>
<div class="section" id="approximating-the-approximated-posterior">
<span id="id7"></span><h2><span class="section-number">8.2. </span>Approximating the Approximated Posterior<a class="headerlink" href="#approximating-the-approximated-posterior" title="Permalink to this headline">¶</a></h2>
<p>The most basic method to perform Approximate Bayesian Computation is
probably rejection sampling. We will describe it using
<a class="reference internal" href="#fig-abc-rejection"><span class="std std-numref">Fig. 8.1</span></a> together with the high level step by step
description of the algorithm as follows.</p>
<ol class="simple">
<li><p>Sample a value of <span class="math notranslate nohighlight">\(\theta\)</span> from the prior distribution.</p></li>
<li><p>Pass that value to the simulator and generate synthetic data.</p></li>
<li><p>If the synthetic data is at a <em>distance</em> <span class="math notranslate nohighlight">\(\delta\)</span> closer than
<span class="math notranslate nohighlight">\(\epsilon\)</span> save the proposed <span class="math notranslate nohighlight">\(\theta\)</span>, otherwise reject it.</p></li>
<li><p>Repeat until having the desired number of samples.</p></li>
</ol>
<div class="figure align-default" id="fig-abc-rejection">
<a class="reference internal image-reference" href="../_images/ABC_rejection.png"><img alt="../_images/ABC_rejection.png" src="../_images/ABC_rejection.png" style="width: 4.5in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.1 </span><span class="caption-text">One step of an ABC-rejection sampler. We sample a set of <span class="math notranslate nohighlight">\(\theta\)</span> values
from the prior distribution (at the top). Each value is passed to the
simulator, which generates synthetic datasets (dashed distributions), we
compare the synthetic data with the observed one (the distribution at
the bottom). In this example only <span class="math notranslate nohighlight">\(\theta_1\)</span> was able to generate a
synthetic dataset close enough to the observed data, thus <span class="math notranslate nohighlight">\(\theta_0\)</span> and
<span class="math notranslate nohighlight">\(\theta_2\)</span> were rejected. Notice that if we were using a summary
statistics, instead of the entire dataset we should compute the summary
statistics of the synthetic and observed data after step 2 and before
step 3.</span><a class="headerlink" href="#fig-abc-rejection" title="Permalink to this image">¶</a></p>
</div>
<p>The major drawback of the ABC-rejection sampler is that if the prior
distribution is too different from the posterior distribution we will
spend most of the time proposing values that will be rejected. A better
idea is to propose from a distribution closer to the actual posterior.
Generally we do not know enough about the posterior to do this by hand,
but we can achieve it using a Sequential Monte Carlo (SMC) method. This
is a general sampler method, like the MCMC methods we have been using in
the book. SMC can be adapted to perform ABC and then it is called
SMC-ABC. If you want to learn more about the detail of the SMC method
you can read Section <a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">Inference Methods</span></a>, but to
understand this chapter you just need to know that SMC proceeds by
increasing the value of a auxiliary parameter <span class="math notranslate nohighlight">\(\beta\)</span> in <span class="math notranslate nohighlight">\(s\)</span> successive
stages <span class="math notranslate nohighlight">\(\{\beta_0=0 &lt; \beta_1  &lt; ...  &lt; \beta_s=1\}\)</span>. This is done in
such a way that we start sampling from the prior (<span class="math notranslate nohighlight">\(\beta = 0\)</span>) until we
reach the posterior (<span class="math notranslate nohighlight">\(\beta = 1\)</span>). Thus, we can think of <span class="math notranslate nohighlight">\(\beta\)</span> as a
parameter that <em>gradually turns the likelihood on</em>. The intermediate
values of <span class="math notranslate nohighlight">\(\beta\)</span> are automatically computed by SMC. The more
informative the data with respect to the prior and/or the more complex
the geometry of the posterior the more intermediate steps SMC will take.
<a class="reference internal" href="#fig-smc-tempering"><span class="std std-numref">Fig. 8.2</span></a> shows an hypothetical sequence of
intermediate distributions from the prior in light grey to the posterior
in blue.</p>
<div class="figure align-default" id="fig-smc-tempering">
<a class="reference internal image-reference" href="../_images/smc_tempering.png"><img alt="../_images/smc_tempering.png" src="../_images/smc_tempering.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.2 </span><span class="caption-text">Hypothetical sequence of tempered posteriors explored by an SMC sampler,
from the prior (<span class="math notranslate nohighlight">\(\beta = 0\)</span>), light gray, to the actual posterior
(<span class="math notranslate nohighlight">\(\beta = 1\)</span>), blue. Low <span class="math notranslate nohighlight">\(\beta\)</span> values at the beginning helps the
sampler to not get stuck in a single maximum.</span><a class="headerlink" href="#fig-smc-tempering" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="fitting-a-gaussian-the-abc-way">
<span id="id8"></span><h2><span class="section-number">8.3. </span>Fitting a Gaussian the ABC-way<a class="headerlink" href="#fitting-a-gaussian-the-abc-way" title="Permalink to this headline">¶</a></h2>
<p>Let us warm up with a simple example, the estimation of the mean and
standard deviation from Gaussian distributed data with mean 0 and
standard deviation 1. For this problem we can fit the model:</p>
<div class="math notranslate nohighlight" id="equation-eq-gauss-model">
<span class="eqno">(8.1)<a class="headerlink" href="#equation-eq-gauss-model" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}\begin{split}\begin{split}
    \boldsymbol{\mu} \sim &amp;\; \mathcal{N}(0, 1) \\
    \boldsymbol{\sigma} \sim &amp;\; \mathcal{HN}(1) \\
    \boldsymbol{s} \sim &amp;\; \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma})\end{split}\\\end{split}\end{aligned}\end{align} \]</div>
<p>The straightforward way to write this model in PyMC3 is shown in Code
Block <a class="reference internal" href="#gauss-nuts"><span class="std std-ref">gauss_nuts</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="gauss-nuts">
<div class="code-block-caption"><span class="caption-number">Listing 8.1 </span><span class="caption-text">gauss_nuts</span><a class="headerlink" href="#gauss-nuts" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">gauss</span><span class="p">:</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">trace_g</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The equivalent model using SMC-ABC is shown in Code Block
<a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="gauss-abc">
<div class="code-block-caption"><span class="caption-number">Listing 8.2 </span><span class="caption-text">gauss_abc</span><a class="headerlink" href="#gauss-abc" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">gauss</span><span class="p">:</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">normal_simulator</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">],</span>
                     <span class="n">distance</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span>
                     <span class="n">sum_stat</span><span class="o">=</span><span class="s2">&quot;sort&quot;</span><span class="p">,</span>  
                     <span class="n">epsilon</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">trace_g</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;ABC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can see there are two important differences between Code Block
<a class="reference internal" href="#gauss-nuts"><span class="std std-ref">gauss_nuts</span></a> and Code Block
<a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a>:</p>
<ul class="simple">
<li><p>The use of the <code class="docutils literal notranslate"><span class="pre">pm.Simulator</span></code> <em>distribution</em></p></li>
<li><p>The use of <code class="docutils literal notranslate"><span class="pre">pm.sample_smc(kernel=&quot;ABC&quot;)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">pm.sample()</span></code>.</p></li>
</ul>
<p>By using <code class="docutils literal notranslate"><span class="pre">pm.Simulator</span></code> we are telling PyMC3 that we are not going to
use a closed form expression for the likelihood, and instead we are
going to define a pseudo-likelihood. We need to pass a Python function
that generates synthetic data, in this example the function
<code class="docutils literal notranslate"><span class="pre">normal_simulator</span></code>, together with its parameters. Code Block
<a class="reference internal" href="#normal-simulator"><span class="std std-ref">normal_simulator</span></a> shows the definition
of this function for a sample size of 1000 and unknown parameters <span class="math notranslate nohighlight">\(\mu\)</span>
and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<div class="literal-block-wrapper docutils container" id="normal-simulator">
<div class="code-block-caption"><span class="caption-number">Listing 8.3 </span><span class="caption-text">normal_simulator</span><a class="headerlink" href="#normal-simulator" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">normal_simulator</span><span class="p">(</span><span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We may also need to pass other, optional, arguments to <code class="docutils literal notranslate"><span class="pre">pm.Simulator</span></code>
including the distance function <code class="docutils literal notranslate"><span class="pre">distance</span></code>, the summary statistics
<code class="docutils literal notranslate"><span class="pre">sum_stat</span></code> and the value of the tolerance parameter <span class="math notranslate nohighlight">\(\epsilon\)</span>
<code class="docutils literal notranslate"><span class="pre">epsilon</span></code>. We will discuss these arguments in detail later. We also pass
the observed data to the simulator distribution as with a regular
likelihood.</p>
<p>By using <code class="docutils literal notranslate"><span class="pre">pm.sample_smc(kernel=&quot;ABC&quot;)</span></code><a class="footnote-reference brackets" href="#id56" id="id9">3</a> we are telling PyMC3 to look
for a <code class="docutils literal notranslate"><span class="pre">pm.Simulator</span></code> in the model and use it to define a
pseudo-likelihood, the rest of the sampling process is the same as the
one described for the SMC algorithm. Other samplers will fail to run
when <code class="docutils literal notranslate"><span class="pre">pm.Simulator</span></code> is present.</p>
<p>The final ingredient is the <code class="docutils literal notranslate"><span class="pre">normal_simulator</span></code> function. In principle we
can use whatever Python function we want, in fact we can even wrap
non-Python code, like Fortran or C code. That is where the flexibility
of ABC methods reside. In this example our simulator is just a wrapper
around a NumPy random generator function.</p>
<p>As with other samplers it is recommended that we run more than one chain
so we can diagnose if the sampler failed to work properly, PyMC3 will
try to do this automatically. <a class="reference internal" href="#fig-trace-g"><span class="std std-numref">Fig. 8.3</span></a> shows the result of
running Code Block <a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> with two chains.
We can see that we were able to recover the true parameters and that the
sampler is not showing any evident sampling issue.</p>
<div class="figure align-default" id="fig-trace-g">
<a class="reference internal image-reference" href="../_images/trace_g.png"><img alt="../_images/trace_g.png" src="../_images/trace_g.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.3 </span><span class="caption-text">As expected <span class="math notranslate nohighlight">\(\mu \approx 0\)</span> and <span class="math notranslate nohighlight">\(\sigma \approx 1\)</span>, both chains agree
about the posterior as reflected by the KDEs and also by the rank plots.
Notice that each of these 2 chains was obtained by running 2000 parallel
SMC-chains/particles as described in the SMC algorithm.</span><a class="headerlink" href="#fig-trace-g" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="choosing-the-distance-function-epsilon-and-the-summary-statistics">
<span id="id10"></span><h2><span class="section-number">8.4. </span>Choosing the Distance Function, <span class="math notranslate nohighlight">\(\epsilon\)</span> and the Summary Statistics<a class="headerlink" href="#choosing-the-distance-function-epsilon-and-the-summary-statistics" title="Permalink to this headline">¶</a></h2>
<p>Defining a useful distance, summary statistic and <span class="math notranslate nohighlight">\(\epsilon\)</span> is problem
dependent. This means that we should expect some trial and error before
getting good results, especially when jumping into a new problem. As
usual thinking first about good options helps to reduce the number of
choices. But we should embrace running experiments too as they are
always helpful to better understand the problem and to make a more
informed decision about these hyperparameters. In the following sections
we will discuss a few general guidelines.</p>
<div class="section" id="choosing-the-distance">
<span id="id11"></span><h3><span class="section-number">8.4.1. </span>Choosing the Distance<a class="headerlink" href="#choosing-the-distance" title="Permalink to this headline">¶</a></h3>
<p>We run Code Block <a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> with the default
distance function <code class="docutils literal notranslate"><span class="pre">distance=&quot;gaussian&quot;</span></code>, which is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-euclidean-abc">
<span class="eqno">(8.2)<a class="headerlink" href="#equation-eq-euclidean-abc" title="Permalink to this equation">¶</a></span>\[\sum_i - \frac{||X_{oi} - X_{si}||^2}{2 \epsilon_i^2}\]</div>
<p>Where <span class="math notranslate nohighlight">\(X_{o}\)</span> is the observed data, <span class="math notranslate nohighlight">\(X_{s}\)</span> is the simulated data and
<span class="math notranslate nohighlight">\(\epsilon\)</span> its scaling parameter. We call <a class="reference internal" href="#equation-eq-euclidean-abc">(8.2)</a>
<em>Gaussian</em> because it is the Gaussian kernel<a class="footnote-reference brackets" href="#id57" id="id12">4</a> in log scale. We use
the log scale to compute pseudo-likelihood as we did with actual
likelihoods (and priors)<a class="footnote-reference brackets" href="#id58" id="id13">5</a>. <span class="math notranslate nohighlight">\(||X_{oi} - X_{si}||^2\)</span> is the Euclidean
distance (also known as L2 norm) and hence we can also describe Equation
<a class="reference internal" href="#equation-eq-euclidean-abc">(8.2)</a> as a weighted Euclidean distance. This is a very
popular choice in the literature. Other popular options are the L1 norm
(the sum of absolute differences), called Laplace distance in PyMC3, the
L<span class="math notranslate nohighlight">\(\infty\)</span> norm (the maximum absolute value of the differences) or the
Mahalanobis distance: <span class="math notranslate nohighlight">\(\sqrt{(xo - xs)^{T}\Sigma(xo - xs)}\)</span>, where
<span class="math notranslate nohighlight">\(\Sigma\)</span> is a covariance matrix.</p>
<p>Distances such as Gaussian, Laplace, etc can be applied to the whole
data or, as we already mentioned, to summary statistics. There are also
some distance functions that have been introduced specifically to avoid
the need of summary statistics and still provide good results
<span id="id14">[<a class="reference internal" href="references.html#id48">92</a>, <a class="reference internal" href="references.html#id47">93</a>, <a class="reference internal" href="references.html#id46">94</a>]</span>. We are going to discuss two of
them, the Wasserstein distances and the KL divergence.</p>
<p>In Code Block <a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> we use
<code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;sort&quot;</span></code> <a class="footnote-reference brackets" href="#id59" id="id15">6</a>, this tells PyMC3 to sort the data before
computing Equation <a class="reference internal" href="#equation-eq-euclidean-abc">(8.2)</a>. Doing this is equivalent to
computing the 1D 2-Wasserstein distance and if we do the same but we use
the L1 norm we get the 1D 1-Wasserstein distance. It is possible to
define Wasserstein distances for dimensions larger than 1
<span id="id16">[<a class="reference internal" href="references.html#id46">94</a>]</span>.</p>
<p>Sorting the data before computing the distance makes the comparison
between distributions much more fair. To see this imagine we have two
samples that are exactly equal, but out of pure luck one is ordered from
low to high and the other from high to low. In such a case if we apply a
metric like Equation <a class="reference internal" href="#equation-eq-euclidean-abc">(8.2)</a> we would conclude both
samples are very dissimilar, even when they are the same sample. But if
we sort first, we will conclude they are the same. This is a very
extreme scenario but it helps clarify the intuition behind sorting the
data. One more thing, if we sort the data we are assuming we only care
about the distribution and not the order of the data, otherwise sorting
will destroy the structure in the data. This could happen, for example,
with a time series, see Chapter <a class="reference internal" href="chp_06.html#chap4"><span class="std std-ref">6</span></a>.</p>
<p>Another distance introduced to avoid the need to define a summary
statistic is the use of the KL divergence (see Section <a class="reference internal" href="chp_11.html#dkl"><span class="std std-ref">Kullback-Leibler Divergence</span></a>). The KL
divergence is approximated using the following expression
<span id="id17">[<a class="reference internal" href="references.html#id48">92</a>, <a class="reference internal" href="references.html#id47">93</a>]</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-abc">
<span class="eqno">(8.3)<a class="headerlink" href="#equation-eq-kl-abc" title="Permalink to this equation">¶</a></span>\[\frac{d}{n}  \sum \left(- \frac{\log(\frac{\nu_d}{\rho_d})}{\epsilon} \right) + \log\left(\frac{n}{n-1}\right)\]</div>
<p>Where <span class="math notranslate nohighlight">\(d\)</span> is the dimension of the dataset (number of variables or
features), <span class="math notranslate nohighlight">\(n\)</span> is the number of observed datapoints. <span class="math notranslate nohighlight">\(\nu_d\)</span> contains
the 1-nearest neighbor distances of the observed to simulated data and
<span class="math notranslate nohighlight">\(\rho_d\)</span> the 2-nearest neighbor distances of the observed data to itself
(notice that if you compare a dataset with itself the 1-nearest neighbor
distances will always be zero). As this method involves 2n operations of
nearest neighbor search, it is generally implemented using k-d trees
<span id="id18">[<a class="reference internal" href="references.html#id49">95</a>]</span>.</p>
</div>
<div class="section" id="choosing-epsilon">
<span id="id19"></span><h3><span class="section-number">8.4.2. </span>Choosing <span class="math notranslate nohighlight">\(\epsilon\)</span><a class="headerlink" href="#choosing-epsilon" title="Permalink to this headline">¶</a></h3>
<p>In many ABC methods the <span class="math notranslate nohighlight">\(\epsilon\)</span> parameter works as a hard-threshold,
<span class="math notranslate nohighlight">\(\theta\)</span> values generating samples with distance larger than <span class="math notranslate nohighlight">\(\epsilon\)</span>
are rejected. Additionally <span class="math notranslate nohighlight">\(\epsilon\)</span> can be a list of decreasing values
that the user has to set or the algorithm adaptive finds <a class="footnote-reference brackets" href="#id60" id="id20">7</a>.</p>
<p>In PyMC3, <span class="math notranslate nohighlight">\(\epsilon\)</span> is the scale of the distance function, like in
Equation <a class="reference internal" href="#equation-eq-euclidean-abc">(8.2)</a>, so it does not work as a
hard-threshold. We can set <span class="math notranslate nohighlight">\(\epsilon\)</span> according to our needs. We can
choose a scalar value (which is equivalent to setting <span class="math notranslate nohighlight">\(\epsilon_i\)</span> equal
for all <span class="math notranslate nohighlight">\(i\)</span>). This is useful when evaluating the distance over the data
instead of using summary statistics. In this case a reasonably educated
guess could be the empirical standard deviation of the data. If we
instead use a summary statistic then we can set <span class="math notranslate nohighlight">\(\epsilon\)</span> to a list of
values. This is usually necessary as each summary statistic may have a
different scale. If the scales are too different then the contribution
of each summary statistic will be uneven, it may even occur that a
single summary statistic dominates the computed distances. A popular
choice for <span class="math notranslate nohighlight">\(\epsilon\)</span> in those cases is the empirical standard deviation
of the <span class="math notranslate nohighlight">\(i^{\text{th}}\)</span> summary statistic under the prior predictive distribution,
or the median absolute deviation, as this is more robust to outliers. A
problem with using the prior predictive distribution is that it can be
way broader than the posterior predictive distribution. Thus to find a
useful value of <span class="math notranslate nohighlight">\(\epsilon\)</span> we may want to take these educated guesses
previously mentioned as upper bound and then from those values try also
a few lower values. Then we could choose a final value of <span class="math notranslate nohighlight">\(\epsilon\)</span>
based on several factors including the computational cost, the needed
level of precision/error and the efficiency of the sampler. In general
the lower the value of <span class="math notranslate nohighlight">\(\epsilon\)</span> the better the approximation.
<a class="reference internal" href="#fig-trace-g-many-eps"><span class="std std-numref">Fig. 8.4</span></a> shows a forest plot for <span class="math notranslate nohighlight">\(\mu\)</span> and
<span class="math notranslate nohighlight">\(\sigma\)</span> for several values of <span class="math notranslate nohighlight">\(\epsilon\)</span> and also for the “NUTS”
sampler (using a normal likelihood instead of a simulator).</p>
<div class="figure align-default" id="fig-trace-g-many-eps">
<a class="reference internal image-reference" href="../_images/trace_g_many_eps.png"><img alt="../_images/trace_g_many_eps.png" src="../_images/trace_g_many_eps.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Forest plot for <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>, obtained using NUTS or ABC with
increasing values of <span class="math notranslate nohighlight">\(\epsilon\)</span>, 1, 5, and 10.</span><a class="headerlink" href="#fig-trace-g-many-eps" title="Permalink to this image">¶</a></p>
</div>
<p>Decreasing the value of <span class="math notranslate nohighlight">\(\epsilon\)</span> has a limit, a too low value will
make the sampler very inefficient, signaling that we are aiming at an
accuracy level that does not make too much sense.
<a class="reference internal" href="#fig-trace-g-eps-too-low"><span class="std std-numref">Fig. 8.5</span></a> shows how the SMC sampler fails to
converge when the model from Code Block
<a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> is sampled with a value of
<code class="docutils literal notranslate"><span class="pre">epsilon=0.1</span></code>. As we can see the sampler fails spectacularly.</p>
<div class="figure align-default" id="fig-trace-g-eps-too-low">
<a class="reference internal image-reference" href="../_images/trace_g_eps_too_low.png"><img alt="../_images/trace_g_eps_too_low.png" src="../_images/trace_g_eps_too_low.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.5 </span><span class="caption-text">KDE and rank plot for model <code class="docutils literal notranslate"><span class="pre">trace_g_001</span></code>, failure of convergence could
indicate that the value <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> is too low for this problem.</span><a class="headerlink" href="#fig-trace-g-eps-too-low" title="Permalink to this image">¶</a></p>
</div>
<p>To help us decide on a good value for <span class="math notranslate nohighlight">\(\epsilon\)</span> we can get help from
the model criticism tools we have been using for non-ABC methods, like
Bayesian p-values and posterior predictive checks as exemplified in
Figures <a class="reference internal" href="#fig-bpv-g-many-eps-00"><span class="std std-numref">Fig. 8.6</span></a>,
<a class="reference internal" href="#fig-bpv-g-many-eps-01"><span class="std std-numref">Fig. 8.7</span></a>, and <a class="reference internal" href="#fig-ppc-g-many-eps"><span class="std std-numref">Fig. 8.8</span></a>.
<a class="reference internal" href="#fig-bpv-g-many-eps-00"><span class="std std-numref">Fig. 8.6</span></a> includes the value <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span>. We do
that here to show what poorly calibrated models look like. But in
practice if we obtain a rank plot like the one in
<a class="reference internal" href="#fig-trace-g-eps-too-low"><span class="std std-numref">Fig. 8.5</span></a> we should stop right there with the
analysis of the computed posterior and reinspect the model definition.
Additionally, for ABC methods, we should also inspect the value of the
hyperparameter <span class="math notranslate nohighlight">\(\epsilon\)</span>, the summary statistics we have chosen or the
distance function.</p>
<div class="figure align-default" id="fig-bpv-g-many-eps-00">
<a class="reference internal image-reference" href="../_images/bpv_g_many_eps_00.png"><img alt="../_images/bpv_g_many_eps_00.png" src="../_images/bpv_g_many_eps_00.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.6 </span><span class="caption-text">Distribution of marginal Bayesian p-values for increasing values of
<span class="math notranslate nohighlight">\(\epsilon\)</span>. For a well calibrated model we should expect a Uniform
distribution. We can see that for <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> the calibration is
terrible, this is not surprising as this value of <span class="math notranslate nohighlight">\(\epsilon\)</span> is too. For
all the other values of <span class="math notranslate nohighlight">\(\epsilon\)</span> the distribution looks much more
Uniform and the level of uniformity decreases as <span class="math notranslate nohighlight">\(\epsilon\)</span> increases.
The <code class="docutils literal notranslate"><span class="pre">se</span></code> values are the (scaled) squared differences between the
expected Uniform distribution and the computed KDEs.</span><a class="headerlink" href="#fig-bpv-g-many-eps-00" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-bpv-g-many-eps-01">
<a class="reference internal image-reference" href="../_images/bpv_g_many_eps_01.png"><img alt="../_images/bpv_g_many_eps_01.png" src="../_images/bpv_g_many_eps_01.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Bayesian p-values for increasing values of epsilon. The blue curve is
the observed distribution and the gray curves the expected ones. For a
well calibrated model we should expect a distribution concentrated
around 0.5. We can see that for <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> the calibration is
terrible, this is not surprising as this value of <span class="math notranslate nohighlight">\(\epsilon\)</span> is too low.
We can see that <span class="math notranslate nohighlight">\(\epsilon=1\)</span> provides the best results.</span><a class="headerlink" href="#fig-bpv-g-many-eps-01" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-ppc-g-many-eps">
<a class="reference internal image-reference" href="../_images/ppc_g_many_eps.png"><img alt="../_images/ppc_g_many_eps.png" src="../_images/ppc_g_many_eps.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.8 </span><span class="caption-text">Posterior predictive checks for increasing values of <span class="math notranslate nohighlight">\(\epsilon\)</span>. The
blue curve is the observed distribution and the gray curves the expected
ones. Surprisingly from <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> we get what seems to be a good
adjustment, even when we know that the samples from that posterior are
not trustworthy, this is a very simple example and we got the right
answer out of pure luck. This is an example of <em>a too good to be true
fit</em>. These are the worst! If we only consider models with posterior
samples that look reasonable (i.e. not <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span> ), we can see that
<span class="math notranslate nohighlight">\(\epsilon=1\)</span> provides the best results.</span><a class="headerlink" href="#fig-ppc-g-many-eps" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="choosing-summary-statistics">
<span id="id21"></span><h3><span class="section-number">8.4.3. </span>Choosing Summary Statistics<a class="headerlink" href="#choosing-summary-statistics" title="Permalink to this headline">¶</a></h3>
<p>The choice of summary statistics is arguably more difficult and will
have a larger effect than the choice of the distance function. For that
reason a lot of research has been focused on the subject from the use of
distance that does not require summary statistics
<span id="id22">[<a class="reference internal" href="references.html#id47">93</a>, <a class="reference internal" href="references.html#id46">94</a>]</span> to strategies for choosing summary
statistics <span id="id23">[<a class="reference internal" href="references.html#id50">96</a>]</span>.</p>
<p>A good summary statistic provides a balance between low dimension and
informativeness. When we do not have a sufficient summary statistic it
is tempting to overcompensate by adding a lot of summary statistics. The
intuition is that the more information the better. However, increasing
the number of summary statistics can actually reduce the quality of the
approximated posterior <span id="id24">[<a class="reference internal" href="references.html#id50">96</a>]</span>. One explanation for this is that
we move from computing distances over data to distances over summaries
to reduce the dimensionality, by increasing the number of summaries
statistics we are defeating that purpose.</p>
<p>In some fields like population genetics, where ABC methods are very
common, people have developed a large collection of useful summary
statistics <span id="id25">[<a class="reference internal" href="references.html#id53">97</a>, <a class="reference internal" href="references.html#id54">98</a>, <a class="reference internal" href="references.html#id51">99</a>]</span>. In general it is
a good idea to check the literature from the applied field you are
working on to see what others are doing, as chances are high they have
already tried and tested many alternatives.</p>
<p>When in doubt we can follow the same recommendations from the previous
section to evaluate the model fit, i.e rank plots, Bayesian p-values,
posterior predictive checks, etc and try alternatives if necessary (see
Figures <a class="reference internal" href="#fig-trace-g-eps-too-low"><span class="std std-numref">Fig. 8.5</span></a>,
<a class="reference internal" href="#fig-bpv-g-many-eps-00"><span class="std std-numref">Fig. 8.6</span></a>, <a class="reference internal" href="#fig-bpv-g-many-eps-01"><span class="std std-numref">Fig. 8.7</span></a>, and
<a class="reference internal" href="#fig-ppc-g-many-eps"><span class="std std-numref">Fig. 8.8</span></a>).</p>
</div>
</div>
<div class="section" id="g-and-k-distribution">
<span id="id26"></span><h2><span class="section-number">8.5. </span>g-and-k Distribution<a class="headerlink" href="#g-and-k-distribution" title="Permalink to this headline">¶</a></h2>
<p>Carbon monoxide (CO) is a colorless, odorless gas that can be harmful,
even fatal, when inhaled in large amounts. This gas is generated when
something is burned, especially in situations when oxygen levels are
low. CO together with other gases like Nitrogen Dioxide (NO₂) are
usually monitored in many cities around the world to assess the level of
air pollution and the quality of air. In a city the main sources of CO
are cars, and other vehicles or machinery that work by burning fossil
fuels. <a class="reference internal" href="#fig-co-ppm-bsas"><span class="std std-numref">Fig. 8.9</span></a> shows a histogram of the daily CO
levels measured by one station in the city of Buenos Aires from 2010 to 2018.
As we can see the data seems to be slightly right skewed.
Additionally the data present a few observations with very high values.
The bottom panel omits 8 observations between 3 and 30.</p>
<div class="figure align-default" id="fig-co-ppm-bsas">
<a class="reference internal image-reference" href="../_images/co_ppm_bsas.png"><img alt="../_images/co_ppm_bsas.png" src="../_images/co_ppm_bsas.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.9 </span><span class="caption-text">Histogram of CO levels. The top panel shows the entire data and the
bottom one omits values larger than 3.</span><a class="headerlink" href="#fig-co-ppm-bsas" title="Permalink to this image">¶</a></p>
</div>
<p>To fit this data we are going to introduce the univariate g-and-k
distribution. This is a 4 parameter distribution able to describe data
with high skewness and/or kurtosis <span id="id27">[<a class="reference internal" href="references.html#id40">100</a>, <a class="reference internal" href="references.html#id41">101</a>]</span>. Its
density function in unavailable in closed form and the g-and-k
distribution are defined through its quantile function, i.e. the inverse
of the cumulative distribution function:</p>
<div class="math notranslate nohighlight" id="equation-eq-g-and-k">
<span class="eqno">(8.4)<a class="headerlink" href="#equation-eq-g-and-k" title="Permalink to this equation">¶</a></span>\[a + b \ \left(1 + c \ \text{tanh}\left[\frac{gz(x)}{2}\right]\right) \left(1+z(x)^2\right)^k z(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(z\)</span> is the inverse of the standard normal cumulative distribution
function and <span class="math notranslate nohighlight">\(x \in (0,1)\)</span>.</p>
<p>The parameters <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(k\)</span> are the location, scale, skewness
and kurtosis parameters respectively. If <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(k\)</span> are 0, we recover
the Gaussian distribution with mean <span class="math notranslate nohighlight">\(a\)</span> and standard deviation <span class="math notranslate nohighlight">\(b\)</span>.
<span class="math notranslate nohighlight">\(g &gt; 0\)</span> gives positive (right) skewness and <span class="math notranslate nohighlight">\(g &lt; 0\)</span> gives negative
(left) skewness. The parameter <span class="math notranslate nohighlight">\(k \geqslant 0\)</span> gives longer tails than
the normal and <span class="math notranslate nohighlight">\(k &lt; 0\)</span> gives shorter tails than the normal. <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(g\)</span>
can take any real value. It is common to restrict <span class="math notranslate nohighlight">\(b\)</span> to be positive and
<span class="math notranslate nohighlight">\(k \geqslant -0.5\)</span> or sometimes <span class="math notranslate nohighlight">\(k \geqslant 0\)</span> i.e. tails as heavy or
heavier than those from a Gaussian distribution. Additionally it is
common to fix <span class="math notranslate nohighlight">\(c=0.8\)</span>. With all these restrictions we are guaranteed to
get a strictly increasing quantile function <span id="id28">[<a class="reference internal" href="references.html#id41">101</a>]</span> which is a
hallmark of well-defined continuous distribution functions.</p>
<p>Code Block <a class="reference internal" href="#gk-quantile"><span class="std std-ref">gk_quantile</span></a> defines a g-and-k
quantile distribution. We have omitted the calculation of the cdf and
pdf because they are a little bit more involved, but most importantly
because we are not going to use them for our examples <a class="footnote-reference brackets" href="#id61" id="id29">8</a>. While the
probability density function of the g-and-k distribution can be
evaluated numerically <span id="id30">[<a class="reference internal" href="references.html#id41">101</a>, <a class="reference internal" href="references.html#id45">102</a>]</span>, simulating from the
g-and-k model using the inversion method is more straightforward and
fast <span id="id31">[<a class="reference internal" href="references.html#id45">102</a>, <a class="reference internal" href="references.html#id42">103</a>]</span>. To implement the inversion method we
sample <span class="math notranslate nohighlight">\(x \sim \mathcal{U}(0, 1)\)</span> and replace in Equation
<a class="reference internal" href="#equation-eq-g-and-k">(8.4)</a>. Code Block <a class="reference internal" href="#gk-quantile"><span class="std std-ref">gk_quantile</span></a>
shows how to do this in Python and <a class="reference internal" href="#fig-gk-quantile"><span class="std std-numref">Fig. 8.10</span></a> shows
examples of g-and-k distributions.</p>
<div class="literal-block-wrapper docutils container" id="gk-quantile">
<div class="code-block-caption"><span class="caption-number">Listing 8.4 </span><span class="caption-text">gk_quantile</span><a class="headerlink" href="#gk-quantile" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">g_and_k_quantile</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantile_normal</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">ppf</span>

    <span class="k">def</span> <span class="nf">ppf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantile_normal</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">g</span><span class="o">*</span><span class="n">z</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">z</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">rvs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">samples</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ppf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-gk-quantile">
<a class="reference internal image-reference" href="../_images/gk_quantile.png"><img alt="../_images/gk_quantile.png" src="../_images/gk_quantile.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.10 </span><span class="caption-text">The first row shows the quantile function, also known as the inverse of
the cumulative distribution function. You feed it with a quantile value
and it returns the value of the variable that represents that quantile.
For example, if you have <span class="math notranslate nohighlight">\(P(X &lt;= x_q) = q\)</span>, you pass <span class="math notranslate nohighlight">\(q\)</span> to the quantile
function and you get <span class="math notranslate nohighlight">\(x_q\)</span>. The second row shows the (approximated) pdf.
For this example the pdf has been computed using a kernel density
estimation from the random samples generated with Code Block
<a class="reference internal" href="#gk-quantile"><span class="std std-ref">gk_quantile</span></a>.</span><a class="headerlink" href="#fig-gk-quantile" title="Permalink to this image">¶</a></p>
</div>
<p>To fit a g-and-k distribution using SMC-ABC, we can use the Gaussian
distance and <code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;sort&quot;</span></code> as we did for the Gaussian example.
Alternative, we can also think of a summary statistic tailored for this
problem. As we know, the parameters <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(k\)</span> are associated
with location, scale, skewness and kurtosis respectively. Thus, we can
think of a summary statistic based on robust estimates for these
quantities <span id="id32">[<a class="reference internal" href="references.html#id42">103</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
sa &amp;= e4 \\
sb &amp;= e6 - e2 \\  
sg &amp;= (e6 + e2 - 2*e4)/sb \\ 
sk &amp;= (e7 - e5 + e3 - e1)/sb \\
\end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(e1\)</span> to <span class="math notranslate nohighlight">\(e7\)</span> are octiles, i.e. the quantiles that divide a sample
into eight subsets.</p>
<p>If we pay attention we can see that <span class="math notranslate nohighlight">\(sa\)</span> is the median and <span class="math notranslate nohighlight">\(sb\)</span> the
interquartile range, which are robust estimators of location and
dispersion. Even when <span class="math notranslate nohighlight">\(sg\)</span> and <span class="math notranslate nohighlight">\(sk\)</span> may look a little bit more obscure,
they are also robust estimators of skewness <span id="id33">[<a class="reference internal" href="references.html#id43">104</a>]</span> and kurtosis
<span id="id34">[<a class="reference internal" href="references.html#id44">105</a>]</span>, respectively. Let us make this more clear. For a symmetric
distribution <span class="math notranslate nohighlight">\(e6-e4\)</span> and <span class="math notranslate nohighlight">\(e2-e4\)</span> will have the same magnitude but
opposite signs so in such a case <span class="math notranslate nohighlight">\(sg\)</span> will be zero, and for skewed
distributions either <span class="math notranslate nohighlight">\(e6-e4\)</span> will be larger than <span class="math notranslate nohighlight">\(e2-e4\)</span> or vice versa.
The two terms in the numerator of <span class="math notranslate nohighlight">\(sk\)</span> increase when the mass in the
neighbourhood of <span class="math notranslate nohighlight">\(e6\)</span> and <span class="math notranslate nohighlight">\(e2\)</span> decreases, i.e. when we <em>move</em> mass from
the central part of the distribution to the tails. The denominator in
both <span class="math notranslate nohighlight">\(sg\)</span> and <span class="math notranslate nohighlight">\(sk\)</span> acts as a normalization factor.</p>
<p>With this idea in mind we can use Python to create a summary statistic
for our problem as specified in the following code block.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">octo_summary</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">e1</span><span class="p">,</span> <span class="n">e2</span><span class="p">,</span> <span class="n">e3</span><span class="p">,</span> <span class="n">e4</span><span class="p">,</span> <span class="n">e5</span><span class="p">,</span> <span class="n">e6</span><span class="p">,</span> <span class="n">e7</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mf">.125</span><span class="p">,</span> <span class="mf">.25</span><span class="p">,</span> <span class="mf">.375</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.625</span><span class="p">,</span> <span class="mf">.75</span><span class="p">,</span> <span class="mf">.875</span><span class="p">])</span>
    <span class="n">sa</span> <span class="o">=</span> <span class="n">e4</span>
    <span class="n">sb</span> <span class="o">=</span> <span class="n">e6</span> <span class="o">-</span> <span class="n">e2</span>
    <span class="n">sg</span> <span class="o">=</span> <span class="p">(</span><span class="n">e6</span> <span class="o">+</span> <span class="n">e2</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">e4</span><span class="p">)</span><span class="o">/</span><span class="n">sb</span>
    <span class="n">sk</span> <span class="o">=</span> <span class="p">(</span><span class="n">e7</span> <span class="o">-</span> <span class="n">e5</span> <span class="o">+</span> <span class="n">e3</span> <span class="o">-</span> <span class="n">e1</span><span class="p">)</span><span class="o">/</span><span class="n">sb</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sa</span><span class="p">,</span> <span class="n">sb</span><span class="p">,</span> <span class="n">sg</span><span class="p">,</span> <span class="n">sk</span><span class="p">])</span>
</pre></div>
</div>
<p>Now we need to define a simulator, we can just wrap the <code class="docutils literal notranslate"><span class="pre">rvs</span></code> method
from the <code class="docutils literal notranslate"><span class="pre">g_and_k_quantile()</span></code> function previously defined in Code Block
<a class="reference internal" href="#gk-quantile"><span class="std std-ref">gk_quantile</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gk</span> <span class="o">=</span> <span class="n">g_and_k_quantile</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">gk_simulator</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">gk</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bsas_co</span><span class="p">),</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
<p>Having defined the summary statistic and the simulator and having
imported the data, we can define our model. For this example we use
weakly informative priors based on the fact that all the parameters are
restricted to be positive. CO levels can not take negative values so <span class="math notranslate nohighlight">\(a\)</span>
is positive and <span class="math notranslate nohighlight">\(g\)</span> is also expected to be 0 or positive as most common
levels are expected to be “low”, with some measurement taking larger
values. We also have reasons to assume that the parameters are most
likely to be below 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">gkm</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">s</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">gk_simulator</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span>        
                     <span class="n">sum_stat</span><span class="o">=</span><span class="n">octo_summary</span><span class="p">,</span>
                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                     <span class="n">observed</span><span class="o">=</span><span class="n">bsas_co</span><span class="p">)</span>
    
    <span class="n">trace_gk</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;ABC&quot;</span><span class="p">,</span> <span class="n">parallel</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-plot-pair"><span class="std std-numref">Fig. 8.11</span></a> shows a pair plot of the fitted <code class="docutils literal notranslate"><span class="pre">gkm</span></code> model.</p>
<div class="figure align-default" id="fig-plot-pair">
<a class="reference internal image-reference" href="../_images/pair_gk.png"><img alt="../_images/pair_gk.png" src="../_images/pair_gk.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.11 </span><span class="caption-text">The distribution is slightly skewed and with some degree of kurtosis as
expected from the few CO levels with values one or two orders of
magnitude larger than the bulk of CO values. We can see that <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(k\)</span>
are (slightly) correlated. This is expected, as the density in the tail
(kurtosis) increases, the dispersion increases, but the g-and-k
distribution can keep <span class="math notranslate nohighlight">\(b\)</span> small if <span class="math notranslate nohighlight">\(k\)</span> increases. It is like <span class="math notranslate nohighlight">\(k\)</span> is
<em>absorbing</em> part of the dispersion, similar to what we observed with the
scale and the <span class="math notranslate nohighlight">\(\nu\)</span> parameter in a Student’s t-distribution.</span><a class="headerlink" href="#fig-plot-pair" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="approximating-moving-averages">
<span id="abc-ma"></span><h2><span class="section-number">8.6. </span>Approximating Moving Averages<a class="headerlink" href="#approximating-moving-averages" title="Permalink to this headline">¶</a></h2>
<p>The moving-average (MA) model is a common approach for modeling
univariate time series (see Chapter <a class="reference internal" href="chp_06.html#chap4"><span class="std std-ref">6</span></a>). The MA(q) model
specifies that the output variable depends linearly on the current and
<span class="math notranslate nohighlight">\(q\)</span> previous past values of a stochastic term <span class="math notranslate nohighlight">\(\lambda\)</span>. <span class="math notranslate nohighlight">\(q\)</span> is known as
the order of the MA model.</p>
<div class="math notranslate nohighlight">
\[y_t = \mu + \lambda_t + \theta_1 \lambda_{t-1} + \cdots + \theta_q \lambda_{t-q}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> are white Gaussian noise error terms <a class="footnote-reference brackets" href="#id63" id="id35">9</a>.</p>
<p>We are going to use a toy-model taken from <span id="id36">Marin <em>et al.</em> [<a class="reference internal" href="references.html#id39">106</a>]</span>. For
this example we are going to use the MA(2) model with mean value 0 (i.e.
<span class="math notranslate nohighlight">\(\mu =0\)</span>), thus our model looks like:</p>
<div class="math notranslate nohighlight">
\[y_t = \lambda_t + \theta_1 \lambda_{t-1} +  \theta_2 \lambda_{t-2}\]</div>
<p>Code Block <a class="reference internal" href="#ma2-simulator-abc"><span class="std std-ref">ma2_simulator_abc</span></a> shows a
Python simulator for this model and in <a class="reference internal" href="#fig-ma2-simulator-abc"><span class="std std-numref">Fig. 8.12</span></a>
we can see two realizations from that simulator for the values
<span class="math notranslate nohighlight">\(\theta1 = 0.6, \theta2=0.2\)</span>.</p>
<div class="literal-block-wrapper docutils container" id="ma2-simulator-abc">
<div class="code-block-caption"><span class="caption-number">Listing 8.5 </span><span class="caption-text">ma2_simulator_abc</span><a class="headerlink" href="#ma2-simulator-abc" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">moving_average_2</span><span class="p">(</span><span class="n">θ1</span><span class="p">,</span> <span class="n">θ2</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">λ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_obs</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">λ</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span> <span class="o">+</span> <span class="n">θ1</span><span class="o">*</span><span class="n">λ</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">θ2</span><span class="o">*</span><span class="n">λ</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-ma2-simulator-abc">
<a class="reference internal image-reference" href="../_images/ma2_simulator_abc.png"><img alt="../_images/ma2_simulator_abc.png" src="../_images/ma2_simulator_abc.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.12 </span><span class="caption-text">Two realizations of a MA(2) model, one with <span class="math notranslate nohighlight">\(\theta1=0.6, \theta2 =0.2\)</span>.
On the left column the kernel density estimation on the right column the
time series.</span><a class="headerlink" href="#fig-ma2-simulator-abc" title="Permalink to this image">¶</a></p>
</div>
<p>In principle we could try to fit a MA(q) model using any distance
function and/or summary statistic we want. Instead, we can use some
properties of the MA(q) model as a guide. One property that is often of
interest in MA(q) models is their autocorrelation. Theory establishes
that for a MA(q) model the lags larger than q will be zero, so for a
MA(2) seems to be reasonable to use as summary statistics the
autocorrelation function for lag 1 and lag 2. Additionally, and just to
avoid computing the variance of the data, we will use the
auto-covariance function instead of the auto-correlation function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">autocov</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">:]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
</pre></div>
</div>
<p>Additionally MA(q) models are non-identifiable unless we introduce a few
restrictions. For a MA(1) model, we need to restrict <span class="math notranslate nohighlight">\(-1&lt;\theta_1&lt;1\)</span>.
For MA(2) we have <span class="math notranslate nohighlight">\(-2&lt;\theta_1&lt;2\)</span>, <span class="math notranslate nohighlight">\(\theta_1+\theta_2&gt;-1\)</span> and
<span class="math notranslate nohighlight">\(\theta_1-\theta_2&lt;1\)</span>, this implies that we need to sample from a
triangle as shown in <a class="reference internal" href="#fig-ma2-triangle"><span class="std std-numref">Fig. 8.14</span></a>.</p>
<p>Combining the custom summary statistics and the identifiable
restrictions we have that the ABC model is specified as in Code Block
<a class="reference internal" href="#ma2-abc"><span class="std std-ref">MA2_abc</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="ma2-abc">
<div class="code-block-caption"><span class="caption-number">Listing 8.6 </span><span class="caption-text">MA2_abc</span><a class="headerlink" href="#ma2-abc" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m_ma2</span><span class="p">:</span>
    <span class="n">θ1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;θ1&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">θ2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;θ2&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s2">&quot;p1&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">θ1</span><span class="o">+</span><span class="n">θ2</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">))</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Potential</span><span class="p">(</span><span class="s2">&quot;p2&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">θ1</span><span class="o">-</span><span class="n">θ2</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">))</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">moving_average_2</span><span class="p">,</span> 
                     <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">θ1</span><span class="p">,</span> <span class="n">θ2</span><span class="p">],</span>
                     <span class="n">sum_stat</span><span class="o">=</span><span class="n">autocov</span><span class="p">,</span>
                     <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                     <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>

    <span class="n">trace_ma2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">3000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;ABC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">pm.Potential</span></code> is a way to incorporate arbitrary terms to a
(pseudo)-likelihood, without adding new variables to the model. It is
specially useful to introduce restrictions, like in this example. In
Code Block <a class="reference internal" href="#ma2-abc"><span class="std std-ref">MA2_abc</span></a> we sum 0 to the likelihood
if the first argument in <code class="docutils literal notranslate"><span class="pre">pm.math.switch</span></code> is true or <span class="math notranslate nohighlight">\(-\infty\)</span>
otherwise.</p>
<div class="figure align-default" id="fig-ma2-trace">
<a class="reference internal image-reference" href="../_images/ma2_trace.png"><img alt="../_images/ma2_trace.png" src="../_images/ma2_trace.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.13 </span><span class="caption-text">ABC trace plot for the MA(2) model. As expected the true parameters are
recovered and the rank plots look satisfactorily flat.</span><a class="headerlink" href="#fig-ma2-trace" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-ma2-triangle">
<a class="reference internal image-reference" href="../_images/ma2_triangle.png"><img alt="../_images/ma2_triangle.png" src="../_images/ma2_triangle.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.14 </span><span class="caption-text">ABC posterior for the MA(2) model as defined in Code Block
<a class="reference internal" href="#ma2-abc"><span class="std std-ref">MA2_abc</span></a>. In the center subplot the joint
posterior and in the margins the marginal distributions for <span class="math notranslate nohighlight">\(\theta1\)</span>
and <span class="math notranslate nohighlight">\(\theta2\)</span>. The gray triangle represents the prior distribution. The
mean is indicated with a black dot.</span><a class="headerlink" href="#fig-ma2-triangle" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="model-comparison-in-the-abc-context">
<span id="id37"></span><h2><span class="section-number">8.7. </span>Model Comparison in the ABC Context<a class="headerlink" href="#model-comparison-in-the-abc-context" title="Permalink to this headline">¶</a></h2>
<p>ABC methods are frequently used for model choice. While many methods
have been proposed <span id="id38">[<a class="reference internal" href="references.html#id50">96</a>, <a class="reference internal" href="references.html#id52">107</a>]</span>, here we will discuss
two approaches; Bayes factors, including a comparison with LOO, and
random forest <span id="id39">[<a class="reference internal" href="references.html#id51">99</a>]</span>.</p>
<p>As with parameter inference, the choice of summaries is of crucial
importance for model comparison. If we evaluate two or more models using
their predictions, we can not favour one model over the other if they
all make roughly the same predictions. The same reasoning can be applied
to model choice under ABC with summary statistics. If we use the mean as
the summary statistic, but models predict the same mean, then this
summary statistic will not be sufficient to discriminate between models.
We should take more time to think about what makes models different.</p>
<div class="section" id="marginal-likelihood-and-loo">
<span id="id40"></span><h3><span class="section-number">8.7.1. </span>Marginal Likelihood and LOO<a class="headerlink" href="#marginal-likelihood-and-loo" title="Permalink to this headline">¶</a></h3>
<p>One common quantity used to perform model comparison for ABC methods is
the marginal likelihood. Generally such comparison takes the form of a
ratio of marginal likelihoods, which is known as Bayes factors. If the
value of a Bayes factor is larger than 1, the model in the numerator is
preferred over the one in the denominator and vice versa. In Section <a class="reference internal" href="chp_11.html#bayes-factors"><span class="std std-ref">Marginal Likelihood and Model Comparison</span></a> we discuss more details
about Bayes factors, including their caveats. One such caveat is that
the marginal likelihood is generally difficult to compute. Fortunately,
SMC methods and by extension SMC-ABC methods are able to compute the
marginal likelihood as a byproduct of sampling. PyMC3’s SMC computes and
saves the log marginal likelihood in the trace. We can access its value
by doing <code class="docutils literal notranslate"><span class="pre">trace.report.log_marginal_likelihood</span></code>. As this value is in a
log scale, to compute a Bayes factor we can do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ml1</span> <span class="o">=</span> <span class="n">trace_1</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">log_marginal_likelihood</span>
<span class="n">ml2</span> <span class="o">=</span> <span class="n">trace_2</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">log_marginal_likelihood</span>
<span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">ml1</span> <span class="o">-</span> <span class="n">ml2</span><span class="p">)</span>
</pre></div>
</div>
<p>When using summary statistics the marginal likelihood computed from ABC
methods cannot be generally trusted to discriminate between competing
models <span id="id41">[<a class="reference internal" href="references.html#id61">108</a>]</span>, unless the summary statistics are sufficient for
model comparison. This is worrisome as, outside a few formal examples or
particular models, there is no general guide to ensure sufficiency
across models <span id="id42">[<a class="reference internal" href="references.html#id61">108</a>]</span>. This is not a problem if we use all the
data i.e. we do not rely on summary statistics <a class="footnote-reference brackets" href="#id64" id="id43">10</a>. This resembles our
discussion (see Section <a class="reference internal" href="chp_11.html#bayes-factors"><span class="std std-ref">Marginal Likelihood and Model Comparison</span></a>) about how computing
the marginal likelihood is generally a much more difficult problem that
computing the posterior. Even if we manage to find a summary statistic
that is good enough to compute a posterior, that is not a guarantee it
will be also useful for model comparison.</p>
<p>To better understand how the marginal likelihood behaves in the context
of ABC methods we will now analyze a short experiment. We also include
LOO, as we consider LOO an overall better metric than the marginal
likelihood and thus Bayes factors.</p>
<p>The basic setup of our experiment is to compare the values of the log
marginal likelihood and the values computed using LOO for models with an
explicit likelihood against values from ABC models using a simulator
with and without summary statistics. The results are shown in Figures
<a class="reference internal" href="#fig-model-comp-normal-0"><span class="std std-numref">Fig. 8.15</span></a> models in Code Blocks
<a class="reference internal" href="#gauss-nuts"><span class="std std-ref">gauss_nuts</span></a> and
<a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a>. The values of the marginal
(pseudo)likelihood are computed as by products of SMC and the values of
LOO using <code class="docutils literal notranslate"><span class="pre">az.loo()</span></code>. Notice that LOO is properly defined on the
pointwise log-likelihood values, but in ABC we only have access to the
pointwise log-<em>pseudo</em>likelihood values.</p>
<p>From <a class="reference internal" href="#fig-model-comp-normal-0"><span class="std std-numref">Fig. 8.15</span></a> we can see that in general both
LOO and the log marginal likelihood behave similarly. From the first
column we see that <code class="docutils literal notranslate"><span class="pre">model_1</span></code> is consistently chosen as better than
<code class="docutils literal notranslate"><span class="pre">model_0</span></code> (here higher is better). The difference between models (the
slopes) is larger for the log marginal likelihood than for LOO, this can
be explained as the computation of the marginal likelihood explicitly
takes the prior into account while LOO only does it indirectly through
the posterior (see Section <a class="reference internal" href="chp_11.html#bayes-factors"><span class="std std-ref">Marginal Likelihood and Model Comparison</span></a> for details). Even
when the values of LOO and the marginal likelihood vary across samples
they do it in a consistent way. We can see this from the slopes of the
lines connecting <code class="docutils literal notranslate"><span class="pre">model_0</span></code> and <code class="docutils literal notranslate"><span class="pre">model_1</span></code>. While the slopes of the lines
are not exactly the same, they are very similar. This is the ideal
behavior of a model selection method. We can reach similar conclusions
if we compare <code class="docutils literal notranslate"><span class="pre">model_1</span></code> and <code class="docutils literal notranslate"><span class="pre">model_2</span></code>. With the additional consideration
that both models are basically indistinguishable for LOO, while the
marginal likelihood reflects a larger difference. Once again the reason
is that LOO is computed just from the posterior while the marginal
likelihood directly takes the prior into account.</p>
<div class="figure align-default" id="fig-model-comp-normal-0">
<a class="reference internal image-reference" href="../_images/model_comp_normal_00.png"><img alt="../_images/model_comp_normal_00.png" src="../_images/model_comp_normal_00.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.15 </span><span class="caption-text">Model <code class="docutils literal notranslate"><span class="pre">m_0</span></code> is similar as the model described in Equation
<a class="reference internal" href="#equation-eq-gauss-model">(8.1)</a> but with <span class="math notranslate nohighlight">\(\sigma \sim \mathcal{HN}(0.1)\)</span>. <code class="docutils literal notranslate"><span class="pre">model_1</span></code>
the same as Equation <a class="reference internal" href="#equation-eq-gauss-model">(8.1)</a>. <code class="docutils literal notranslate"><span class="pre">model_2</span></code> is the same as
Equation <a class="reference internal" href="#equation-eq-gauss-model">(8.1)</a> but with <span class="math notranslate nohighlight">\(\sigma \sim \mathcal{HN}(10)\)</span>.
The first row corresponds to values of the log marginal likelihood and
the second row to values computed using LOO. Sequential Monte Carlo
<code class="docutils literal notranslate"><span class="pre">SMC</span></code>, SMC-ABC with the entire dataset <code class="docutils literal notranslate"><span class="pre">SMC-ABC</span></code>, SMC-ABC using the mean
as summary statistic <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sm</span></code> and finally SMC-ABC using the mean and
standard deviation <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sq</span></code>. We run 50 experiments each one with
sample size 50.</span><a class="headerlink" href="#fig-model-comp-normal-0" title="Permalink to this image">¶</a></p>
</div>
<p>The second column shows what happens when we move into the ABC realm. We
still get <code class="docutils literal notranslate"><span class="pre">model_1</span></code> as a better choice, but now the dispersion of
<code class="docutils literal notranslate"><span class="pre">model_0</span></code> is much larger than the one from <code class="docutils literal notranslate"><span class="pre">model_1</span></code> or <code class="docutils literal notranslate"><span class="pre">model_2</span></code>.
Additionally we now get lines crossing each other. Taken together, both
observations seems to indicate that we still can use LOO or the log
marginal likelihood to select the best model, but the values of the
relative weights, like the ones computed by <code class="docutils literal notranslate"><span class="pre">az.compare()</span></code> or the Bayes
factors will have larger variability.</p>
<p>The third column shows what happens when we use the mean as summary
statistic. Now model <code class="docutils literal notranslate"><span class="pre">model_0</span></code> and <code class="docutils literal notranslate"><span class="pre">model_1</span></code> seems to be on par and
<code class="docutils literal notranslate"><span class="pre">model_2</span></code> looks like a bad choice. It is almost like the specular image
of the previous column. This shows that when using an ABC method with
summary statistics the log marginal likelihood and LOO can fail to
provide a reasonable answer.</p>
<p>The fourth column shows what happens when we use the standard deviation
in addition to the mean as summary statistic. We see than we can
qualitatively recover the behavior observed when using ABC with the
entire dataset (second column).</p>
<div class="admonition-on-the-scale-of-the-pseudolikelihood admonition">
<p class="admonition-title">On the scale of the pseudolikelihood</p>
<p>Notice how the scale on the y-axes
is different, especially across columns. The reason is two-fold, first
when using ABC we are approximating the likelihood with a kernel
function scaled by <span class="math notranslate nohighlight">\(\epsilon\)</span>, second when using a summary statistic we
are decreasing the size of the data. Notice also that this size will
keep constant if we increase the sample size for summary statistics like
the mean or quantiles, i.e. the mean is a single number irrespective if
we compute it from 10 or 1000 observations.</p>
</div>
<p><a class="reference internal" href="#fig-model-comp-normal-forest"><span class="std std-numref">Fig. 8.16</span></a> can help us to understand what we
just discussed from <a class="reference internal" href="#fig-model-comp-normal-0"><span class="std std-numref">Fig. 8.15</span></a>. We recommend you
analyze both figures together by yourself. For the moment we will focus
on two observations. First, when performing <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sm</span></code> we have a
sufficient statistics for the mean but nothing to say about the
dispersion of the data, thus the posterior uncertainty of both
parameters <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">σ</span></code> is essentially controlled by the prior. See how
the estimates from <code class="docutils literal notranslate"><span class="pre">model_0</span></code> and <code class="docutils literal notranslate"><span class="pre">model_1</span></code> are very similar for <code class="docutils literal notranslate"><span class="pre">μ</span></code> and
the uncertainty from <code class="docutils literal notranslate"><span class="pre">model_2</span></code> is ridiculously large. Second, and
regarding the parameter <code class="docutils literal notranslate"><span class="pre">σ</span></code> the uncertainty is very small for <code class="docutils literal notranslate"><span class="pre">model_0</span></code>,
wider that it should be for <code class="docutils literal notranslate"><span class="pre">model_1</span></code> and ridiculously large for
<code class="docutils literal notranslate"><span class="pre">model_2</span></code>. Taken all together we can see why the log marginal likelihood
and LOO indicate that <code class="docutils literal notranslate"><span class="pre">model_0</span></code> and <code class="docutils literal notranslate"><span class="pre">model_1</span></code> are on par but <code class="docutils literal notranslate"><span class="pre">model_2</span></code>
is very different. Basically, <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sm</span></code> is failing to provide a good
fit! Once we see this is no longer surprising that the log marginal
likelihood and LOO computed from <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sm</span></code> contradicts what is
observed when we use <code class="docutils literal notranslate"><span class="pre">SMC</span></code> or <code class="docutils literal notranslate"><span class="pre">SMC-ABC</span></code>. If we use the mean and the
standard deviation <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sq</span></code> as summary statistics we partially
recover the behavior of using the whole data-set <code class="docutils literal notranslate"><span class="pre">SMC-ABC</span></code>.</p>
<div class="figure align-default" id="fig-model-comp-normal-forest">
<a class="reference internal image-reference" href="../_images/model_comp_normal_forest.png"><img alt="../_images/model_comp_normal_forest.png" src="../_images/model_comp_normal_forest.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.16 </span><span class="caption-text">Model <code class="docutils literal notranslate"><span class="pre">m_0</span></code> is similar as the model described in Equation
<a class="reference internal" href="#equation-eq-gauss-model">(8.1)</a> but with <span class="math notranslate nohighlight">\(\sigma \sim \mathcal{HN}(0.1)\)</span>. <code class="docutils literal notranslate"><span class="pre">model_1</span></code>
the same as Equation <a class="reference internal" href="#equation-eq-gauss-model">(8.1)</a>. <code class="docutils literal notranslate"><span class="pre">model_2</span></code> is the same as
Equation <a class="reference internal" href="#equation-eq-gauss-model">(8.1)</a> but with <span class="math notranslate nohighlight">\(\sigma \sim \mathcal{HN}(10)\)</span>.
The first row contains the values of the marginal likelihood and the
second the values of LOO. The column represents different methods of
computing these values. Sequential Monte Carlo <code class="docutils literal notranslate"><span class="pre">SMC</span></code>, SMC-ABC with the
entire dataset <code class="docutils literal notranslate"><span class="pre">SMC-ABC</span></code>, SMC-ABC using the mean as summary statistic
<code class="docutils literal notranslate"><span class="pre">SMC-ABC_sm</span></code> and finally SMC-ABC using the mean and standard deviation
<code class="docutils literal notranslate"><span class="pre">SMC-ABC_sq</span></code>. We run 50 experiments each one with sample size 50.</span><a class="headerlink" href="#fig-model-comp-normal-forest" title="Permalink to this image">¶</a></p>
</div>
<p>Figures <a class="reference internal" href="#fig-model-comp-pois-geom-0"><span class="std std-numref">Fig. 8.17</span></a> and
<a class="reference internal" href="#fig-model-comp-pois-geom-forest"><span class="std std-numref">Fig. 8.18</span></a> shows a similar analysis but
<code class="docutils literal notranslate"><span class="pre">model_0</span></code> is a geometric model and <code class="docutils literal notranslate"><span class="pre">model_1</span></code> is a Poisson model. The
data follows a shifted Poisson distribution
<span class="math notranslate nohighlight">\(\mu \sim 1 + \text{Pois}(2.5)\)</span>. We leave the analysis of these figures
as an exercise for the readers.</p>
<div class="figure align-default" id="fig-model-comp-pois-geom-0">
<a class="reference internal image-reference" href="../_images/model_comp_pois_geom_00.png"><img alt="../_images/model_comp_pois_geom_00.png" src="../_images/model_comp_pois_geom_00.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.17 </span><span class="caption-text">Model <code class="docutils literal notranslate"><span class="pre">m_0</span></code> is a geometric distribution with prior
<span class="math notranslate nohighlight">\(p \sim \mathcal{U}(0, 1)\)</span> and <code class="docutils literal notranslate"><span class="pre">model_1</span></code> is a Poisson distribution with
prior <span class="math notranslate nohighlight">\(\mu \sim \mathcal{E}(1)\)</span>. The data follows a shifted Poisson
distribution <span class="math notranslate nohighlight">\(\mu \sim 1 + \text{Pois}(2.5)\)</span>. Sequential Monte Carlo
<code class="docutils literal notranslate"><span class="pre">SMC</span></code>, SMC-ABC with the entire dataset <code class="docutils literal notranslate"><span class="pre">SMC-ABC</span></code>, SMC-ABC using the mean
as summary statistic <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sm</span></code> and finally SMC-ABC using the mean and
standard deviation <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sq</span></code>.We run 50 experiments each one with
sample size 50.</span><a class="headerlink" href="#fig-model-comp-pois-geom-0" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-model-comp-pois-geom-forest">
<a class="reference internal image-reference" href="../_images/model_comp_pois_geom_forest.png"><img alt="../_images/model_comp_pois_geom_forest.png" src="../_images/model_comp_pois_geom_forest.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 8.18 </span><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">model_0</span></code> a geometric model/simulator with prior
<span class="math notranslate nohighlight">\(p \sim \mathcal{U}(0, 1)\)</span> <code class="docutils literal notranslate"><span class="pre">model_1</span></code> A Poisson model/simulator with
prior <span class="math notranslate nohighlight">\(p \sim \text{Expo}(1)\)</span>. The firs row contains the values of the
marginal likelihood and the second the values of LOO. The column
represents different methods of computing these values. Sequential Monte
Carlo <code class="docutils literal notranslate"><span class="pre">SMC</span></code>, SMC-ABC with the entire dataset <code class="docutils literal notranslate"><span class="pre">SMC-ABC</span></code>, SMC-ABC using
the mean as summary statistic <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sq</span></code> and finally SMC-ABC using
quartiles <code class="docutils literal notranslate"><span class="pre">SMC-ABC_sq</span></code>. We run 50 experiments each one with sample size
50.</span><a class="headerlink" href="#fig-model-comp-pois-geom-forest" title="Permalink to this image">¶</a></p>
</div>
<p>In the ABC literature it is common to use Bayes factors in an attempt to
assign relative probabilities to models. We understand this can be
perceived as valuable in certain fields. So we want to warn those
practitioners about the potential problems of this practice under the
ABC framework, especially because it is much more common to use summary
statistics than not. Model comparison can still be useful, mainly if a
more exploratory approach is adopted together with model criticism
performed before model comparison to improve or discard clearly
misspecified models. This is the general approach we have adopted in
this book for non-ABC methods so we consider natural to extend it into
the ABC framework as well. In this book we also favor LOO over the
marginal likelihood, while research about the benefits and drawbacks of
LOO for ABC methods are currently lacking, we consider LOO to be
potentially useful for ABC methods too. Stay tuned for future news!</p>
<div class="admonition-model-criticism-and-model-comparison admonition">
<p class="admonition-title">Model criticism and model comparison</p>
<p>While some amount of misspecification is always expected, and model comparison can help to
better understand models and their misspecification. Model comparison
should be done only after we have shown the models provide a reasonable
fit to the data. It does not make too much sense to compare models that
are clearly a bad fit.</p>
</div>
</div>
<div class="section" id="model-choice-via-random-forest">
<span id="id44"></span><h3><span class="section-number">8.7.2. </span>Model Choice via Random Forest<a class="headerlink" href="#model-choice-via-random-forest" title="Permalink to this headline">¶</a></h3>
<p>The caveats we discussed in the previous section has motivated the
research of new methods for model choice under the ABC framework. One
such alternative method reframes the model selection problem as random
forest classification problem <span id="id45">[<a class="reference internal" href="references.html#id51">99</a>]</span> <a class="footnote-reference brackets" href="#id65" id="id46">11</a>. A random forest is a
method for classification and regression based on the combination of
many decision trees and it is closely related to BARTs from Chapter
<a class="reference internal" href="chp_07.html#chap6"><span class="std std-ref">7</span></a>.</p>
<p>The main idea of this method is that the most probable model can be
obtained by constructing a random forest classifier from simulations
from the prior or posterior predictive distributions. In the original
paper authors use the prior predictive distribution but mention that for
more advanced ABC method other distributions can be used too. Here we
will use the posterior predictive distribution. For each model up to <span class="math notranslate nohighlight">\(m\)</span> models, the simulations are
ordered in a reference table (see <a class="reference internal" href="#table-abc-random-forest-ref-table"><span class="std std-numref">Table 8.1</span></a>).
where each row is a sample
from the posterior predictive distribution and each column is one out of
<span class="math notranslate nohighlight">\(n\)</span> summary statistics. We use this reference table to train the
classifier, the task is to correctly classify the models given the
values of the summary statistics. It is important to note that the
summary statistics used for model choice does not need to be the same
used to compute the posterior. In fact it is recommended to include many
summary statistics. Once the classifier is trained we feed it with the
same <span class="math notranslate nohighlight">\(n\)</span> summary statistics we used in the reference table, but this
time applied to our observed data. The predicted model by the classifier
will be our best model.</p>
<table class="table" id="table-abc-random-forest-ref-table">
<caption><span class="caption-number">Table 8.1 </span><span class="caption-text">Reference table</span><a class="headerlink" href="#table-abc-random-forest-ref-table" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>Model</strong></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{S^{0}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{S^{1}}\)</span></p></td>
<td><p>…</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{S^{n}}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td></td>
<td></td>
<td><p>…</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>0</p></td>
<td></td>
<td></td>
<td><p>…</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td></td>
<td></td>
<td><p>…</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td></td>
<td></td>
<td><p>…</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>m</p></td>
<td></td>
<td></td>
<td><p>…</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>Additionally, we can also compute an approximated posterior probability
for the <em>best</em> model, relative to the rest of the models. Once again,
this can be done using a random forest, but this time we use a
regression, with the misclassification error rate as response variable
and the summary statistics in the reference table as independent
variables <span id="id47">[<a class="reference internal" href="references.html#id51">99</a>]</span>.</p>
</div>
<div class="section" id="model-choice-for-ma-model">
<span id="id48"></span><h3><span class="section-number">8.7.3. </span>Model Choice for MA Model<a class="headerlink" href="#model-choice-for-ma-model" title="Permalink to this headline">¶</a></h3>
<p>Let us go back to the moving average example, this time we will focus on
the following question. Is a MA(1) or MA(2) a better choice? To answer
this question we will use LOO (based on the pointwise pseudo-likelihood
values) and random forest. The MA(1) models looks like this</p>
<div class="literal-block-wrapper docutils container" id="ma1-abc">
<div class="code-block-caption"><span class="caption-number">Listing 8.7 </span><span class="caption-text">MA1_abc</span><a class="headerlink" href="#ma1-abc" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m_ma1</span><span class="p">:</span>
    <span class="n">θ1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;θ1&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">moving_average_1</span><span class="p">,</span>
                     <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="n">θ1</span><span class="p">],</span> <span class="n">sum_stat</span><span class="o">=</span><span class="n">autocov</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">trace_ma1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_smc</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;ABC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In order to compare ABC-models using LOO. We cannot directly use the
function <code class="docutils literal notranslate"><span class="pre">az.compare</span></code>. We first to need to create an <code class="docutils literal notranslate"><span class="pre">InferenceData</span></code>
object with a <code class="docutils literal notranslate"><span class="pre">log_likelihood</span></code> group as detailed in Code Block
<a class="reference internal" href="#idata-pseudo"><span class="std std-ref">idata_pseudo</span></a> <a class="footnote-reference brackets" href="#id66" id="id49">12</a>. The result of this
comparison is summarized in <a class="reference internal" href="#table-abc-loo"><span class="std std-numref">Table 8.2</span></a>. As
expected, we can see that the MA(2) model is preferred.</p>
<div class="literal-block-wrapper docutils container" id="idata-pseudo">
<div class="code-block-caption"><span class="caption-number">Listing 8.8 </span><span class="caption-text">idata_pseudo</span><a class="headerlink" href="#idata-pseudo" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">idata_ma1</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace_ma1</span><span class="p">)</span>
<span class="n">lpll</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;s&quot;</span><span class="p">:</span> <span class="n">trace_ma2</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">log_pseudolikelihood</span><span class="p">}</span>
<span class="n">idata_ma1</span><span class="o">.</span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">dict_to_dataset</span><span class="p">(</span><span class="n">lpll</span><span class="p">)</span>

<span class="n">idata_ma2</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace_ma2</span><span class="p">)</span>
<span class="n">lpll</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;s&quot;</span><span class="p">:</span> <span class="n">trace_ma2</span><span class="o">.</span><span class="n">report</span><span class="o">.</span><span class="n">log_pseudolikelihood</span><span class="p">}</span>
<span class="n">idata_ma2</span><span class="o">.</span><span class="n">log_likelihood</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">base</span><span class="o">.</span><span class="n">dict_to_dataset</span><span class="p">(</span><span class="n">lpll</span><span class="p">)</span>

<span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">({</span><span class="s2">&quot;m_ma1&quot;</span><span class="p">:</span><span class="n">idata_ma1</span><span class="p">,</span> <span class="s2">&quot;m_ma2&quot;</span><span class="p">:</span><span class="n">idata_ma2</span><span class="p">})</span>
</pre></div>
</div>
</div>
<table class="table" id="table-abc-loo">
<caption><span class="caption-number">Table 8.2 </span><span class="caption-text">Summary ABC-model comparison using LOO</span><a class="headerlink" href="#table-abc-loo" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>rank</strong></p></td>
<td><p><strong>loo</strong></p></td>
<td><p><strong>p_loo</strong></p></td>
<td><p><strong>d_loo</strong></p></td>
<td><p><strong>weight</strong></p></td>
<td><p><strong>se</strong></p></td>
<td><p><strong>dse</strong></p></td>
<td><p><strong>warning</strong></p></td>
<td><p><strong>loo_scale</strong></p></td>
</tr>
<tr class="row-even"><td><p>model_ma2</p></td>
<td><p>0</p></td>
<td><p>-2.22</p></td>
<td><p>1.52</p></td>
<td><p>0.00</p></td>
<td><p>1.0</p></td>
<td><p>0.08</p></td>
<td><p>0.00</p></td>
<td><p>False</p></td>
<td><p>log</p></td>
</tr>
<tr class="row-odd"><td><p>model_ma1</p></td>
<td><p>1</p></td>
<td><p>-3.53</p></td>
<td><p>2.04</p></td>
<td><p>1.31</p></td>
<td><p>0.0</p></td>
<td><p>1.50</p></td>
<td><p>1.43</p></td>
<td><p>False</p></td>
<td><p>log</p></td>
</tr>
</tbody>
</table>
<p>To use the random forest method we can use the <code class="docutils literal notranslate"><span class="pre">select_model</span></code> function
included in the accompanying code for this book. To make this function
work we need to pass a list of tuples with the PyMC3’s model names and
traces, a list of summary statistics, and the observed data. Here as
summary statistics we will use the first six auto-correlations. We
choose these particular summary statistics for two reasons, first to
show that we can use a set of summary statistics different from the one
used to fit the data and second two show that we can mix useful summary
statistics (the first two auto-correlations), with not very useful ones
(the rest). Remember that theory says that for a MA(q) processes there
are at most q auto-correlations. For complex problems, like those from
population genetics it is not uncommon to use several hundreds or even
tens of thousands of summary statistics <span id="id50">[<a class="reference internal" href="references.html#id64">109</a>]</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="n">select_model</span><span class="p">([(</span><span class="n">m_ma1</span><span class="p">,</span> <span class="n">trace_ma1</span><span class="p">),</span> <span class="p">(</span><span class="n">m_ma2</span><span class="p">,</span> <span class="n">trace_ma2</span><span class="p">)],</span>
             <span class="n">statistics</span><span class="o">=</span><span class="p">[</span><span class="n">partial</span><span class="p">(</span><span class="n">autocov</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)],</span>
             <span class="n">n_samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
             <span class="n">observations</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">select_model</span></code> returns the index of the best model (starting from 0) and
the estimated posterior probability for that model. For our example we
get model 0 with a probability of 0.68. It is a little bit reassuring
that, at least for this example, both LOO and the random forest method
agree on model choice and even their relative weight.</p>
</div>
</div>
<div class="section" id="choosing-priors-for-abc">
<span id="id51"></span><h2><span class="section-number">8.8. </span>Choosing Priors for ABC<a class="headerlink" href="#choosing-priors-for-abc" title="Permalink to this headline">¶</a></h2>
<p>Not having a closed form likelihood makes it more difficult to get good
models and thus ABC methods are in general more brittle than other
approximations. Consequently we should be extra careful about modeling
choices, including prior elicitation, and more thorough about model
evaluation than when we have an explicit likelihood. These are the costs
we pay for approximating the likelihood.</p>
<p>A more careful prior elicitation can be much more rewarding with ABC
methods than with other approaches. If we lose information by
approximating the likelihood we can maybe partially compensate that loss
by using a more informative prior. Additionally, better priors will
generally save us from wasting computational resources and time. For ABC
rejection methods, where we use the prior as the sampling distribution,
this is more or less evident. But it is also the case for SMC methods,
specifically if the simulator are sensitive to the input parameter. For
example, when using ABC to inference a ordinary differential equation,
some parameter combination could be numerical challenging to simulate,
resulting in extremely slow simulation. Another problem of using vague
prior arise during the weighted sampling in SMC and SMC-ABC, as almost
all but few samples from the prior would have extremely small weights
when evaluated on the tempered posteriors. This leads to the SMC
particles to become singular after just a few steps (as only few samples
with large weight are selected). This phenomenon is called weight
collapse, a well known issue for particle methods <span id="id52">[<a class="reference internal" href="references.html#id143">110</a>]</span>.
Good priors can help to reduce the computational cost and thus to some
extent allow us to fit more complex models when we are using SMC and
SMC-ABC. Beyond the general advice of more informative prior and what we
have already discussed elsewhere in the book about prior
elicitation/evaluation, we do not have further recommendation specific
for ABC methods.</p>
</div>
<div class="section" id="exercises">
<span id="exercises8"></span><h2><span class="section-number">8.9. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>8E1.</strong> In your words explain how ABC is approximate? What
object or quantity is approximated and how.</p>
<p><strong>8E2.</strong> In the context of ABC, what is the problem that SMC
is trying to solve compared to rejection sampling?</p>
<p><strong>8E3.</strong> Write a Python function to compute the Gaussian
kernel as in Equation <a class="reference internal" href="#equation-eq-euclidean-abc">(8.2)</a>, but without the summation.
Generate two random samples of size 100 from the same distribution. Use
the implemented function to compute the distances between those two
random samples. You will get two distributions each of size 100. Show
the differences using a KDE plot, the mean and the standard deviation.</p>
<p><strong>8E4.</strong> What do you expect to the results to be in terms of
accuracy and convergence of the sampler if in model <code class="docutils literal notranslate"><span class="pre">gauss</span></code> model from
Code Block <a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> we would have used
<code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;identity&quot;</span></code>. Justify.</p>
<p><strong>8E5.</strong> Refit the <code class="docutils literal notranslate"><span class="pre">gauss</span></code> model from Code Block
<a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> using <code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;identity&quot;</span></code>.
Evaluate the results using:</p>
<ol class="simple">
<li><p>Trace Plot</p></li>
<li><p>Rank Plot</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat R\)</span></p></li>
<li><p>The mean and HDI for the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p></li>
</ol>
<p>Compare the results with those from the example in the book (i.e. using
<code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;sort&quot;</span></code>).</p>
<p><strong>8E6.</strong> Refit the <code class="docutils literal notranslate"><span class="pre">gauss</span></code> model from Code Block
<a class="reference internal" href="#gauss-abc"><span class="std std-ref">gauss_abc</span></a> using quintiles as summary
statistics.</p>
<ol class="simple">
<li><p>How the results compare with the example in the book?</p></li>
<li><p>Try other values for <code class="docutils literal notranslate"><span class="pre">epsilon</span></code>. Is 1 a good choice?</p></li>
</ol>
<p><strong>8E7.</strong> Use the <code class="docutils literal notranslate"><span class="pre">g_and_k_quantile</span></code> class to generate a
sample (n=500) from a g-and-k distribution with parameters
a=0,b=1,g=0.4,k=0. Then use the <code class="docutils literal notranslate"><span class="pre">gkm</span></code> model to fit it using 3 different
values of <span class="math notranslate nohighlight">\(\epsilon\)</span> (0.05, 0.1, 0.5). Which value of <span class="math notranslate nohighlight">\(\epsilon\)</span> do you
think is the best for this problem? Use diagnostics tools to help you
answer this question.</p>
<p><strong>8E8.</strong> Use the sample from the previous exercise and the
<code class="docutils literal notranslate"><span class="pre">gkm</span></code> model. Fit the using the summary statistics <code class="docutils literal notranslate"><span class="pre">octo_summary</span></code>, the
<code class="docutils literal notranslate"><span class="pre">octile-vector</span></code> (i.e. the quantiles 0.125, 0.25, 0.375, 0.5, 0.625,
0.75, 0.875) and <code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;sorted&quot;</span></code>. Compare the results with the known
parameter values, which option provides higher accuracy and lower
uncertainty?</p>
<p><strong>8M9.</strong> In the GitHub repository you will find a dataset of
the distribution of citations of scientific papers. Use SMC-ABC to fit a
g-and-k distribution to this dataset. Perform all the necessary steps to
find a suitable value for <code class="docutils literal notranslate"><span class="pre">&quot;epsilon&quot;</span></code> and ensuring the model converge
and results provides a suitable fit.</p>
<p><strong>8M10.</strong> The Lotka-Volterra is well-know biological model
describing how the number of individuals of two species change when
there is a predator-prey interaction <span id="id53">[<a class="reference internal" href="references.html#id174">111</a>]</span>. Basically, as the
population of prey increase there is more food for the predator which
leads to an increase in the predator population. But a large number of
predators produce a decline in the number of pray which in turn produce
a decline in the predator as food becomes scarce. Under certain
conditions this leads to an stable cyclic pattern for both populations.
In the GitHub repository you will find a Lotka-Volterra simulator with
unknown parameters and the data set <code class="docutils literal notranslate"><span class="pre">Lotka-Volterra_00</span></code>. Assume the
unknown parameters are positive. Use a SMC-ABC model to find the
posterior distribution of the parameters.</p>
<p><strong>8H11.</strong> Following with the Lotka-Volterra example. The
dataset <code class="docutils literal notranslate"><span class="pre">Lotka-Volterra_01</span></code> includes data for a predator prey with the
twist that at some point a disease suddenly decimate the prey
population. Expand the model to allow for a “switchpoint”, i.e. a point
that marks two different predator-prey dynamics (and hence two different
set of parameters).</p>
<p><strong>8H12.</strong> This exercise is based in the sock problem
formulated by Rasmus Bååth. The problem goes like this. We get 11 socks
out of the laundry and to our surprise we find that they are all unique,
that is we can not pair them. What is the total number of socks that we
laundry? Let assume that the laundry contains both paired and unpaired
socks, we do not have more than two socks of the same kind. That is we
either have 1 or 2 socks of each kind.</p>
<p>Assume the number of socks follows a <span class="math notranslate nohighlight">\(\text{NB}(30, 4.5)\)</span>. And that the
proportion of unpaired socks follows a <span class="math notranslate nohighlight">\(\text{Beta}(15, 2)\)</span></p>
<p>Generate a simulator suitable for this problem and create a SMC-ABC
model to compute the posterior distribution of the number of socks, the
proportion of unpaired socks, and the number of pairs.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>It can work for discrete variables, especially if they take only a
few possible values.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>This is another manifestation of the curse of dimensionality. See
Section <a class="reference internal" href="chp_11.html#high-dimensions"><span class="std std-ref">Moving out of Flatland</span></a> for a full explanation.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id9">3</a></span></dt>
<dd><p>The default SMC <code class="docutils literal notranslate"><span class="pre">kernel</span></code> is <code class="docutils literal notranslate"><span class="pre">&quot;metropolis&quot;</span></code>. See <a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">Inference Methods</span></a> for details.</p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id12">4</a></span></dt>
<dd><p>Is similar to the Gaussian distribution but without the
normalization term <span class="math notranslate nohighlight">\(\frac{1}{\sigma\sqrt{2\pi}}\)</span>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id13">5</a></span></dt>
<dd><p>This is something PyMC3 does, other packages could be different</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id15">6</a></span></dt>
<dd><p>Even when PyMC3 uses <code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;sort&quot;</span></code> as summary statistic,
sorting is not a true summary as we are still using the whole data</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id20">7</a></span></dt>
<dd><p>In a similar fashion as the <span class="math notranslate nohighlight">\(\beta\)</span> parameters in the description
of the SMC/SMC-ABC algorithm explained before</p>
</dd>
<dt class="label" id="id61"><span class="brackets"><a class="fn-backref" href="#id29">8</a></span></dt>
<dd><p>In Prangle <span id="id62">[<a class="reference internal" href="references.html#id45">102</a>]</span> you will find a description of an R
package with a lot of functions to work with g-and-k distributions.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id35">9</a></span></dt>
<dd><p>In the literature is common to use <span class="math notranslate nohighlight">\(\varepsilon\)</span> to denote these
terms, but we want to avoid confusion with the <span class="math notranslate nohighlight">\(\epsilon\)</span> parameter
in the SMC-ABC sampler</p>
</dd>
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id43">10</a></span></dt>
<dd><p>Good moment to remember that <code class="docutils literal notranslate"><span class="pre">sum_stat=&quot;sort&quot;</span></code> is not actually a
summary statistic as we are using the entire dataset</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id46">11</a></span></dt>
<dd><p>Other classifiers could have been chosen, but the authors decided
to use a random forest.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id49">12</a></span></dt>
<dd><p>In future versions of PyMC <code class="docutils literal notranslate"><span class="pre">pm.sample_smc</span></code> will return and
InferenceData object with the proper groups.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_07.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7. </span>Bayesian Additive Regression Trees</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="chp_09.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">9. </span>End to End Bayesian Workflows</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Martin, Kumar, Lao<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>