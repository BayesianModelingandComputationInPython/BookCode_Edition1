
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3. Linear Models and Probabilistic Programming Languages &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Extending Linear Models" href="chp_04.html" />
    <link rel="prev" title="2. Exploratory Analysis of Bayesian Models" href="chp_02.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_03.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparing-two-or-more-groups">
   3.1. Comparing Two (or More) Groups
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-two-ppls">
     3.1.1. Comparing Two PPLs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   3.2. Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-penguins">
     3.2.1. Linear Penguins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictions">
     3.2.2. Predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering">
     3.2.3. Centering
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression">
   3.3. Multiple Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#counterfactuals">
     3.3.1. Counterfactuals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-linear-models">
   3.4. Generalized Linear Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression">
     3.4.1. Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classifying-penguins">
     3.4.2. Classifying Penguins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-log-odds">
     3.4.3. Interpreting Log Odds
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#picking-priors-in-regression-models">
   3.5. Picking Priors in Regression Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   3.6. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear Models and Probabilistic Programming Languages</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparing-two-or-more-groups">
   3.1. Comparing Two (or More) Groups
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparing-two-ppls">
     3.1.1. Comparing Two PPLs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-regression">
   3.2. Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-penguins">
     3.2.1. Linear Penguins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictions">
     3.2.2. Predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#centering">
     3.2.3. Centering
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiple-linear-regression">
   3.3. Multiple Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#counterfactuals">
     3.3.1. Counterfactuals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-linear-models">
   3.4. Generalized Linear Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression">
     3.4.1. Logistic Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classifying-penguins">
     3.4.2. Classifying Penguins
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-log-odds">
     3.4.3. Interpreting Log Odds
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#picking-priors-in-regression-models">
   3.5. Picking Priors in Regression Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   3.6. Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="linear-models-and-probabilistic-programming-languages">
<span id="chap2"></span><h1><span class="section-number">3. </span>Linear Models and Probabilistic Programming Languages<a class="headerlink" href="#linear-models-and-probabilistic-programming-languages" title="Permalink to this headline">¶</a></h1>
<p>With the advent of Probabilistic Programming Languages, modern Bayesian
modeling can be as simple as coding a model and “pressing a button”.
However, effective model building and analysis usually takes more work.
As we progress through this book we will be building many different
types of models but in this chapter we will start with the humble linear
model. Linear models are a broad class of models where the expected
value of a given observation is the linear combination of the associated
predictors. A strong understanding of how to fit and interpret linear
models is a strong foundation for the models that will follow. This will
also help us to consolidate the fundamentals of Bayesian inference
(Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a>) and exploratory analysis of Bayesian models
(Chapter <a class="reference internal" href="chp_02.html#chap1bis"><span class="std std-ref">2</span></a>) and apply them with different PPLs.
This chapter introduces the two PPLs we will use for the majority of
this book, PyMC3, which you have briefly seen, as well as TensorFlow
Probability (TFP). While we are building models in these two PPLs,
focusing on how the same underlying statistical ideas are mapped to
implementation in each PPL. We will first fit an intercept only model,
that is a model with no covariates, and then we will add extra
complexity by adding one or more covariates, and extend to generalized
linear models. By the end of this chapter you will be more comfortable
with linear models, more familiar with many of the steps in a Bayesian
workflow, and more comfortable conducting Bayesian workflows with PyMC3,
TFP and ArviZ.</p>
<div class="section" id="comparing-two-or-more-groups">
<span id="comparing-distributions"></span><h2><span class="section-number">3.1. </span>Comparing Two (or More) Groups<a class="headerlink" href="#comparing-two-or-more-groups" title="Permalink to this headline">¶</a></h2>
<p>If you are looking for something to compare it is hard to beat penguins.
After all, what is not to like about these cute flightless birds? Our
first question may be “What is the average mass of each penguin
species?”, or may be “How different are those averages?”, or in
statistics parlance “What is the dispersion of the average?” Luckily
Kristen Gorman also likes studying penguins, so much so that she visited
3 Antarctic islands and collected data about Adelie, Gentoo and
Chinstrap species, which is compiled into the Palmer Penguins
dataset <span id="id1">[<a class="reference internal" href="references.html#id58" title="Allison Marie Horst, Alison Presmanes Hill, and Kristen B Gorman. palmerpenguins: Palmer Archipelago (Antarctica) penguin data. 2020. R package version 0.1.0. URL: https://allisonhorst.github.io/palmerpenguins/, doi:10.5281/zenodo.3960218.">28</a>]</span>. The observations consist of physical
characteristics of the penguin mass, flipper length, and sex, as well as
geographic characteristics such as the island they reside on.</p>
<p>We start by loading the data and filtering out any rows where data is
missing in Code Block <a class="reference internal" href="#penguin-load"><span class="std std-ref">penguin_load</span></a>. This
is called a complete case analysis where, as the name suggests, we only
use the rows where all observations are present. While it is possible to
handle the missing values in another way, either through data
imputation, or imputation during modeling, we will opt to take the
simplest approach for this chapter.</p>
<div class="literal-block-wrapper docutils container" id="penguin-load">
<div class="code-block-caption"><span class="caption-number">Listing 3.1 </span><span class="caption-text">penguin_load</span><a class="headerlink" href="#penguin-load" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/penguins.csv&quot;</span><span class="p">)</span>
<span class="c1"># Subset to the columns needed</span>
<span class="n">missing_data</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">isnull</span><span class="p">()[</span>
    <span class="p">[</span><span class="s2">&quot;bill_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;sex&quot;</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span>
<span class="p">]</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># Drop rows with any missing data</span>
<span class="n">penguins</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="o">~</span><span class="n">missing_data</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>We can then calculate the empirical mean of the mass <code class="docutils literal notranslate"><span class="pre">body_mass_g</span></code> in
Code Block
<a class="reference internal" href="#penguin-mass-empirical"><span class="std std-ref">penguin_mass_empirical</span></a> with just
a little bit of code, the results of which are in <a class="reference internal" href="#tab-penguin-mass-parameters-point-estimates"><span class="std std-numref">Table 3.1</span></a></p>
<div class="literal-block-wrapper docutils container" id="penguin-mass-empirical">
<div class="code-block-caption"><span class="caption-number">Listing 3.2 </span><span class="caption-text">penguin_mass_empirical</span><a class="headerlink" href="#penguin-mass-empirical" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">summary_stats</span> <span class="o">=</span> <span class="p">(</span><span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]]</span>
                         <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;species&quot;</span><span class="p">)</span>
                         <span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">,</span> <span class="s2">&quot;count&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<table class="table" id="tab-penguin-mass-parameters-point-estimates">
<caption><span class="caption-number">Table 3.1 </span><span class="caption-text">Empirical mean and standard deviation of penguin mass. The count column indicates the observed number of penguins per species.</span><a class="headerlink" href="#tab-penguin-mass-parameters-point-estimates" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>species</strong></p></td>
<td><p><strong>mean (grams)</strong></p></td>
<td><p><strong>std (grams)</strong></p></td>
<td><p><strong>count</strong></p></td>
</tr>
<tr class="row-even"><td><p><strong>Adelie</strong></p></td>
<td><p>3706</p></td>
<td><p>459</p></td>
<td><p>146</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Chinstrap</strong></p></td>
<td><p>3733</p></td>
<td><p>384</p></td>
<td><p>68</p></td>
</tr>
<tr class="row-even"><td><p><strong>Gentoo</strong></p></td>
<td><p>5092</p></td>
<td><p>501</p></td>
<td><p>119</p></td>
</tr>
</tbody>
</table>
<p>Now we have point estimates for both the mean and the dispersion, but we
do not know the uncertainty of those statistics. One way to get
estimates of uncertainty is by using Bayesian methods. In order to do so
we need to conjecture a relationship of observations to parameters as
example:</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-bayes">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-eq-gaussian-bayes" title="Permalink to this equation">¶</a></span>\[\overbrace{p(\mu, \sigma \mid Y)}^{Posterior} \propto \overbrace{\mathcal{N}(Y \mid \mu, \sigma)}^{Likelihood}\;  \overbrace{\underbrace{\mathcal{N}(4000, 3000)}_{\mu}
     \underbrace{\mathcal{H}\text{T}(100, 2000)}_{\sigma}}^{Prior}\]</div>
<p>Equation <a class="reference internal" href="#equation-eq-gaussian-bayes">(3.1)</a> is a restatement of Equation
<a class="reference internal" href="chp_01.html#equation-eq-proportional-bayes">(1.3)</a> where each parameter
is explicitly listed. Since we have no specific reason to choose an
informative prior, we will use wide priors for both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.
In this case, the priors are chosen based on the empirical mean and
standard deviation of the observed data. And lastly instead of
estimating the mass of all species we will first start with the mass of
the Adelie penguin species. A Gaussian is a reasonable choice of
likelihood for penguin mass and biological mass in general, so we will
go with it. Let us translate Equation <a class="reference internal" href="#equation-eq-gaussian-bayes">(3.1)</a> into a
computational model.</p>
<div class="literal-block-wrapper docutils container" id="penguin-mass">
<div class="code-block-caption"><span class="caption-number">Listing 3.3 </span><span class="caption-text">penguin_mass</span><a class="headerlink" href="#penguin-mass" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">adelie_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s2">&quot;species&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;Adelie&quot;</span><span class="p">)</span>
<span class="n">adelie_mass_obs</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">adelie_mask</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_adelie_penguin_mass</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="mi">4000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">mass</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mass&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">adelie_mass_obs</span><span class="p">)</span>

    <span class="n">prior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">(</span><span class="n">samples</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">inf_data_adelie_penguin_mass</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Before computing the posterior we are going to check the prior. In
particular we are first checking that sampling from our model is
computationally feasible and that our choice of priors is reasonable
based on our domain knowledge. We plot the samples from the prior in
<a class="reference internal" href="#fig-singlespecies-prior-predictive"><span class="std std-numref">Fig. 3.1</span></a>. Since we can get a plot at
all we know our model has no “obvious” computational issues, such as
shape problems or mispecified random variables or likelihoods. From the
prior samples themselves it is evident we are not overly constraining
the possible penguin masses, we may in fact be under constraining the
prior as the prior for the mean of the mass includes negative values.
However, since this is a simple model and we have a decent number of
observations we will just note this aberration and move onto estimating
the posterior distribution.</p>
<div class="figure align-default" id="fig-singlespecies-prior-predictive">
<a class="reference internal image-reference" href="../_images/SingleSpecies_Prior_Predictive.png"><img alt="../_images/SingleSpecies_Prior_Predictive.png" src="../_images/SingleSpecies_Prior_Predictive.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Prior samples generated in Code Block
<a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a>. The distribution estimates
of both the mean and standard deviation for the mass distribution cover
a wide range of possibilities.</span><a class="headerlink" href="#fig-singlespecies-prior-predictive" title="Permalink to this image">¶</a></p>
</div>
<p>After sampling from our model, we can create
<a class="reference internal" href="#fig-single-penguins-rank-kde-plot"><span class="std std-numref">Fig. 3.2</span></a> which includes 4 subplots,
the two on the right are the rank plots and the left the KDE of each
parameter, one line for each chain. We also can reference the numerical
diagnostics in <a class="reference internal" href="#tab-penguin-mass-parameters-bayesian-estimates"><span class="std std-numref">Table 3.2</span></a> to confirm our
belief that the chains converged. Using the intuition we built in
Chapter <a class="reference internal" href="chp_02.html#chap1bis"><span class="std std-ref">2</span></a> we can judge that these fits are
acceptable and we will continue with our analysis.</p>
<table class="table" id="tab-penguin-mass-parameters-bayesian-estimates">
<caption><span class="caption-number">Table 3.2 </span><span class="caption-text">Bayesian estimates of the mean (μ) and standard deviation (σ) of Adelie penguin mass. Sampling diagnostics also included</span><a class="headerlink" href="#tab-penguin-mass-parameters-bayesian-estimates" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
<td><p><strong>mcse_mean</strong></p></td>
<td><p><strong>mcse_sd</strong></p></td>
<td><p><strong>ess_bulk</strong></p></td>
<td><p><strong>ess_tail</strong></p></td>
<td><p><strong>r_hat</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td><p>3707</p></td>
<td><p>38</p></td>
<td><p>3632</p></td>
<td><p>3772</p></td>
<td><p>0.6</p></td>
<td><p>0.4</p></td>
<td><p>3677.0</p></td>
<td><p>2754.0</p></td>
<td><p>1.0</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td><p>463</p></td>
<td><p>27</p></td>
<td><p>401</p></td>
<td><p>511</p></td>
<td><p>0.5</p></td>
<td><p>0.3</p></td>
<td><p>3553.0</p></td>
<td><p>2226.0</p></td>
<td><p>1.0</p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="fig-single-penguins-rank-kde-plot">
<a class="reference internal image-reference" href="../_images/SingleSpecies_KDE_RankPlot.png"><img alt="../_images/SingleSpecies_KDE_RankPlot.png" src="../_images/SingleSpecies_KDE_RankPlot.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.2 </span><span class="caption-text">KDE and rank plot of the posterior of the Bayesian model in Code Block
<a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a> of Adelie penguin mass. This
plot serves as a visual diagnostic of the sampling to help judge if
there were any issues during sampling across the multiple sampling
chains.</span><a class="headerlink" href="#fig-single-penguins-rank-kde-plot" title="Permalink to this image">¶</a></p>
</div>
<p>Comfortable with the fit we plot a posterior plot in
<a class="reference internal" href="#fig-singlespecies-mass-posteriorplot"><span class="std std-numref">Fig. 3.3</span></a> that combines all the
chains. Compare the point estimates from <a class="reference internal" href="#tab-penguin-mass-parameters-point-estimates"><span class="std std-numref">Table 3.1</span></a> of the mean and
standard deviation with our Bayesian estimates as shown in
<a class="reference internal" href="#fig-singlespecies-mass-posteriorplot"><span class="std std-numref">Fig. 3.3</span></a>.</p>
<div class="figure align-default" id="fig-singlespecies-mass-posteriorplot">
<a class="reference internal image-reference" href="../_images/SingleSpecies_Mass_PosteriorPlot.png"><img alt="../_images/SingleSpecies_Mass_PosteriorPlot.png" src="../_images/SingleSpecies_Mass_PosteriorPlot.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.3 </span><span class="caption-text">Posterior plot of the posterior of the Bayesian model in Code Block
<a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a> of Adelie penguins mass. The
vertical lines are the empirical mean and standard deviation.</span><a class="headerlink" href="#fig-singlespecies-mass-posteriorplot" title="Permalink to this image">¶</a></p>
</div>
<p>With the Bayesian estimate however, we also get the distribution of
plausible parameters. Using the tabular summary in Table <a class="reference internal" href="#tab-penguin-mass-parameters-bayesian-estimates"><span class="std std-numref">Table 3.2</span></a> from the same
posterior distribution in <a class="reference internal" href="#fig-single-penguins-rank-kde-plot"><span class="std std-numref">Fig. 3.2</span></a>
values of the mean from 3632 to 3772 grams are quite plausible. Note
that the standard deviation of the marginal posterior distribution
varies quite a bit as well. And remember the posterior distribution is
not the distribution of an individual penguin mass but rather possible
parameters of a Gaussian distribution that we assume describes penguin
mass. If we wanted the estimated distribution of individual penguin mass
we would need to generate a posterior predictive distribution. In this
case it will be the same Gaussian distribution conditioned on the
posterior of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>Now that we have characterized the Adelie penguin’s mass, we can do the
same for the other species. We could do so by writing two more models
but instead let us just run one model with 3 separated groups, one per
species.</p>
<div class="literal-block-wrapper docutils container" id="nocovariate-mass">
<div class="code-block-caption"><span class="caption-number">Listing 3.4 </span><span class="caption-text">nocovariate_mass</span><a class="headerlink" href="#nocovariate-mass" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pd.categorical makes it easy to index species below</span>
<span class="n">all_species</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s2">&quot;species&quot;</span><span class="p">])</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_penguin_mass_all_species</span><span class="p">:</span>
    <span class="c1"># Note the addition of the shape parameter</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="mi">4000</span><span class="p">,</span> <span class="mi">3000</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">mass</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mass&quot;</span><span class="p">,</span>
                     <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">[</span><span class="n">all_species</span><span class="o">.</span><span class="n">codes</span><span class="p">],</span>
                     <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">[</span><span class="n">all_species</span><span class="o">.</span><span class="n">codes</span><span class="p">],</span>
                     <span class="n">observed</span><span class="o">=</span><span class="n">penguins</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">])</span>

    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">inf_data_model_penguin_mass_all_species</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;μ_dim_0&quot;</span><span class="p">:</span> <span class="n">all_species</span><span class="o">.</span><span class="n">categories</span><span class="p">,</span>
                <span class="s2">&quot;σ_dim_0&quot;</span><span class="p">:</span> <span class="n">all_species</span><span class="o">.</span><span class="n">categories</span><span class="p">})</span>
</pre></div>
</div>
</div>
<p>We use the optional shape argument in each parameter and add an index in
our likelihood indicating to PyMC3 that we want to condition the
posterior estimate for each species individually. In programming
language design small tricks that make expressing ideas more seamless
are called <strong>syntactic sugar</strong>, and probabilistic programming developers
include these as well. Probabilistic Programming Languages strive to
allow expressing models with ease and with less errors.</p>
<p>After we run the model we once again inspect the KDE and rank plots, see
<a class="reference internal" href="#fig-all-penguins-rank-kde-plot"><span class="std std-numref">Fig. 3.4</span></a>. Compared to
<a class="reference internal" href="#fig-single-penguins-rank-kde-plot"><span class="std std-numref">Fig. 3.2</span></a> you will see 4 additional
plots, 2 each for the additional parameters added. Take a moment to
compare the estimate of the mean with the summary mean shown for each
species in <a class="reference internal" href="#tab-penguin-mass-parameters-point-estimates"><span class="std std-numref">Table 3.1</span></a>. To
better visualize the differences between the distributions for each
species, we plot the posterior again in a forest plot using Code Block
<a class="reference internal" href="#mass-forest-plot"><span class="std std-ref">mass_forest_plot</span></a>.
<a class="reference internal" href="#fig-forest-plot-means"><span class="std std-numref">Fig. 3.5</span></a> makes it easier to compare our estimates
across species and note that the Gentoo penguins seem to have more mass
than Adelie or Chinstrap penguins.</p>
<div class="figure align-default" id="fig-all-penguins-rank-kde-plot">
<a class="reference internal image-reference" href="../_images/AllSpecies_KDE_RankPlot.png"><img alt="../_images/AllSpecies_KDE_RankPlot.png" src="../_images/AllSpecies_KDE_RankPlot.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.4 </span><span class="caption-text">KDE and rank plot for posterior estimates of parameters of masses for
each species of penguins from the <code class="docutils literal notranslate"><span class="pre">penguins_masses</span></code> model. Note how each
species has its own pair of estimates for each parameter.</span><a class="headerlink" href="#fig-all-penguins-rank-kde-plot" title="Permalink to this image">¶</a></p>
</div>
<div class="literal-block-wrapper docutils container" id="mass-forest-plot">
<div class="code-block-caption"><span class="caption-number">Listing 3.5 </span><span class="caption-text">mass_forest_plot</span><a class="headerlink" href="#mass-forest-plot" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">inf_data_model_penguin_mass_all_species</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;μ&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-forest-plot-means">
<a class="reference internal image-reference" href="../_images/Independent_Model_ForestPlotMeans.png"><img alt="../_images/Independent_Model_ForestPlotMeans.png" src="../_images/Independent_Model_ForestPlotMeans.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.5 </span><span class="caption-text">Forest plot of the mean of mass of each species group in
<code class="docutils literal notranslate"><span class="pre">model_penguin_mass_all_species</span></code>.  Each line represents one chain in the sampler, the dot is the
mean, the thick line is the interquartile range and the thin line is the 94% Highest Density
Interval.</span><a class="headerlink" href="#fig-forest-plot-means" title="Permalink to this image">¶</a></p>
</div>
<p><a class="reference internal" href="#fig-forest-plot-means"><span class="std std-numref">Fig. 3.5</span></a> makes it easier to compare our estimates
and easily note that the Gentoo penguins have more mass than Adelie or
Chinstrap penguins. Let us also look at the standard deviation in
<a class="reference internal" href="#fig-forest-plot-sigma"><span class="std std-numref">Fig. 3.6</span></a>. The 94% highest density interval of the
posterior is reporting uncertainty in the order of 100 grams.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>az.plot_forest(inf_data_model_penguin_mass_all_species, var_names=[&quot;σ&quot;])
</pre></div>
</div>
<div class="figure align-default" id="fig-forest-plot-sigma">
<a class="reference internal image-reference" href="../_images/Independent_Model_ForestPlotSigma.png"><img alt="../_images/Independent_Model_ForestPlotSigma.png" src="../_images/Independent_Model_ForestPlotSigma.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.6 </span><span class="caption-text">Forest plot of the standard deviations of the mass for each species
group in <code class="docutils literal notranslate"><span class="pre">model_penguin_mass_all_species</span></code>. This plot depicts our
estimation of the dispersion of penguin mass, so for example, given a
mean estimate of the Gentoo penguin distribution, the associated
standard deviation is plausibly anywhere between 450 grams to 550 grams.</span><a class="headerlink" href="#fig-forest-plot-sigma" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="comparing-two-ppls">
<span id="id2"></span><h3><span class="section-number">3.1.1. </span>Comparing Two PPLs<a class="headerlink" href="#comparing-two-ppls" title="Permalink to this headline">¶</a></h3>
<p>Before expanding on the statistical and modeling ideas further, we will
take a moment to talk about the probabilistic programming languages and
introduce another PPL we will be using in this book, TensorFlow
Probability (TFP). We will do so by translating the PyMC3 intercept only
model in Code Block <a class="reference internal" href="#nocovariate-mass"><span class="std std-ref">nocovariate_mass</span></a>
into TFP.</p>
<p>It may seem unnecessary to learn different PPLs. However, there are
specific reasons we chose to use two PPLs instead of one in this book.
Seeing the same workflow in different PPLs will give you a more thorough
understanding of computational Bayesian modeling, help you separate
computational details from statistical ideas, and make you a stronger
modeler overall. Moreover, different PPLs have different strength and
focus. PyMC3 is a higher level PPL that makes it easier to express
models with less code, whereas TFP provides a lower level PPL for
composable modeling and inference. Another is that not all PPLs are
able to express all models as easily as each other. For instance Time
Series models (Chapter <a class="reference internal" href="chp_06.html#chap4"><span class="std std-ref">6</span></a>) are more easily defined in
TFP whereas Bayesian Additive Regression Trees are more easily expressed
in PyMC3 (Chapter <a class="reference internal" href="chp_07.html#chap6"><span class="std std-ref">7</span></a>). Through this exposure to multiple
languages you will come out with a stronger understanding of both the
fundamental elements of Bayesian modeling and how they are implemented
computationally.</p>
<p>Probabilistic Programming Languages (emphasis on language) are composed
of primitives. The primitives in a programming language are the simplest
elements available to construct more complex programs. You can think of
primitives are like words in natural languages which can form more
complex structures, like sentences. And as different languages use
different words, different PPLs use different primitives. These
primitives are mainly used to express models, perform inference, or
express other parts of the workflow. In PyMC3, model building related
primitives are contained under the namespace <code class="docutils literal notranslate"><span class="pre">pm.</span></code> For example, in Code
Block <a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a> we see
<code class="docutils literal notranslate"><span class="pre">pm.HalfStudentT(.)</span></code>, and <code class="docutils literal notranslate"><span class="pre">pm.Normal(.)</span></code>, which represent a random
variable. The <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">pm.Model()</span> <span class="pre">as</span> <span class="pre">.</span></code> statement evokes a Python context
manager, which PyMC3 uses to build the model <code class="docutils literal notranslate"><span class="pre">model_adelie_penguin_mass</span></code>
by collecting the random variables within the context manager. We then
use <code class="docutils literal notranslate"><span class="pre">pm.sample_prior_predictive(.)</span></code> and <code class="docutils literal notranslate"><span class="pre">pm.sample(.)</span></code> to obtain samples
from the prior predictive distribution and from the posterior
distribution, respectively.</p>
<p>Similarly, TFP provides primitives for user to specify distributions and
model in <code class="docutils literal notranslate"><span class="pre">tfp.distributions</span></code>, running MCMC (<code class="docutils literal notranslate"><span class="pre">tfp.mcmc</span></code>), and more. For
example, to construct a Bayesian model, TensorFlow provides multiple
primitives under the name <code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code> <span id="id3">[<a class="reference internal" href="references.html#id128" title="Dan Piponi, Dave Moore, and Joshua V. Dillon. Joint distributions for tensorflow probability. arXiv preprint arXiv:2001.11819, 2020.">29</a>]</span>
API. In this chapter and the remaining of the book we mostly use
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code>, but there are other variants of
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code> which may better suit your use case <a class="footnote-reference brackets" href="#id35" id="id4">1</a>. Since
basic data import and summary statistics stays the same as Code Block
<a class="reference internal" href="#penguin-load"><span class="std std-ref">penguin_load</span></a> and
<a class="reference internal" href="#penguin-mass-empirical"><span class="std std-ref">penguin_mass_empirical</span></a> we can
focus on the model building and inference.
<code class="docutils literal notranslate"><span class="pre">model_penguin_mass_all_species</span></code> expressed in TFP which is shown in Code
Block <a class="reference internal" href="#penguin-mass-tfp"><span class="std std-ref">penguin_mass_tfp</span></a> below</p>
<div class="literal-block-wrapper docutils container" id="penguin-mass-tfp">
<div class="code-block-caption"><span class="caption-number">Listing 3.6 </span><span class="caption-text">penguin_mass_tfp</span><a class="headerlink" href="#penguin-mass-tfp" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_probability</span> <span class="k">as</span> <span class="nn">tfp</span>

<span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span><span class="o">.</span><span class="n">Root</span>

<span class="n">species_idx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">all_species</span><span class="o">.</span><span class="n">codes</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">body_mass_g</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">],</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">jd_penguin_mass_all_species</span><span class="p">():</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2000</span><span class="p">),</span>
            <span class="n">sample_shape</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sigma&quot;</span><span class="p">))</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3000</span><span class="p">),</span>
            <span class="n">sample_shape</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mu&quot;</span><span class="p">))</span>
    <span class="n">mass</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">μ</span><span class="p">,</span> <span class="n">species_idx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
                   <span class="n">scale</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">σ</span><span class="p">,</span> <span class="n">species_idx</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span>
        <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mass&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Since this is our first encounter with a Bayesian model written in TFP,
let us spend a few paragraphs to detail the API. The primitives are
distribution classes in <code class="docutils literal notranslate"><span class="pre">tfp.distributions</span></code>, which we assign a shorter
alias <code class="docutils literal notranslate"><span class="pre">tfd</span> <span class="pre">=</span> <span class="pre">tfp.distributions</span></code>. <code class="docutils literal notranslate"><span class="pre">tfd</span></code> contains commonly used
distributions like <code class="docutils literal notranslate"><span class="pre">tfd.Normal(.)</span></code>. We also used <code class="docutils literal notranslate"><span class="pre">tfd.Sample</span></code>, which
returns multiple independent copies of the base distribution
(conceptually we achieve the similar goal as using the syntactic sugar
<code class="docutils literal notranslate"><span class="pre">shape=(.)</span></code> in PyMC3). <code class="docutils literal notranslate"><span class="pre">tfd.Independent</span></code> is used to indicate that the
distribution contains multiple copies that we would like to sum over
some axis when computing the log-likelihood, which specified by the
<code class="docutils literal notranslate"><span class="pre">reinterpreted_batch_ndims</span></code> function argument. Usually we wrap the
distributions associated with the observation with <code class="docutils literal notranslate"><span class="pre">tfd.Independent</span></code>
<a class="footnote-reference brackets" href="#id36" id="id5">2</a>. You can read a bit more about shape handling in TFP and PPL in
Section <a class="reference internal" href="chp_10.html#shape-ppl"><span class="std std-ref">Shape Handling in PPLs</span></a>.</p>
<p>An interesting signature of a <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> model is,
as the name suggests, the usage of Coroutine in Python. Without getting
into too much detail about Generators and Coroutines, here a <code class="docutils literal notranslate"><span class="pre">yield</span></code>
statement of a distribution gives you some random variable inside of
your model function. You can view <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">yield</span> <span class="pre">Normal(.)</span></code> as the way to
express <span class="math notranslate nohighlight">\(y \sim \text{Normal(.)}\)</span>. Also, we need to identify the random
variables without dependencies as root nodes by wrapping them with
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine.Root</span></code>. The model is written as a Python
function with no input argument and no return value. Lastly, it is
convenient to put <code class="docutils literal notranslate"><span class="pre">&#64;tfd.JointDistributionCoroutine</span></code> on top of the Python
function as a decorator to get the model (i.e., a
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code>) directly.</p>
<p>The resulting <code class="docutils literal notranslate"><span class="pre">jd_penguin_mass_all_species</span></code> is the intercept only
regression model in Code Block <a class="reference internal" href="#nocovariate-mass"><span class="std std-ref">nocovariate_mass</span></a>
restated in TFP. It has similar methods like other <code class="docutils literal notranslate"><span class="pre">tfd.Distribution</span></code>,
which we can utilize in our Bayesian workflow. For example, to draw
prior and prior predictive samples, we can call the <code class="docutils literal notranslate"><span class="pre">.sample(.)</span></code> method,
which returns a custom nested Python structure similar to a
<code class="docutils literal notranslate"><span class="pre">namedtuple</span></code>. In Code Block
<a class="reference internal" href="#penguin-mass-tfp-prior-predictive"><span class="std std-ref">penguin_mass_tfp_prior_predictive</span></a>
we draw 1000 prior and prior predictive samples.</p>
<div class="literal-block-wrapper docutils container" id="penguin-mass-tfp-prior-predictive">
<div class="code-block-caption"><span class="caption-number">Listing 3.7 </span><span class="caption-text">penguin_mass_tfp_prior_predictive</span><a class="headerlink" href="#penguin-mass-tfp-prior-predictive" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prior_predictive_samples</span> <span class="o">=</span> <span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">.sample(.)</span></code> method of a <code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code> can also draw
conditional samples, which is the mechanism we will make use of to draw
posterior predictive samples. You can run Code Block
<a class="reference internal" href="#penguin-mass-tfp-prior-predictive2"><span class="std std-ref">penguin_mass_tfp_prior_predictive2</span></a>
and inspect the output to see how random samples change if you condition
some random variables in the model to some specific values. Overall, we
invoke the <em>forward</em> generative process when calling <code class="docutils literal notranslate"><span class="pre">.sample(.)</span></code>.</p>
<div class="literal-block-wrapper docutils container" id="penguin-mass-tfp-prior-predictive2">
<div class="code-block-caption"><span class="caption-number">Listing 3.8 </span><span class="caption-text">penguin_mass_tfp_prior_predictive2</span><a class="headerlink" href="#penguin-mass-tfp-prior-predictive2" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sigma</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">]))</span>
<span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<p>Once we condition the generative model <code class="docutils literal notranslate"><span class="pre">jd_penguin_mass_all_species</span></code> to
the observed penguin body mass, we can get the posterior distribution.
From the computational perspective, we want to generate a function that
returns the posterior log-probability (up to some constant) evaluated at
the input. This could be done by creating a Python function closure or
using the <code class="docutils literal notranslate"><span class="pre">.experimental_pin</span></code> method, as shown in Code Block
<a class="reference internal" href="#tfp-posterior-generation"><span class="std std-ref">tfp_posterior_generation</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="tfp-posterior-generation">
<div class="code-block-caption"><span class="caption-number">Listing 3.9 </span><span class="caption-text">tfp_posterior_generation</span><a class="headerlink" href="#tfp-posterior-generation" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">target_density_function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
    <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">mass</span><span class="o">=</span><span class="n">body_mass_g</span><span class="p">)</span>

<span class="n">jd_penguin_mass_observed</span> <span class="o">=</span> <span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">experimental_pin</span><span class="p">(</span>
    <span class="n">mass</span><span class="o">=</span><span class="n">body_mass_g</span><span class="p">)</span>
<span class="n">target_density_function</span> <span class="o">=</span> <span class="n">jd_penguin_mass_observed</span><span class="o">.</span><span class="n">unnormalized_log_prob</span>
</pre></div>
</div>
</div>
<p>Inference is done using <code class="docutils literal notranslate"><span class="pre">target_density_function</span></code>, for example, we can
find the maximum of the function, which gives the <strong>maximum a posteriori
probability</strong> (MAP) estimate. We can also use methods in <code class="docutils literal notranslate"><span class="pre">tfp.mcmc</span></code>
<span id="id6">[<a class="reference internal" href="references.html#id133" title="Junpeng Lao, Christopher Suter, Ian Langmore, Cyril Chimisov, Ashish Saxena, Pavel Sountsov, Dave Moore, Rif A Saurous, Matthew D Hoffman, and Joshua V. Dillon. Tfp.mcmc: modern markov chain monte carlo tools built for modern hardware. arXiv preprint arXiv:2002.01184, 2020.">30</a>]</span> to sample from the posterior. Or more conveniently,
using a standard sampling routine similar to what is currently used in
PyMC3 <a class="footnote-reference brackets" href="#id37" id="id7">3</a> as shown in Code Block
<a class="reference internal" href="#tfp-posterior-inference"><span class="std std-ref">tfp_posterior_inference</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="tfp-posterior-inference">
<div class="code-block-caption"><span class="caption-number">Listing 3.10 </span><span class="caption-text">tfp_posterior_inference</span><a class="headerlink" href="#tfp-posterior-inference" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">run_mcmc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">mcmc</span><span class="o">.</span><span class="n">windowed_adaptive_nuts</span><span class="p">,</span>
    <span class="n">autograph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">jit_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mcmc_samples</span><span class="p">,</span> <span class="n">sampler_stats</span> <span class="o">=</span> <span class="n">run_mcmc</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">jd_penguin_mass_all_species</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">mass</span><span class="o">=</span><span class="n">body_mass_g</span><span class="p">)</span>

<span class="n">inf_data_model_penguin_mass_all_species2</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="n">posterior</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># TFP mcmc returns (num_samples, num_chains, ...), we swap</span>
        <span class="c1"># the first and second axis below for each RV so the shape</span>
        <span class="c1"># is what ArviZ expected.</span>
        <span class="n">k</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
    <span class="n">sample_stats</span><span class="o">=</span><span class="p">{</span>
        <span class="n">k</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">sampler_stats</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;target_log_prob&quot;</span><span class="p">,</span> <span class="s2">&quot;diverging&quot;</span><span class="p">,</span> <span class="s2">&quot;accept_ratio&quot;</span><span class="p">,</span> <span class="s2">&quot;n_steps&quot;</span><span class="p">]}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>In Code Block
<a class="reference internal" href="#tfp-posterior-inference"><span class="std std-ref">tfp_posterior_inference</span></a> we ran
4 MCMC chains, each with 1000 posterior samples after 1000 adaptation
steps. Internally it invokes the <code class="docutils literal notranslate"><span class="pre">experimental_pin</span></code> method by
conditioning the model (pass into the function as an argument) with the
observed (additional keyword argument <code class="docutils literal notranslate"><span class="pre">mass=body_mass_g</span></code> at the end).
Lines 8-18 parse the sampling result into an ArviZ InferenceData, which
we can now run diagnostics and exploratory analysis of Bayesian models
in ArviZ. We can additionally add prior and posterior predictive samples
and data log-likelihood to <code class="docutils literal notranslate"><span class="pre">inf_data_model_penguin_mass_all_species2</span></code> in
a transparent way in Code Block
<a class="reference internal" href="#tfp-idata-additional"><span class="std std-ref">tfp_idata_additional</span></a> below. Note
that we make use of the <code class="docutils literal notranslate"><span class="pre">sample_distributions</span></code> method of a
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code> that draws samples <em>and</em> generates a
distribution conditioned on the posterior samples.</p>
<div class="literal-block-wrapper docutils container" id="tfp-idata-additional">
<div class="code-block-caption"><span class="caption-number">Listing 3.11 </span><span class="caption-text">tfp_idata_additional</span><a class="headerlink" href="#tfp-idata-additional" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prior_predictive_samples</span> <span class="o">=</span> <span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">sample</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">])</span>
<span class="n">dist</span><span class="p">,</span> <span class="n">samples</span> <span class="o">=</span> <span class="n">jd_penguin_mass_all_species</span><span class="o">.</span><span class="n">sample_distributions</span><span class="p">(</span>
    <span class="n">value</span><span class="o">=</span><span class="n">mcmc_samples</span><span class="p">)</span>
<span class="n">ppc_samples</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ppc_distribution</span> <span class="o">=</span> <span class="n">dist</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">distribution</span>
<span class="n">data_log_likelihood</span> <span class="o">=</span> <span class="n">ppc_distribution</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">body_mass_g</span><span class="p">)</span>

<span class="c1"># Be careful not to run this code twice during REPL workflow.</span>
<span class="n">inf_data_model_penguin_mass_all_species2</span><span class="o">.</span><span class="n">add_groups</span><span class="p">(</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior_predictive_samples</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">_asdict</span><span class="p">(),</span>
    <span class="n">prior_predictive</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mass&quot;</span><span class="p">:</span> <span class="n">prior_predictive_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]},</span>
    <span class="n">posterior_predictive</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mass&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">ppc_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)},</span>
    <span class="n">log_likelihood</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mass&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">data_log_likelihood</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)},</span>
    <span class="n">observed_data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mass&quot;</span><span class="p">:</span> <span class="n">body_mass_g</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>This concludes our whirlwind tour of TensorFlow Probability. Like any
language you likely will not gain fluency in your initial exposure. But
by comparing the two models you should now have a better sense of what
concepts are <em>Bayesian centric</em> and what concepts are <em>PPL centric</em>. For
the remainder of this chapter and the next we will switch between PyMC3
and TFP to continue helping you identify this difference and see more
worked examples. We include exercises to translate Code Block examples
from one to the other to aid your practice journey in becoming a PPL
polyglot.</p>
</div>
</div>
<div class="section" id="linear-regression">
<span id="id8"></span><h2><span class="section-number">3.2. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>In the previous section we modeled the distribution of penguin mass by
setting prior distributions over the mean and standard deviation of a
Gaussian distribution. Importantly we assumed that the mass did not vary
with other features in the data. However, we would expect that other
observed data points could provide information about expected penguins
mass. Intuitively if we see two penguins, one with long flippers and one
with short flippers, we would expect the larger penguin, the one with
long flippers, to have more mass even if we did not have a scale on hand
to measure their mass precisely. One of the simplest ways to estimate
this relationship of observed flipper length on estimated mass is to fit
a linear regression model, where the mean is <em>conditionally</em> modeled as
a linear combination of other variables</p>
<div class="math notranslate nohighlight" id="equation-eq-expanded-regression">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-eq-expanded-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \mu =&amp; \beta_0 + \beta_1 X_1 + \dots + \beta_m X_m \\
Y \sim&amp; \mathcal{N}(\mu, \sigma)
\end{split}\end{split}\]</div>
<p>where the coefficients, also referred as parameters, are represented with <span class="math notranslate nohighlight">\(\beta_i\)</span>.
For example, <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept of
the linear model. <span class="math notranslate nohighlight">\(X_i\)</span> is referred to predictors or independent
variables, and <span class="math notranslate nohighlight">\(Y\)</span> is usually referred to as target, output, response,
or dependent variable. It is important to notice that both
<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are observed data and that they are paired
<span class="math notranslate nohighlight">\(\{y_j, x_j\}\)</span>. That is, if we change the order of <span class="math notranslate nohighlight">\(Y\)</span> without changing
<span class="math notranslate nohighlight">\(X\)</span> we will destroy some of the information in our data.</p>
<p>We call this a linear regression because the parameters (not the
covariates) enter the model in a linear fashion. Also for models with a
single covariate, we can think of this model as fitting a line to the
<span class="math notranslate nohighlight">\((X, y)\)</span> data, and for higher dimensions a plane or more generally a
hyperplane.</p>
<p>Alternatively we can express Equation <a class="reference internal" href="#equation-eq-expanded-regression">(3.2)</a> using
matrix notation:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-model-matrix">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-eq-linear-model-matrix" title="Permalink to this equation">¶</a></span>\[\mu = \mathbf{X}\boldsymbol{\beta}\]</div>
<p>where we are taking the matrix-vector product between the coefficient
column vector <span class="math notranslate nohighlight">\(\beta\)</span> and the matrix of covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>An alternative expression you might have seen in other (non-Bayesian)
occasions is to rewrite Equation <a class="reference internal" href="#equation-eq-expanded-regression">(3.2)</a> as noisy
observation of some linear prediction:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-model-enginner">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-eq-linear-model-enginner" title="Permalink to this equation">¶</a></span>\[Y = \mathbf{X}\boldsymbol{\beta} + \epsilon,\; \epsilon \sim \mathcal{N}(0, \sigma)\]</div>
<p>The formulation in Equation <a class="reference internal" href="#equation-eq-linear-model-enginner">(3.4)</a> separates the
deterministic part (linear prediction) and the stochastic part (noise)
of linear regression. However, we prefer Equation
<a class="reference internal" href="#equation-eq-expanded-regression">(3.2)</a> as it shows the generative process more
clearly.</p>
<div class="admonition-design-matrix admonition">
<p class="admonition-title">Design Matrix</p>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> in Equation
<a class="reference internal" href="#equation-eq-linear-model-matrix">(3.3)</a> is known as design matrix and is a matrix
of values of explanatory variables of a given set of objects, plus an
additional column of ones to represent the intercept. Each row
represents an unique observation (e.g., a penguin), with the successive
columns corresponding to the variables (like flipper length) and their
specific values for that object.</p>
<p>A design matrix is not limited to continuous covariates. For discrete
covariates that represent categorical predictors (i.e., there are only a
few categories), a common way to turn those into a design matrix is
called dummy coding or one-hot coding. For example, in our intercept per
penguin model (Code Block
<a class="reference internal" href="#nocovariate-mass"><span class="std std-ref">nocovariate_mass</span></a>), instead of
<code class="docutils literal notranslate"><span class="pre">mu</span> <span class="pre">=</span> <span class="pre">μ[species.codes]</span></code> we can use <code class="docutils literal notranslate"><span class="pre">pandas.get_dummies</span></code> to parse the
categorical information into a design matrix, and then write
<code class="docutils literal notranslate"><span class="pre">mu</span> <span class="pre">=</span> <span class="pre">pd.get_dummies(penguins[&quot;species&quot;])</span> <span class="pre">&#64;</span> <span class="pre">μ</span></code>, where <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> is a Python
operator for performing matrix multiplication. There are also few other
functions to perform one hot encoding in Python, for example,
<code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.OneHotEncoder</span></code>, as this is a very common data
manipulation technique.</p>
<p>Alternatively, categorical predictors could be encoded such that the
resulting column and associated coefficient representing linear
contrast. For example, different design matrix encoding of two
categorical predictors are associated with Type I, II and III sums of
squares in null-hypothesis testing setting for ANOVA.</p>
</div>
<p>If we plot Equation <a class="reference internal" href="#equation-eq-expanded-regression">(3.2)</a> in “three dimensions”
we get <a class="reference internal" href="#fig-3d-linear-regression"><span class="std std-numref">Fig. 3.7</span></a>, which shows how the estimated
parameters of the likelihood distribution can change based on other
observed data <span class="math notranslate nohighlight">\(x\)</span>. While in this one illustration, and in this chapter,
we are using a linear relationship to model the relationship between <span class="math notranslate nohighlight">\(x\)</span>
and <span class="math notranslate nohighlight">\(Y\)</span>, and a Gaussian distribution as a likelihood, in other model
architectures, we may opt for different choices as we will see in
Chapter <a class="reference internal" href="chp_04.html#chap3"><span class="std std-ref">4</span></a>.</p>
<div class="figure align-default" id="fig-3d-linear-regression">
<a class="reference internal image-reference" href="../_images/3d_linear_regression.png"><img alt="../_images/3d_linear_regression.png" src="../_images/3d_linear_regression.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.7 </span><span class="caption-text">A linear regression with the Gaussian likelihood function evaluated at 3
points. Note this plot only shows one possible Gaussian distribution at
each value of <span class="math notranslate nohighlight">\(x\)</span>, where after fitting a Bayesian model we will end up
with a distribution of Gaussian, whose parameters may follow a
distribution other than Gaussian.</span><a class="headerlink" href="#fig-3d-linear-regression" title="Permalink to this image">¶</a></p>
</div>
<div class="section" id="linear-penguins">
<span id="linear-regression-intro"></span><h3><span class="section-number">3.2.1. </span>Linear Penguins<a class="headerlink" href="#linear-penguins" title="Permalink to this headline">¶</a></h3>
<p>If we recall our penguins we were interested using additional data to
better estimate the mean mass of a group of penguins. Using linear
regression we write the model in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a>, which
includes two new parameters <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> typically called the
intercept and slope. For this example we set wide priors of
<span class="math notranslate nohighlight">\(\mathcal{N}(0, 4000)\)</span> to focus on the model, which also is the same as
saying we assume no domain expertise. We subsequently run our sampler,
which has now estimated three parameters <span class="math notranslate nohighlight">\(\sigma\)</span>, <span class="math notranslate nohighlight">\(\beta_1\)</span> and
<span class="math notranslate nohighlight">\(\beta_0\)</span>.</p>
<div class="literal-block-wrapper docutils container" id="non-centered-regression">
<div class="code-block-caption"><span class="caption-number">Listing 3.12 </span><span class="caption-text">non_centered_regression</span><a class="headerlink" href="#non-centered-regression" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">adelie_flipper_length_obs</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">adelie_mask</span><span class="p">,</span> <span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_adelie_flipper_regression</span><span class="p">:</span>
    <span class="c1"># pm.Data allows us to change the underlying value in a later code block</span>
    <span class="n">adelie_flipper_length</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="s2">&quot;adelie_flipper_length&quot;</span><span class="p">,</span>
                                    <span class="n">adelie_flipper_length_obs</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
    <span class="n">β_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_0&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4000</span><span class="p">)</span>
    <span class="n">β_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_1&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">4000</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">β_1</span> <span class="o">*</span> <span class="n">adelie_flipper_length</span><span class="p">)</span>

    <span class="n">mass</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mass&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">adelie_mass_obs</span><span class="p">)</span>

    <span class="n">inf_data_adelie_flipper_regression</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To save space in the book we are not going to show the diagnostics each
time but you should neither trust us or your sampler blindly. Instead
you should run the diagnostics to verify you have a reliable posterior
approximation.</p>
<div class="figure align-default" id="fig-adelie-coefficient-posterior-plots">
<a class="reference internal image-reference" href="../_images/adelie_coefficient_posterior_plots.png"><img alt="../_images/adelie_coefficient_posterior_plots.png" src="../_images/adelie_coefficient_posterior_plots.png" style="width: 5in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.8 </span><span class="caption-text">Estimates of the parameter value distributions of our linear regression
coefficient from <code class="docutils literal notranslate"><span class="pre">model_adelie_flipper_regression</span></code>.</span><a class="headerlink" href="#fig-adelie-coefficient-posterior-plots" title="Permalink to this image">¶</a></p>
</div>
<p>After our sampler finishes running we can plot
<a class="reference internal" href="#fig-adelie-coefficient-posterior-plots"><span class="std std-numref">Fig. 3.8</span></a> which shows a full
posterior plot we can use to inspect <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>. The
coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> expresses that for every millimeter change of
Adelie flipper length we can nominally expect a change of 32 grams of
mass, although anywhere between 22 grams to 41 grams could reasonably
occur as well. Additionally, from
<a class="reference internal" href="#fig-adelie-coefficient-posterior-plots"><span class="std std-numref">Fig. 3.8</span></a> we can note how the 94%
highest density interval does not cross 0 grams. This supports our
assumption that there is a relationship between mass and flipper length.
This observation is quite useful for interpreting how flipper length and
mass correlate. However, we should be careful about not
over-interpreting the coefficients or thinking a linear model
necessarily implies a causal link. For example, if we perform a flipper
extension surgery to a penguin this will not necessarily translate into
a gain in mass, it could actually be the opposite due to stress or
impediments of this penguin to get food. The opposite relation is not
necessarily true either, providing more food to a penguin could help her
to have a larger flipper, but it could also make it just a fatter
penguin. Now focusing on <span class="math notranslate nohighlight">\(\beta_0\)</span> however, what does it represent? From
our posterior estimate we can state that if we saw an Adelie penguin
with a 0 mm flipper length we would expect the mass of this impossible
penguin to be somewhere between -4151 and -510 grams. According to our
model this statement is true, but negative mass does not make sense.
This is not necessarily an issue, there is no rule that every parameter
in a model needs to be interpretable, nor that the model provide
reasonable prediction at every parameter value. At this point in our
journey the purpose of this particular model was to estimate the
relationship between flipper length and penguin mass and with our
posterior estimates, we have succeeded with that goal.</p>
<div class="admonition-models-a-balance-between-math-and-reality admonition">
<p class="admonition-title">Models: A balance between math and reality</p>
<p>In our penguin example it would not make sense if penguin mass was below 0 (or even close to it),
even though the model allowed it. Because we fit the model using values
for the masses that are far from 0, we should not be surprised that the
model fails if we want to extrapolate conclusions for values close to 0
or below it. A model does not necessarily have to provide sensible
predictions for all possible values, it just needs to provide sensible
predictions for the purposes that we are building it for.</p>
</div>
<p>We started on this section surmising that incorporating a covariate
would lead to better predictions of penguin mass. We can verify this is
the case by comparing the posterior estimates of <span class="math notranslate nohighlight">\(\sigma\)</span> from our fixed
mean model and with our linearly varying mean model in
<a class="reference internal" href="#fig-singlespecies-singleregression-forest-sigma-comparison"><span class="std std-numref">Fig. 3.9</span></a>,
our estimate of the likelihood’s standard deviation has dropped from a
mean of around <span class="math notranslate nohighlight">\(\approx 460\)</span> grams to <span class="math notranslate nohighlight">\(\approx 380\)</span> grams.</p>
<div class="figure align-default" id="fig-singlespecies-singleregression-forest-sigma-comparison">
<a class="reference internal image-reference" href="../_images/SingleSpecies_SingleRegression_Forest_Sigma_Comparison.png"><img alt="../_images/SingleSpecies_SingleRegression_Forest_Sigma_Comparison.png" src="../_images/SingleSpecies_SingleRegression_Forest_Sigma_Comparison.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.9 </span><span class="caption-text">By using the covariate of flipper length when estimating penguin mass
the magnitude of the estimated error is reduced from a mean of slightly
over 460 grams to around 380 grams. This intuitively makes sense as if
we are given information about a quantity we are estimating, we can
leverage that information to make better estimates.</span><a class="headerlink" href="#fig-singlespecies-singleregression-forest-sigma-comparison" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-flipper-length-mass-regression">
<a class="reference internal image-reference" href="../_images/Flipper_length_mass_regression.png"><img alt="../_images/Flipper_length_mass_regression.png" src="../_images/Flipper_length_mass_regression.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.10 </span><span class="caption-text">Observed Adelie data of flipper length vs mass as scatter plot, and mean
estimate of the likelihood as black line, and 94% HDI of the mean as
gray interval. Note how our mean estimate varies as flipper varies.</span><a class="headerlink" href="#fig-flipper-length-mass-regression" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="predictions">
<span id="chp2-predictions"></span><h3><span class="section-number">3.2.2. </span>Predictions<a class="headerlink" href="#predictions" title="Permalink to this headline">¶</a></h3>
<p>In the <a class="reference internal" href="#linear-regression-intro"><span class="std std-ref">Linear Penguins</span></a> we estimated a linear relationship
between flipper length and mass. Another use of regression is to
leverage that relationship in order to make predictions. In our case
given the flipper length of a penguin, can we predict its mass? In fact
we can. We will use our results from <code class="docutils literal notranslate"><span class="pre">model_adelie_flipper_regression</span></code>
to do so. Because in Bayesian statistics we are dealing with
distributions we do not end up with a single predicted value but instead
a distribution of possible values. That is the posterior predictive
distribution as defined in Equation
<a class="reference internal" href="chp_01.html#equation-eq-post-pred-dist">(1.8)</a>. In practice, more often than
not, we will not compute our predictions analytically but we will use a
PPL to estimate them using our posterior samples. For example, if we had
a penguin of average flipper length and wanted to predict the likely
mass using <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> we would write Code Block
<a class="reference internal" href="#penguins-ppd"><span class="std std-ref">penguins_ppd</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="penguins-ppd">
<div class="code-block-caption"><span class="caption-number">Listing 3.13 </span><span class="caption-text">penguins_ppd</span><a class="headerlink" href="#penguins-ppd" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">model_adelie_flipper_regression</span><span class="p">:</span>
    <span class="c1"># Change the underlying value to the mean observed flipper length</span>
    <span class="c1"># for our posterior predictive samples</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s2">&quot;adelie_flipper_length&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">adelie_flipper_length_obs</span><span class="o">.</span><span class="n">mean</span><span class="p">()]})</span>
    <span class="n">posterior_predictions</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">inf_data_adelie_flipper_regression</span><span class="o">.</span><span class="n">posterior</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mass&quot;</span><span class="p">,</span> <span class="s2">&quot;μ&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>In the first line of Code Block
<a class="reference internal" href="#penguins-ppd"><span class="std std-ref">penguins_ppd</span></a> we fix the value of our
flipper length to the average observed flipper length. Then using the
regression model <code class="docutils literal notranslate"><span class="pre">model_adelie_flipper_regression</span></code>, we can generate
posterior predictive samples of the mass at that fixed value. In
<a class="reference internal" href="#fig-flipper-length-mass-posterior-predictive"><span class="std std-numref">Fig. 3.11</span></a> we plot the
posterior predictive distribution of the mass for penguins of average
flipper length, along the posterior of the mean.</p>
<div class="figure align-default" id="fig-flipper-length-mass-posterior-predictive">
<a class="reference internal image-reference" href="../_images/Flipper_length_mass_posterior_predictive.png"><img alt="../_images/Flipper_length_mass_posterior_predictive.png" src="../_images/Flipper_length_mass_posterior_predictive.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.11 </span><span class="caption-text">The posterior distribution of the mean, <span class="math notranslate nohighlight">\(\mu\)</span>, evaluated at the mean
flipper length in blue and the posterior predictive distribution
evaluated at the mean flipper length in black. The black curve is wider
as it describes the distribution of the predicted data (for a given
flipper length), while the blue curve represents the distribution of
just the mean of the predicted data.</span><a class="headerlink" href="#fig-flipper-length-mass-posterior-predictive" title="Permalink to this image">¶</a></p>
</div>
<p>In short not only can we use our model in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> to
estimate the relationship between flipper length and mass, we also can
obtain an estimate of the penguin mass at any arbitrary flipper length.
In other words we can use the estimated <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span>
coefficients to make predictions of the mass of unseen penguins of any
flipper length using posterior predictive distributions.</p>
<p>As such, the posterior predictive distribution is an especially powerful
tool in a Bayesian context as it let us predict not just the most likely
value, but a distribution of plausible values incorporating the
uncertainty about our estimates, as seen from Equation
<a class="reference internal" href="chp_01.html#equation-eq-post-pred-dist">(1.8)</a>.</p>
</div>
<div class="section" id="centering">
<span id="id9"></span><h3><span class="section-number">3.2.3. </span>Centering<a class="headerlink" href="#centering" title="Permalink to this headline">¶</a></h3>
<p>Our model in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> worked
well for estimating the correlation between flipper length and penguin
mass, and in predicting the mass of penguins at a given flipper length.
Unfortunately with the data and the model provided our estimate of
<span class="math notranslate nohighlight">\(\beta_0\)</span> was not particularly useful. However, we can use a
transformation to make <span class="math notranslate nohighlight">\(\beta_0\)</span> more interpretable. In this case we
will opt for a centering transformation, which takes a set of values and
centers its mean value at zero as shown in Code Block
<a class="reference internal" href="#flipper-centering"><span class="std std-ref">flipper_centering</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="flipper-centering">
<div class="code-block-caption"><span class="caption-number">Listing 3.14 </span><span class="caption-text">flipper_centering</span><a class="headerlink" href="#flipper-centering" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">adelie_flipper_length_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">adelie_flipper_length_obs</span> <span class="o">-</span>
                           <span class="n">adelie_flipper_length_obs</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
</div>
<p>With our now centered covariate let us fit our model again, this time
using TFP.</p>
<div class="literal-block-wrapper docutils container" id="tfp-penguins-centered-predictor">
<div class="code-block-caption"><span class="caption-number">Listing 3.15 </span><span class="caption-text">tfp_penguins_centered_predictor</span><a class="headerlink" href="#tfp-penguins-centered-predictor" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_adelie_flipper_model</span><span class="p">(</span><span class="n">adelie_flipper_length</span><span class="p">):</span>
    <span class="n">adelie_flipper_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">adelie_flipper_length</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
    <span class="k">def</span> <span class="nf">jd_adelie_flipper_regression</span><span class="p">():</span>
        <span class="n">σ</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sigma&quot;</span><span class="p">))</span>
        <span class="n">β_1</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_1&quot;</span><span class="p">))</span>
        <span class="n">β_0</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_0&quot;</span><span class="p">))</span>
        <span class="n">μ</span> <span class="o">=</span> <span class="n">β_0</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">β_1</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">adelie_flipper_length</span>
        <span class="n">mass</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">σ</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
            <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mass&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jd_adelie_flipper_regression</span>

<span class="c1"># If use non-centered predictor, this will give the same model as</span>
<span class="c1"># model_adelie_flipper_regression</span>
<span class="n">jd_adelie_flipper_regression</span> <span class="o">=</span> <span class="n">gen_adelie_flipper_model</span><span class="p">(</span>
    <span class="n">adelie_flipper_length_c</span><span class="p">)</span>

<span class="n">mcmc_samples</span><span class="p">,</span> <span class="n">sampler_stats</span> <span class="o">=</span> <span class="n">run_mcmc</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">jd_adelie_flipper_regression</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">mass</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">adelie_mass_obs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>

<span class="n">inf_data_adelie_flipper_length_c</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="n">posterior</span><span class="o">=</span><span class="p">{</span>
        <span class="n">k</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
    <span class="n">sample_stats</span><span class="o">=</span><span class="p">{</span>
        <span class="n">k</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">sampler_stats</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;target_log_prob&quot;</span><span class="p">,</span> <span class="s2">&quot;diverging&quot;</span><span class="p">,</span> <span class="s2">&quot;accept_ratio&quot;</span><span class="p">,</span> <span class="s2">&quot;n_steps&quot;</span><span class="p">]}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-singlespecies-multipleregression-centered">
<a class="reference internal image-reference" href="../_images/SingleSpecies_MultipleRegression_Centered.png"><img alt="../_images/SingleSpecies_MultipleRegression_Centered.png" src="../_images/SingleSpecies_MultipleRegression_Centered.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.12 </span><span class="caption-text">Estimates of coefficients from Code Block
<a class="reference internal" href="#tfp-penguins-centered-predictor"><span class="std std-ref">tfp_penguins_centered_predictor</span></a>.
Notice that the distribution of <span class="math notranslate nohighlight">\(beta\_1\)</span> is the same as in
<a class="reference internal" href="#fig-adelie-coefficient-posterior-plots"><span class="std std-numref">Fig. 3.8</span></a>, but the distribution
of <span class="math notranslate nohighlight">\(beta\_0\)</span> has shifted. Because we centered the observations around
the mean of flipper length <span class="math notranslate nohighlight">\(beta\_0\)</span> now represents the mass
distribution of the average flipper penguin.</span><a class="headerlink" href="#fig-singlespecies-multipleregression-centered" title="Permalink to this image">¶</a></p>
</div>
<p>The mathematical model we defined in Code Block
<a class="reference internal" href="#tfp-penguins-centered-predictor"><span class="std std-ref">tfp_penguins_centered_predictor</span></a>
is identical to the PyMC3 model <code class="docutils literal notranslate"><span class="pre">model_adelie_flipper_regression</span></code> from
Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a>, with
sole difference being the centering of the predictor. PPL wise however,
the structure of TFP necessitates the addition of <code class="docutils literal notranslate"><span class="pre">tensor_x[...,</span> <span class="pre">None]</span></code>
in various lines to extend a batch of scalars so that they are
broadcastable with a batch of vectors. Specifically <code class="docutils literal notranslate"><span class="pre">None</span></code> appends a new
axis, which could also be done using <code class="docutils literal notranslate"><span class="pre">np.newaxis</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.newaxis</span></code>. We
also wrap the model in a function so we can easily condition on
different predictors. In this case we use the centered flipper length,
but could also use the non-centered predictor which will yield similar
results to our previous model.</p>
<p>When we plot our coefficients again, <span class="math notranslate nohighlight">\(\beta_1\)</span> is the same as our PyMC3
model but the distribution of <span class="math notranslate nohighlight">\(\beta_0\)</span> has changed. Since we have
centered our input data on its mean, the distribution of <span class="math notranslate nohighlight">\(\beta_0\)</span> is
the same as our prediction for the group mean with the non-centered
dataset. By centering the data we now can directly interpret <span class="math notranslate nohighlight">\(\beta_0\)</span>
as the distribution of mean masses for Adelie penguins with a mean
flipper length. The idea of transforming the input variables can also be
performed at arbitrary values of choice. For example, we could subtract
out the minimum flipper length and fit our model. In this transformation
this would change the interpretation <span class="math notranslate nohighlight">\(\beta_0\)</span> to the distribution of
means for the smallest observed flipper length. For a greater discussion
of transformations in linear regression we recommend Applied Regression
Analysis and Generalized Linear Models <span id="id10">[<a class="reference internal" href="references.html#id60" title="J. Fox. Applied Regression Analysis and Generalized Linear Models. SAGE Publications, 2015. ISBN 9781483321318.">31</a>]</span>.</p>
</div>
</div>
<div class="section" id="multiple-linear-regression">
<span id="id11"></span><h2><span class="section-number">3.3. </span>Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>In many species there is a dimorphism, or difference, between different
sexes. The study of sexual dimorphism in penguins actually was the
motivating factor for collecting the Palmer Penguin dataset
<span id="id12">[<a class="reference internal" href="references.html#id65" title="Kristen B Gorman, Tony D Williams, and William R Fraser. Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). PloS one, 9(3):e90081, 2014.">32</a>]</span>. To study penguin dimorphism more closely
let us add a second covariate, this time sex, encoding it as a
categorical variable and seeing if we can estimate a penguins mass more
precisely.</p>
<div class="literal-block-wrapper docutils container" id="penguin-mass-multi">
<div class="code-block-caption"><span class="caption-number">Listing 3.16 </span><span class="caption-text">penguin_mass_multi</span><a class="headerlink" href="#penguin-mass-multi" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Binary encoding of the categorical predictor</span>
<span class="n">sex_obs</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">adelie_mask</span> <span class="p">,</span><span class="s2">&quot;sex&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="s2">&quot;male&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;female&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_penguin_mass_categorical</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
    <span class="n">β_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_0&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">β_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_1&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>
    <span class="n">β_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_2&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3000</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span>
        <span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">β_1</span> <span class="o">*</span> <span class="n">adelie_flipper_length_obs</span> <span class="o">+</span> <span class="n">β_2</span> <span class="o">*</span> <span class="n">sex_obs</span><span class="p">)</span>

    <span class="n">mass</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mass&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">adelie_mass_obs</span><span class="p">)</span>

    <span class="n">inf_data_penguin_mass_categorical</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
        <span class="n">target_accept</span><span class="o">=</span><span class="mf">.9</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>You will notice a new parameter, <span class="math notranslate nohighlight">\(\beta_{2}\)</span> contributing to the value
of <span class="math notranslate nohighlight">\(\mu\)</span>. As sex is a categorical predictor (in this example just female
or male), we encode it as 1 and 0, respectively. For the model this
means that the value of <span class="math notranslate nohighlight">\(\mu\)</span>, for females, is a sum over 3 terms while
for males is a sum of two terms (as the <span class="math notranslate nohighlight">\(\beta_2\)</span> term will zero out).</p>
<div class="figure align-default" id="fig-adelie-sex-coefficient-posterior">
<a class="reference internal image-reference" href="../_images/adelie_sex_coefficient_posterior.png"><img alt="../_images/adelie_sex_coefficient_posterior.png" src="../_images/adelie_sex_coefficient_posterior.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.13 </span><span class="caption-text">Estimate of coefficient for sex covariate, <span class="math notranslate nohighlight">\(\beta_{2}\)</span> in model. As male
is encoded as 0, and female is encoded as 1, this indicates the
additional mass we would expect between a male and female Adelie penguin
with the same flipper length.</span><a class="headerlink" href="#fig-adelie-sex-coefficient-posterior" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-syntactic-linear-sugar admonition">
<p class="admonition-title">Syntactic Linear Sugar</p>
<p>Linear models are so widely used that specialized
syntax, methods, and libraries have been written just for regression.
One such library is Bambi (BAyesian Model-Building
Interface<span id="id13">[<a class="reference internal" href="references.html#id118" title="Tomás Capretto, Camen Piho, Ravin Kumar, Jacob Westfall, Tal Yarkoni, and Osvaldo A Martin. Bambi: a simple interface for fitting bayesian linear models in python. arXiv preprint arXiv:2012.10754, 2020.">33</a>]</span>). Bambi is a Python package for fitting
generalized linear hierarchical models using a formula-based syntax,
similar to what one might find in R packages, like lme4 <span id="id14">[<a class="reference internal" href="references.html#id82" title="Douglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. Fitting linear mixed-effects models using lme4. arXiv preprint arXiv:1406.5823, 2014.">34</a>]</span>, nlme
<span id="id15">[<a class="reference internal" href="references.html#id81" title="Jose Pinheiro, Douglas Bates, Saikat DebRoy, Deepayan Sarkar, and R Core Team. nlme: Linear and Nonlinear Mixed Effects Models. 2020. R package version 3.1-151. URL: https://CRAN.R-project.org/package=nlme.">35</a>]</span>, rstanarm <span id="id16">[<a class="reference internal" href="references.html#id83" title="Jonah Gabry and Ben Goodrich. Estimating generalized (non-)linear models with group-specific terms with rstanarm. 6 2020. URL: https://mc-stan.org/rstanarm/articles/glmer.html.">36</a>]</span> or brms <span id="id17">[<a class="reference internal" href="references.html#id84" title="Paul-Christian Bürkner. Brms: an r package for bayesian multilevel models using stan. Journal of statistical software, 80(1):1–28, 2017.">37</a>]</span>). Bambi uses
PyMC3 underneath and provides a higher level API. To write the same
model, if disregarding the priors<a class="footnote-reference brackets" href="#id38" id="id18">4</a> as the one in Code Block
<a class="reference internal" href="#penguin-mass-multi"><span class="std std-ref">penguin_mass_multi</span></a> in Bambi we would
write:</p>
<div class="literal-block-wrapper docutils container" id="bambi-categorical">
<div class="code-block-caption"><span class="caption-number">Listing 3.17 </span><span class="caption-text">bambi_categorical</span><a class="headerlink" href="#bambi-categorical" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">bambi</span> <span class="k">as</span> <span class="nn">bmb</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">bmb</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="s2">&quot;body_mass_g ~ flipper_length_mm + sex&quot;</span><span class="p">,</span>
                  <span class="n">penguins</span><span class="p">[</span><span class="n">adelie_mask</span><span class="p">])</span>
<span class="n">trace</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>The priors are automatically assigned if not provided, as is the case in
the code example above. Internally, Bambi stores virtually all objects
generated by PyMC3, making it easy for users to retrieve, inspect, and
modify those objects. Additionally Bambi returns an <code class="docutils literal notranslate"><span class="pre">az.InferenceData</span></code>
object which can be directly used with ArviZ.</p>
</div>
<p>Since we have encoded male as 0 this posterior from
<code class="docutils literal notranslate"><span class="pre">model_penguin_mass_categorical</span></code> estimates the difference in mass
compared to a female Adelie penguin <em>with the same flipper length</em>. This
last part is quite important, by adding a second covariate we now have a
multiple linear regression and we must use more caution when
interpreting the coefficients. In this case, the coefficients provides
the relationship of a covariate into the response variable, <strong>if</strong> all
other covariates are held constant <a class="footnote-reference brackets" href="#id39" id="id19">5</a>.</p>
<div class="figure align-default" id="fig-single-species-categorical-regression">
<a class="reference internal image-reference" href="../_images/Single_Species_Categorical_Regression.png"><img alt="../_images/Single_Species_Categorical_Regression.png" src="../_images/Single_Species_Categorical_Regression.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.14 </span><span class="caption-text">Multiple regression for flipper length versus mass with male and female
Adelie penguins coded as a categorical covariate. Note how the
difference mass between male and female penguins is constant at every
flipper length. This difference is equivalent to the magnitude of the
<span class="math notranslate nohighlight">\(\beta_2\)</span> coefficient.</span><a class="headerlink" href="#fig-single-species-categorical-regression" title="Permalink to this image">¶</a></p>
</div>
<p>We again can compare the standard deviations across our three models in
<a class="reference internal" href="#fig-singlespecies-multipleregression-forest-sigma-comparison"><span class="std std-numref">Fig. 3.15</span></a>
to see if we have reduced uncertainty in our estimate and once again the
additional information has helped to improve the estimate. In this case
our estimate of <span class="math notranslate nohighlight">\(\sigma\)</span> has dropped a mean of 462 grams in our no
covariate model defined in Code Block
<a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a> to a mean value 298 grams
from the linear model defined in Code Block
<a class="reference internal" href="#penguin-mass-multi"><span class="std std-ref">penguin_mass_multi</span></a> that includes
flipper length and sex as a covariates. This reduction in uncertainty
suggests that sex does indeed provide information for estimating a
penguin’s mass.</p>
<div class="literal-block-wrapper docutils container" id="forest-multiple-models">
<div class="code-block-caption"><span class="caption-number">Listing 3.18 </span><span class="caption-text">forest_multiple_models</span><a class="headerlink" href="#forest-multiple-models" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">([</span><span class="n">inf_data_adelie_penguin_mass</span><span class="p">,</span>
        <span class="n">inf_data_adelie_flipper_regression</span><span class="p">,</span>
        <span class="n">inf_data_penguin_mass_categorical</span><span class="p">],</span>
        <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;σ&quot;</span><span class="p">],</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-singlespecies-multipleregression-forest-sigma-comparison">
<a class="reference internal image-reference" href="../_images/SingleSpecies_MultipleRegression_Forest_Sigma_Comparison.png"><img alt="../_images/SingleSpecies_MultipleRegression_Forest_Sigma_Comparison.png" src="../_images/SingleSpecies_MultipleRegression_Forest_Sigma_Comparison.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.15 </span><span class="caption-text">By incorporating sex as a covariate in <code class="docutils literal notranslate"><span class="pre">model_penguin_mass_categorical</span></code>
the estimated distribution of <span class="math notranslate nohighlight">\(\sigma\)</span> from this model is centered
around 300 grams, which is a lower value than the estimated by our fixed mean
model and our single covariate model. This figure is generated from Code
Block <a class="reference internal" href="#forest-multiple-models"><span class="std std-ref">forest_multiple_models</span></a>.</span><a class="headerlink" href="#fig-singlespecies-multipleregression-forest-sigma-comparison" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-more-covariates-is-not-always-better admonition">
<p class="admonition-title">More covariates is not always better</p>
<p>All model fitting algorithms will find a signal, even if it is random noise. This phenomenon is called
overfitting and it describes a condition where the algorithm can quite
handily map covariates to outcomes in seen cases, but fails to
generalize to new observations. In linear regressions we can show this
by generating 100 random covariates, and fitting them to a random
simulated dataset <span id="id20">[<a class="reference internal" href="references.html#id33" title="R. McElreath. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2020. ISBN 9781482253481.">10</a>]</span>. Even though there is no relation,
we would be led to believe our linear model is doing quite well.</p>
</div>
<div class="section" id="counterfactuals">
<span id="linear-counter-factuals"></span><h3><span class="section-number">3.3.1. </span>Counterfactuals<a class="headerlink" href="#counterfactuals" title="Permalink to this headline">¶</a></h3>
<p>In Code Block <a class="reference internal" href="#penguins-ppd"><span class="std std-ref">penguins_ppd</span></a> we made a
prediction using parameters fitted in a model with a single covariate
and our target, and changing that covariate, flipper length, to get an
estimate of mass at that fixed flipper length. In multiple regression,
we can do something similar, where we take our regression, hold all
covariates constant except one, and see how that change to that one
covariate changes our expected outcome. This analysis is called a
counterfactual analysis. Let us extend the multiple regression from the
previous section (Code Block
<a class="reference internal" href="#penguin-mass-multi"><span class="std std-ref">penguin_mass_multi</span></a>), this time
including bill length, and run a counterfactual analysis in TFP. The
model building and inference is shown in Code Block
<a class="reference internal" href="#tfp-flipper-bill-sex"><span class="std std-ref">tfp_flipper_bill_sex</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="tfp-flipper-bill-sex">
<div class="code-block-caption"><span class="caption-number">Listing 3.19 </span><span class="caption-text">tfp_flipper_bill_sex</span><a class="headerlink" href="#tfp-flipper-bill-sex" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_jd_flipper_bill_sex</span><span class="p">(</span><span class="n">flipper_length</span><span class="p">,</span> <span class="n">sex</span><span class="p">,</span> <span class="n">bill_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="n">flipper_length</span><span class="p">,</span> <span class="n">sex</span><span class="p">,</span> <span class="n">bill_length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="p">),</span>
        <span class="p">(</span><span class="n">flipper_length</span><span class="p">,</span> <span class="n">sex</span><span class="p">,</span> <span class="n">bill_length</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
    <span class="k">def</span> <span class="nf">jd_flipper_bill_sex</span><span class="p">():</span>
        <span class="n">σ</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sigma&quot;</span><span class="p">))</span>
        <span class="n">β_0</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_0&quot;</span><span class="p">))</span>
        <span class="n">β_1</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_1&quot;</span><span class="p">))</span>
        <span class="n">β_2</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_2&quot;</span><span class="p">))</span>
        <span class="n">β_3</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_3&quot;</span><span class="p">))</span>
        <span class="n">μ</span> <span class="o">=</span> <span class="p">(</span><span class="n">β_0</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
             <span class="o">+</span> <span class="n">β_1</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">flipper_length</span>
             <span class="o">+</span> <span class="n">β_2</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">sex</span>
             <span class="o">+</span> <span class="n">β_3</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">bill_length</span>
            <span class="p">)</span>
        <span class="n">mass</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">σ</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
            <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;mass&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jd_flipper_bill_sex</span>

<span class="n">bill_length_obs</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">adelie_mask</span><span class="p">,</span> <span class="s2">&quot;bill_length_mm&quot;</span><span class="p">]</span>
<span class="n">jd_flipper_bill_sex</span> <span class="o">=</span> <span class="n">gen_jd_flipper_bill_sex</span><span class="p">(</span>
    <span class="n">adelie_flipper_length_obs</span><span class="p">,</span> <span class="n">sex_obs</span><span class="p">,</span> <span class="n">bill_length_obs</span><span class="p">)</span>

<span class="n">mcmc_samples</span><span class="p">,</span> <span class="n">sampler_stats</span> <span class="o">=</span> <span class="n">run_mcmc</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">jd_flipper_bill_sex</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">mass</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">adelie_mass_obs</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>In this model you will note the addition of another coefficient <code class="docutils literal notranslate"><span class="pre">beta_3</span></code>
to correspond to the addition of bill length as a covariate. After
inference, we can simulate the mass of penguins with different fictional
flipper lengths, while holding the sex constant at male, and the bill
length at the observed mean of the dataset. This is done in Code Block
<a class="reference internal" href="#tfp-flipper-bill-sex-counterfactuals"><span class="std std-ref">tfp_flipper_bill_sex_counterfactuals</span></a>
with the result shown in <a class="reference internal" href="#fig-linearcounterfactual"><span class="std std-numref">Fig. 3.16</span></a>. Again since
we wrap the model generation in a Python function (a functional
programming style approach), it is easy to condition on new predictors,
which is useful for counterfactual analyses.</p>
<div class="literal-block-wrapper docutils container" id="tfp-flipper-bill-sex-counterfactuals">
<div class="code-block-caption"><span class="caption-number">Listing 3.20 </span><span class="caption-text">tfp_flipper_bill_sex_counterfactuals</span><a class="headerlink" href="#tfp-flipper-bill-sex-counterfactuals" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mean_flipper_length</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">adelie_mask</span><span class="p">,</span> <span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># Counterfactual dimensions is set to 21 to allow us to get the mean exactly</span>
<span class="n">counterfactual_flipper_lengths</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span>
    <span class="n">mean_flipper_length</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="n">mean_flipper_length</span><span class="o">+</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">sex_male_indicator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">counterfactual_flipper_lengths</span><span class="p">)</span>
<span class="n">mean_bill_length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span>
    <span class="n">counterfactual_flipper_lengths</span><span class="p">)</span> <span class="o">*</span> <span class="n">bill_length_obs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">jd_flipper_bill_sex_counterfactual</span> <span class="o">=</span> <span class="n">gen_jd_flipper_bill_sex</span><span class="p">(</span>
    <span class="n">counterfactual_flipper_lengths</span><span class="p">,</span> <span class="n">sex_male_indicator</span><span class="p">,</span> <span class="n">mean_bill_length</span><span class="p">)</span>
<span class="n">ppc_samples</span> <span class="o">=</span> <span class="n">jd_flipper_bill_sex_counterfactual</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">mcmc_samples</span><span class="p">)</span>
<span class="n">estimated_mass</span> <span class="o">=</span> <span class="n">ppc_samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-linearcounterfactual">
<a class="reference internal image-reference" href="../_images/Linear_CounterFactual.png"><img alt="../_images/Linear_CounterFactual.png" src="../_images/Linear_CounterFactual.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.16 </span><span class="caption-text">Estimated counterfactual mass values for Adelie penguins from Code Block
<a class="reference internal" href="#tfp-flipper-bill-sex-counterfactuals"><span class="std std-ref">tfp_flipper_bill_sex_counterfactuals</span></a>
where flipper length is varied holding all other covariates constant.</span><a class="headerlink" href="#fig-linearcounterfactual" title="Permalink to this image">¶</a></p>
</div>
<p>Following McElreath<span id="id21">[<a class="reference internal" href="references.html#id33" title="R. McElreath. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2020. ISBN 9781482253481.">10</a>]</span> <a class="reference internal" href="#fig-linearcounterfactual"><span class="std std-numref">Fig. 3.16</span></a>
is called a counterfactual plot. As the word counterfactual implies, we
are evaluating a situation counter to the observed data, or facts. In
other words, we are evaluating situations that have not happened. The
simplest use of a counterfactual plot is to adjust a covariate and
explore the result, exactly like we just did. This is great, as it
enables us to explore <em>what-if</em> scenarios, that could be beyond our
reach otherwise <a class="footnote-reference brackets" href="#id40" id="id22">6</a>. However, we must be cautious when interpreting
this trickery. The first trap is that counterfactual values may be
impossible, for example, no penguin may ever exist with a flipper length
larger than 1500mm but the model will happily give us estimates for this
fictional penguin. The second is more insidious, we assumed that we
could vary each covariate independently, but in reality this may not be
possible. For example, as a penguin’s flipper length increases, its bill
length may as well. Counterfactuals are powerful in that they allow us
to explore outcomes that have not happened, or that we at least did not
observe happen. But they can easily generate estimates for situations
that will <em>never</em> happen. It is the model that will not discern between
the two, so you as a modeler must.</p>
<div class="admonition-correlation-vs-causality admonition">
<p class="admonition-title">Correlation vs Causality</p>
<p>When interpreting linear regressions it is tempting to say “An increase
in <span class="math notranslate nohighlight">\(X\)</span> <strong>causes</strong> and increase in <span class="math notranslate nohighlight">\(Y\)</span>”. This is not necessarily the
case, in fact causal statements can not be made from a (linear)
regression alone. Mathematically a linear model links two (or more
variables) together but this link does not need to be causal. For
example, increasing the amount of water we provide to a plant can
certainly (and causally) increase the plant’s growth (at least within
some range), but nothing prevents us from inverting this relationship in
a model and use the growth of plants to estimate the amount of rain,
even when plant growth do not cause rain <a class="footnote-reference brackets" href="#id41" id="id23">7</a>. The statistical sub-field
of Causal Inference is concerned with the tools and procedures necessary
to make causal statements either in the context of randomized
experiments or observational studies (see Chapter <a class="reference internal" href="chp_07.html#chap6"><span class="std std-ref">7</span></a> for
a brief discussion)</p>
</div>
</div>
</div>
<div class="section" id="generalized-linear-models">
<span id="id24"></span><h2><span class="section-number">3.4. </span>Generalized Linear Models<a class="headerlink" href="#generalized-linear-models" title="Permalink to this headline">¶</a></h2>
<p>All linear models discussed so far assumed the distribution of
observations are conditionally Gaussian which works well in many
scenarios. However, we may want to use other distributions. For example,
to model things that are restricted to some interval, a number in the
interval <span class="math notranslate nohighlight">\([0, 1]\)</span> like probabilities, or natural numbers
<span class="math notranslate nohighlight">\(\{1, 2, 3, \dots \}\)</span> like counting events. To do this we will take our
linear function, <span class="math notranslate nohighlight">\(\mathbf{X} \mathit{\beta}\)</span>, and modify it using an
inverse link function <a class="footnote-reference brackets" href="#id42" id="id25">8</a> <span class="math notranslate nohighlight">\(\phi\)</span> as shown in Equation
<a class="reference internal" href="#equation-eq-generalized-linear-model">(3.5)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-generalized-linear-model">
<span class="eqno">(3.5)<a class="headerlink" href="#equation-eq-generalized-linear-model" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}\begin{split}\begin{split}
\mu =&amp; \phi(\mathbf{X} \beta) \\
Y \sim&amp; \Psi (\mu, \theta)\end{split}\\\end{split}\end{aligned}\end{align} \]</div>
<p>where <span class="math notranslate nohighlight">\(\Psi\)</span> is some distribution parameterized by <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span>
indicating the data likelihood.</p>
<p>The specific purpose of the inverse link function is to map outputs from
the range of real numbers <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span> to a parameter range of
the restricted interval. In other words the inverse link function is the
specific “trick” we need to take our linear models and generalize them
to many more model architectures. We are still dealing with a linear model
here in the sense that the expectation of the distribution that
generates the observation still follows a linear function of the
parameter and the covariates but now we can generalize the use and
application of these models to many more scenarios <a class="footnote-reference brackets" href="#id43" id="id26">9</a>.</p>
<div class="section" id="logistic-regression">
<span id="id27"></span><h3><span class="section-number">3.4.1. </span>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h3>
<p>One of the most common generalized linear model is the logistic
regression. It is particularly useful in modeling data where there are
only two possible outcomes, we observed either one thing or another
thing. The probability of a head or tails outcome in a coin flip is the
usual textbook example. More “real world” examples includes the chance
of a defect in manufacturing, a negative or positive cancer test, or the
failure of a rocket launch <span id="id28">[<a class="reference internal" href="references.html#id79" title="C. Davidson-Pilon. Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference. Addison-Wesley Data &amp; Analytics Series. Pearson Education, 2015. ISBN 9780133902921.">38</a>]</span>. In a logistic
regression the inverse link function is called, unsurprisingly, the
logistic function, which maps <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span> to the <span class="math notranslate nohighlight">\((0,1)\)</span>
interval. This is handy because now we can map linear functions to the
range we would expect for a parameter that estimates probability values,
that must be in the range 0 and 1 by definition.</p>
<div class="math notranslate nohighlight" id="equation-eq-logistic">
<span class="eqno">(3.6)<a class="headerlink" href="#equation-eq-logistic" title="Permalink to this equation">¶</a></span>\[p = \frac{1}{1+e^{-\mathbf{X}\beta}}\]</div>
<div class="figure align-default" id="fig-logistic">
<a class="reference internal image-reference" href="../_images/Logistic.png"><img alt="../_images/Logistic.png" src="../_images/Logistic.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.17 </span><span class="caption-text">A plot of a sample logistic function. Note the response has been
“squished” into the interval (0,1).</span><a class="headerlink" href="#fig-logistic" title="Permalink to this image">¶</a></p>
</div>
<p>With logistic regression we are able to use linear models to estimate
probabilities of an event. Sometimes, instead we want to classify, or to
predict, a specific class given some data. In order to do so we want to
turn the continuous prediction in the interval <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span> to
one between 0 and 1. We can do this with a decision boundary to make a
prediction in the set <span class="math notranslate nohighlight">\({0,1}\)</span>. Let us assume we want our decision
boundary set at a probability of 0.5. For a model with an intercept and
one covariate we have:</p>
<div class="math notranslate nohighlight" id="equation-decision-boundary">
<span class="eqno">(3.7)<a class="headerlink" href="#equation-decision-boundary" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
0.5 &amp;= logistic(\beta_{0} + \beta_{1}*x) \\
logit(0.5) &amp;= \beta_{0} + \beta_{1}*x \\
0 &amp;= \beta_{0} + \beta_{1}*x \\
x &amp;= -\frac{\beta_{0}}{\beta_{1}} \\
\end{split}\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(logit\)</span> is the inverse of <span class="math notranslate nohighlight">\(logistic\)</span>. That is, once a logistic
model is fitted we can use the coefficients <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> to
easily compute the value of <span class="math notranslate nohighlight">\(x\)</span> for which the probability of the class
is greater than 0.5.</p>
</div>
<div class="section" id="classifying-penguins">
<span id="id29"></span><h3><span class="section-number">3.4.2. </span>Classifying Penguins<a class="headerlink" href="#classifying-penguins" title="Permalink to this headline">¶</a></h3>
<p>In the previous sections we used the sex, and bill length of a penguin
to estimate the mass of a penguin. Lets now alter the question, if we
were given the mass, sex, and bill length of a penguin can we predict
the species? Let us use two species Adelie and Chinstrap to make this a
binary task. Like last time we use a simple model first with just one
covariate, bill length. We write this logistic model in Code Block
<a class="reference internal" href="#model-logistic-penguins-bill-length"><span class="std std-ref">model_logistic_penguins_bill_length</span></a></p>
<div class="literal-block-wrapper docutils container" id="model-logistic-penguins-bill-length">
<div class="code-block-caption"><span class="caption-number">Listing 3.21 </span><span class="caption-text">model_logistic_penguins_bill_length</span><a class="headerlink" href="#model-logistic-penguins-bill-length" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">species_filter</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s2">&quot;species&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s2">&quot;Adelie&quot;</span><span class="p">,</span> <span class="s2">&quot;Chinstrap&quot;</span><span class="p">])</span>
<span class="n">bill_length_obs</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">species_filter</span><span class="p">,</span> <span class="s2">&quot;bill_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">species</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">species_filter</span><span class="p">,</span> <span class="s2">&quot;species&quot;</span><span class="p">])</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_logistic_penguins_bill_length</span><span class="p">:</span>
    <span class="n">β_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_0&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_1&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">bill_length_obs</span><span class="p">,</span> <span class="n">β_1</span><span class="p">)</span>

    <span class="c1"># Application of our sigmoid  link function</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;θ&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">μ</span><span class="p">))</span>

    <span class="c1"># Useful for plotting the decision boundary later</span>
    <span class="n">bd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;bd&quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">β_0</span><span class="o">/</span><span class="n">β_1</span><span class="p">)</span>

    <span class="c1"># Note the change in likelihood</span>
    <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;yl&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">species</span><span class="o">.</span><span class="n">codes</span><span class="p">)</span>

    <span class="n">prior_predictive_logistic_penguins_bill_length</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
    <span class="n">trace_logistic_penguins_bill_length</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">inf_data_logistic_penguins_bill_length</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior_predictive_logistic_penguins_bill_length</span><span class="p">,</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_logistic_penguins_bill_length</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In generalized linear models, the mapping from parameter prior to
response can sometimes be more challenging to understand. We can utilize
prior predictive samples to help us visualize the expected observations.
In our classifying penguins example we find it reasonable to equally
expect a Chinstrap penguin, as we would an Adelie penguin, at all bill
lengths, prior to seeing any data. We can double-check our modeling
intention has been represented correctly by our priors and model using
the prior predictive distribution. The classes are roughly even in
<a class="reference internal" href="#fig-prior-predictive-logistic"><span class="std std-numref">Fig. 3.18</span></a> prior to seeing data which is
what we would expect.</p>
<div class="figure align-default" id="fig-prior-predictive-logistic">
<a class="reference internal image-reference" href="../_images/Prior_Predictive_Logistic.png"><img alt="../_images/Prior_Predictive_Logistic.png" src="../_images/Prior_Predictive_Logistic.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.18 </span><span class="caption-text">5000 prior predictive samples of class prediction from the
<code class="docutils literal notranslate"><span class="pre">model_logistic_penguins_bill_length</span></code>. This likelihood is discrete, more
specifically binary, as opposed to the continuous distribution of mass
that was being estimated in earlier models.</span><a class="headerlink" href="#fig-prior-predictive-logistic" title="Permalink to this image">¶</a></p>
</div>
<p>After fitting the parameters in our model we can inspect the
coefficients using <code class="docutils literal notranslate"><span class="pre">az.summary(.)</span></code> function (see <a class="reference internal" href="#table-logistic-penguins-bill-length"><span class="std std-numref">Table 3.3</span></a>).
While we can read the
coefficients they are not as directly interpretable as in a linear
regression. We can tell there is some relationship with bill length and
species given the positive <span class="math notranslate nohighlight">\(\beta_1\)</span> coefficient whose HDI does not
cross zero. We can interpret the decision boundary fairly directly
seeing that around 44 mm in bill length is the nominal cutoff for one
species to another. Plotting the regression output in
<a class="reference internal" href="#fig-logistic-bill-length"><span class="std std-numref">Fig. 3.19</span></a> is much more intuitive. Here we see
the now familiar logistic curve move from 0 on the left to 1 on the
right as the classes change, and a decision boundary where one would
expect it given the data.</p>
<div class="figure align-default" id="fig-logistic-bill-length">
<a class="reference internal image-reference" href="../_images/Logistic_bill_length.png"><img alt="../_images/Logistic_bill_length.png" src="../_images/Logistic_bill_length.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.19 </span><span class="caption-text">Fitted logistic regression, showing probability curve, observed data
points and decision boundary for <code class="docutils literal notranslate"><span class="pre">model_logistic_penguins_bill_length</span></code>.
Looking at just the observed data it seems there is a separation around
45mm bill length for both species, and our model similarly discerned the
separation around that value.</span><a class="headerlink" href="#fig-logistic-bill-length" title="Permalink to this image">¶</a></p>
</div>
<table class="table" id="table-logistic-penguins-bill-length">
<caption><span class="caption-number">Table 3.3 </span><span class="caption-text">Logistic regression coefficients for model_logistic_penguins_bill_length.</span><a class="headerlink" href="#table-logistic-penguins-bill-length" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p>-46.052</p></td>
<td><p>7.073</p></td>
<td><p>-58.932</p></td>
<td><p>-34.123</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p>1.045</p></td>
<td><p>0.162</p></td>
<td><p>0.776</p></td>
<td><p>1.347</p></td>
</tr>
</tbody>
</table>
<p>Let us try something different, we still want to classify penguins but
this time using mass as a covariate. Code Block
<a class="reference internal" href="#model-logistic-penguins-mass"><span class="std std-ref">model_logistic_penguins_mass</span></a>
shows a model for that purpose.</p>
<div class="literal-block-wrapper docutils container" id="model-logistic-penguins-mass">
<div class="code-block-caption"><span class="caption-number">Listing 3.22 </span><span class="caption-text">model_logistic_penguins_mass</span><a class="headerlink" href="#model-logistic-penguins-mass" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mass_obs</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">species_filter</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_logistic_penguins_mass</span><span class="p">:</span>
    <span class="n">β_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_0&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_1&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mass_obs</span><span class="p">,</span> <span class="n">β_1</span><span class="p">)</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;θ&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">μ</span><span class="p">))</span>
    <span class="n">bd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;bd&quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">β_0</span><span class="o">/</span><span class="n">β_1</span><span class="p">)</span>

    <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;yl&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">species</span><span class="o">.</span><span class="n">codes</span><span class="p">)</span>

    <span class="n">inf_data_logistic_penguins_mass</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
        <span class="mi">5000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">.9</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<table class="table" id="table-logistic-penguins-mass">
<caption><span class="caption-number">Table 3.4 </span><span class="caption-text">Logistic regression coefficients for model_logistic_penguins_mass.</span><a class="headerlink" href="#table-logistic-penguins-mass" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta_0\)</span></p></td>
<td><p>-1.131</p></td>
<td><p>1.317</p></td>
<td><p>-3.654</p></td>
<td><p>1.268</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\beta_1\)</span></p></td>
<td><p>0.000</p></td>
<td><p>0.000</p></td>
<td><p>0.000</p></td>
<td><p>0.001</p></td>
</tr>
</tbody>
</table>
<p>Our tabular summary in <a class="reference internal" href="#table-logistic-penguins-mass"><span class="std std-numref">Table 3.4</span></a> shows
that <span class="math notranslate nohighlight">\(\beta_1\)</span> is estimated to be 0 indicating there is not enough
information in the mass covariate to separate the two classes. This is
not necessarily a bad thing, just the model indicating to us that it
does not find discernible difference in mass between these two species.
This becomes quite evident once we plot the data and logistic regression
fit in <a class="reference internal" href="#fig-logistic-mass"><span class="std std-numref">Fig. 3.20</span></a>.</p>
<div class="figure align-default" id="fig-logistic-mass">
<a class="reference internal image-reference" href="../_images/Logistic_mass.png"><img alt="../_images/Logistic_mass.png" src="../_images/Logistic_mass.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.20 </span><span class="caption-text">Plot of the observed data and logistic regression for
<code class="docutils literal notranslate"><span class="pre">model_logistic_penguins_mass</span></code>. Unlike
<a class="reference internal" href="#fig-logistic-bill-length"><span class="std std-numref">Fig. 3.19</span></a> the data does not look very separable
and our model did discern one as well.</span><a class="headerlink" href="#fig-logistic-mass" title="Permalink to this image">¶</a></p>
</div>
<p>We should not let this lack of relationship discourage us, effective
modeling includes a dose of trial an error. This does not mean try
random things and hope they work, it instead means that it is ok to use
the computational tools to provide you clues to the next step.</p>
<p>Let us now try using both bill length and mass to create a multiple
logistic regression in Code Block
<a class="reference internal" href="#model-logistic-penguins-bill-length-mass"><span class="std std-ref">model_logistic_penguins_bill_length_mass</span></a>
and plot the decision boundary again in
<a class="reference internal" href="#fig-decision-boundary-logistic-mass-bill-length"><span class="std std-numref">Fig. 3.21</span></a>. This time the
axes of the figure are a little bit different. Instead of the
probability of class on the Y-axis, we instead have mass. This way we
can see the decision boundary between the dependent variables. All these
visual checks have been helpful but subjective. We can quantify our fits
numerically as well using diagnostics.</p>
<div class="literal-block-wrapper docutils container" id="model-logistic-penguins-bill-length-mass">
<div class="code-block-caption"><span class="caption-number">Listing 3.23 </span><span class="caption-text">model_logistic_penguins_bill_length_mass</span><a class="headerlink" href="#model-logistic-penguins-bill-length-mass" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">species_filter</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;bill_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]]</span>

<span class="c1"># Add a column of 1s for the intercept</span>
<span class="n">X</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="s2">&quot;Intercept&quot;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_logistic_penguins_bill_length_mass</span><span class="p">:</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>

    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;θ&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">μ</span><span class="p">))</span>
    <span class="n">bd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;bd&quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">β</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">β</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

    <span class="n">yl</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s2">&quot;yl&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">species</span><span class="o">.</span><span class="n">codes</span><span class="p">)</span>

    <span class="n">inf_data_logistic_penguins_bill_length_mass</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
        <span class="mi">1000</span><span class="p">,</span>
        <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-decision-boundary-logistic-mass-bill-length">
<a class="reference internal image-reference" href="../_images/Decision_Boundary_Logistic_mass_bill_length.png"><img alt="../_images/Decision_Boundary_Logistic_mass_bill_length.png" src="../_images/Decision_Boundary_Logistic_mass_bill_length.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.21 </span><span class="caption-text">Decision boundary of species class plotted against bill length and mass.
We can see that most of the species separability comes from bill length
although mass now adds some extra information in regards to class
separability as indicated by the slope of the line.</span><a class="headerlink" href="#fig-decision-boundary-logistic-mass-bill-length" title="Permalink to this image">¶</a></p>
</div>
<p>To evaluate the model fit for logistic regressions we can use a
separation plot <span id="id30">[<a class="reference internal" href="references.html#id172" title="Brian Greenhill, Michael D Ward, and Audrey Sacks. The separation plot: a new visual method for evaluating the fit of binary models. American Journal of Political Science, 55(4):991–1002, 2011.">39</a>]</span>, as shown in Code Block
<a class="reference internal" href="#separability-plot"><span class="std std-ref">separability_plot</span></a> and
<a class="reference internal" href="#fig-penguins-separation-plot"><span class="std std-numref">Fig. 3.22</span></a>. A separation plot is a way to
assess the calibration of a model with binary observed data. It shows
the sorted predictions per class, the idea being that with perfect
separation there would be two distinct rectangles. In our case we see
that none of our models did a perfect job separating the two species,
but the models that included bill length performed much better than the
model that included mass only. In general, perfect calibration is not
the goal of a Bayesian analysis, nevertheless separation plots (and
other calibration assessment methods like LOO-PIT) can help us to
compare models and reveal opportunities to improve them.</p>
<div class="literal-block-wrapper docutils container" id="separability-plot">
<div class="code-block-caption"><span class="caption-number">Listing 3.24 </span><span class="caption-text">separability_plot</span><a class="headerlink" href="#separability-plot" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;bill&quot;</span><span class="p">:</span> <span class="n">inf_data_logistic_penguins_bill_length</span><span class="p">,</span>
          <span class="s2">&quot;mass&quot;</span><span class="p">:</span> <span class="n">inf_data_logistic_penguins_mass</span><span class="p">,</span>
          <span class="s2">&quot;mass bill&quot;</span><span class="p">:</span> <span class="n">inf_data_logistic_penguins_bill_length_mass</span><span class="p">}</span>

<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">model</span><span class="p">),</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">az</span><span class="o">.</span><span class="n">plot_separation</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C4&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-penguins-separation-plot">
<a class="reference internal image-reference" href="../_images/Penguins_Separation_Plot.png"><img alt="../_images/Penguins_Separation_Plot.png" src="../_images/Penguins_Separation_Plot.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.22 </span><span class="caption-text">Separation plot of all three penguin models. The light versus dark value
indicates the binary class label. In this plot its much more evident
that the mass only model does a poor job separating the two species,
where are the bill and mass bill models perform better at this task.</span><a class="headerlink" href="#fig-penguins-separation-plot" title="Permalink to this image">¶</a></p>
</div>
<p>We can also use LOO to compare the three models we have just created,
the one for the mass, the one for the bill length and the one including
both covariates in Code Block
<a class="reference internal" href="#penguin-model-loo"><span class="std std-ref">penguin_model_loo</span></a> and <a class="reference internal" href="#tab-penguin-loo"><span class="std std-numref">Table 3.5</span></a>. According to LOO the mass only
model is the worst at separating the species, the bill length only is
the middle candidate model, and the mass and bill length model performed
the best. This is unsurprising given what we have seen from the plots,
and now we have a numerical confirmation as well.</p>
<div class="literal-block-wrapper docutils container" id="penguin-model-loo">
<div class="code-block-caption"><span class="caption-number">Listing 3.25 </span><span class="caption-text">penguin_model_loo</span><a class="headerlink" href="#penguin-model-loo" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">({</span><span class="s2">&quot;mass&quot;</span><span class="p">:</span><span class="n">inf_data_logistic_penguins_mass</span><span class="p">,</span>
            <span class="s2">&quot;bill&quot;</span><span class="p">:</span> <span class="n">inf_data_logistic_penguins_bill_length</span><span class="p">,</span>
            <span class="s2">&quot;mass_bill&quot;</span><span class="p">:</span><span class="n">inf_data_logistic_penguins_bill_length_mass</span><span class="p">})</span>
</pre></div>
</div>
</div>
<table class="table" id="tab-penguin-loo">
<caption><span class="caption-number">Table 3.5 </span><span class="caption-text">Summary of model comparison. Models are ranked from lowest to highest ELPD values (loo column).</span><a class="headerlink" href="#tab-penguin-loo" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>rank</strong></p></td>
<td><p><strong>loo</strong></p></td>
<td><p><strong>p_loo</strong></p></td>
<td><p><strong>d_loo</strong></p></td>
<td><p><strong>weight</strong></p></td>
<td><p><strong>se</strong></p></td>
<td><p><strong>dse</strong></p></td>
<td><p><strong>warning</strong></p></td>
<td><p><strong>loo_scale</strong></p></td>
</tr>
<tr class="row-even"><td><p><strong>mass_bill</strong></p></td>
<td><p>0</p></td>
<td><p>-11.3</p></td>
<td><p>1.6</p></td>
<td><p>0.0</p></td>
<td><p>1.0</p></td>
<td><p>3.1</p></td>
<td><p>0.0</p></td>
<td><p>True</p></td>
<td><p>log</p></td>
</tr>
<tr class="row-odd"><td><p><strong>bill</strong></p></td>
<td><p>1</p></td>
<td><p>-27.0</p></td>
<td><p>1.7</p></td>
<td><p>15.6</p></td>
<td><p>0.0</p></td>
<td><p>6.2</p></td>
<td><p>4.9</p></td>
<td><p>True</p></td>
<td><p>log</p></td>
</tr>
<tr class="row-even"><td><p><strong>mass</strong></p></td>
<td><p>2</p></td>
<td><p>-135.8</p></td>
<td><p>2.1</p></td>
<td><p>124.5</p></td>
<td><p>0.0</p></td>
<td><p>5.3</p></td>
<td><p>5.8</p></td>
<td><p>True</p></td>
<td><p>log</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="interpreting-log-odds">
<span id="log-odds"></span><h3><span class="section-number">3.4.3. </span>Interpreting Log Odds<a class="headerlink" href="#interpreting-log-odds" title="Permalink to this headline">¶</a></h3>
<p>In a logistic regression the slope is telling you the increase in log
odds units when x is incremented one unit. Odds most simply are the
ratio between the probability of occurrence and probability of no
occurrence. For example, in our penguin example if we were to pick a
random penguin from Adelie or Chinstrap penguins the probability that we
pick an Adelie penguin would be 0.68 as seen in Code Block
<a class="reference internal" href="#adelie-prob"><span class="std std-ref">adelie_prob</span></a></p>
<div class="literal-block-wrapper docutils container" id="adelie-prob">
<div class="code-block-caption"><span class="caption-number">Listing 3.26 </span><span class="caption-text">adelie_prob</span><a class="headerlink" href="#adelie-prob" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Class counts of each penguin species</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s2">&quot;species&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
<span class="n">adelie_count</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="s2">&quot;Adelie&quot;</span><span class="p">],</span>
<span class="n">chinstrap_count</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="s2">&quot;Chinstrap&quot;</span><span class="p">]</span>
<span class="n">adelie_count</span> <span class="o">/</span> <span class="p">(</span><span class="n">adelie_count</span> <span class="o">+</span> <span class="n">chinstrap_count</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">0.68224299</span><span class="p">])</span>
</pre></div>
</div>
<p>And for the same event the odds would be</p>
<div class="literal-block-wrapper docutils container" id="adelie-odds">
<div class="code-block-caption"><span class="caption-number">Listing 3.27 </span><span class="caption-text">adelie_odds</span><a class="headerlink" href="#adelie-odds" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">adelie_count</span> <span class="o">/</span> <span class="n">chinstrap_count</span>
</pre></div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="mf">2.14705882</span><span class="p">])</span>
</pre></div>
</div>
<p>Odds are made up of the same components as probability but are
transformed in a manner that makes interpreting the ratio of one event
occurring from another more straightforward. Stated in odds, if we were
to randomly sample from Adelie and Chinstrap penguins we would expect to
end up with a ratio of 2.14 more Adelie penguins than Chinstrap penguins
as calculated by Code Block <a class="reference internal" href="#adelie-odds"><span class="std std-ref">adelie_odds</span></a>.</p>
<p>Using our knowledge of odds we can define the logit. The logit is the
natural log of the odds which is the fraction shown in Equation
<a class="reference internal" href="#equation-eq-logit">(3.8)</a>. We can rewrite the logistic regression in Equation
<a class="reference internal" href="#equation-eq-logistic">(3.6)</a> in an alternative form of using the logit.</p>
<div class="math notranslate nohighlight" id="equation-eq-logit">
<span class="eqno">(3.8)<a class="headerlink" href="#equation-eq-logit" title="Permalink to this equation">¶</a></span>\[\log \left(\frac{p}{1-p} \right) = \boldsymbol{X} \beta\]</div>
<p>This alternative formulation lets us interpret the coefficients of
logistic regression as the change in log odds. Using this knowledge we
can calculate the probability of observing Adelie to Chinstrap penguins
given a change in the observed bill length as shown in Code Block
<a class="reference internal" href="#logistic-interpretation"><span class="std std-ref">logistic_interpretation</span></a>.
Transformations like these are both interesting mathematically, but also
very practically useful when discussing statistical results, a topic we
will discuss more deeply in <a class="reference internal" href="chp_09.html#section-sharing-results"><span class="std std-ref">Sharing the Results With a Particular Audience</span></a>).</p>
<div class="literal-block-wrapper docutils container" id="logistic-interpretation">
<div class="code-block-caption"><span class="caption-number">Listing 3.28 </span><span class="caption-text">logistic_interpretation</span><a class="headerlink" href="#logistic-interpretation" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="mi">45</span>
<span class="n">β_0</span> <span class="o">=</span> <span class="n">inf_data_logistic_penguins_bill_length</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;β_0&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">values</span>
<span class="n">β_1</span> <span class="o">=</span> <span class="n">inf_data_logistic_penguins_bill_length</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;β_1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">values</span>
<span class="n">bill_length</span> <span class="o">=</span> <span class="mi">45</span>

<span class="n">val_1</span> <span class="o">=</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">β_1</span><span class="o">*</span><span class="n">bill_length</span>
<span class="n">val_2</span> <span class="o">=</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">β_1</span><span class="o">*</span><span class="p">(</span><span class="n">bill_length</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="sa">f</span><span class="s2">&quot;&quot;&quot;(Class Probability change from 45mm Bill Length to 46mm:</span>
<span class="si">{</span><span class="p">(</span><span class="n">special</span><span class="o">.</span><span class="n">expit</span><span class="p">(</span><span class="n">val_2</span><span class="p">)</span> <span class="o">-</span>  <span class="n">special</span><span class="o">.</span><span class="n">expit</span><span class="p">(</span><span class="n">val_1</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">%)&quot;&quot;&quot;</span>
</pre></div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;Class Probability change from 45mm Bill Length to 46mm: 15%&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="picking-priors-in-regression-models">
<span id="id31"></span><h2><span class="section-number">3.5. </span>Picking Priors in Regression Models<a class="headerlink" href="#picking-priors-in-regression-models" title="Permalink to this headline">¶</a></h2>
<p>Now that we are familiar with generalized linear models let us focus on
the prior and its effect on posterior estimation. We will be borrowing
an example from Regression and Other Stories <span id="id32">[<a class="reference internal" href="references.html#id62" title="A. Gelman, J. Hill, and A. Vehtari. Regression and Other Stories. Analytical Methods for Social Research. Cambridge University Press, 2020. ISBN 9781107023987.">40</a>]</span>, in particular a
study<span id="id33">[<a class="reference internal" href="references.html#id63" title="Andrew Gelman, Daniel Simpson, and Michael Betancourt. The prior can often only be understood in the context of the likelihood. Entropy, 19(10):555, 2017.">13</a>]</span> where the relationship between the
attractiveness of parents and the percentage of girl births of those
parents is explored. In this study researchers estimated the
attractiveness of American teenagers on a five-point scale. Eventually
many of these subjects had children, of which the ratio of gender per
each attractiveness category was calculated, the resulting data points
of which are shown in Code Block
<a class="reference internal" href="#uninformative-prior-sex-ratio"><span class="std std-ref">uninformative_prior_sex_ratio</span></a>
and plotted in <a class="reference internal" href="#fig-beautyratio"><span class="std std-numref">Fig. 3.23</span></a>. In the same code block we also
write a model for single variable regression. This time however, focus
specifically on how priors and likelihoods should be assessed together
and not independently.</p>
<div class="figure align-default" id="fig-beautyratio">
<a class="reference internal image-reference" href="../_images/BeautyRatio.png"><img alt="../_images/BeautyRatio.png" src="../_images/BeautyRatio.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.23 </span><span class="caption-text">Data on the attractiveness of parents plotted against the gender ratio
of their children.</span><a class="headerlink" href="#fig-beautyratio" title="Permalink to this image">¶</a></p>
</div>
<div class="literal-block-wrapper docutils container" id="uninformative-prior-sex-ratio">
<div class="code-block-caption"><span class="caption-number">Listing 3.29 </span><span class="caption-text">uninformative_prior_sex_ratio</span><a class="headerlink" href="#uninformative-prior-sex-ratio" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">56</span><span class="p">])</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_uninformative_prior_sex_ratio</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)</span>
    <span class="n">β_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_1&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">β_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_0&quot;</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">β_1</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">ratio</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;ratio&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="n">prior_predictive_uninformative_prior_sex_ratio</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">(</span>
        <span class="n">samples</span><span class="o">=</span><span class="mi">10000</span>
    <span class="p">)</span>
    <span class="n">trace_uninformative_prior_sex_ratio</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">inf_data_uninformative_prior_sex_ratio</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_uninformative_prior_sex_ratio</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior_predictive_uninformative_prior_sex_ratio</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-posterioruninformativelinearregression">
<a class="reference internal image-reference" href="../_images/PosteriorUninformativeLinearRegression.png"><img alt="../_images/PosteriorUninformativeLinearRegression.png" src="../_images/PosteriorUninformativeLinearRegression.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.24 </span><span class="caption-text">With vague or very wide priors the model shows that large differences in
birth ratios are possible for parents rated as attractive. Some of these
possible fits are as large as a 20% change which seems implausible as no
other study has shown an effect this large on the sex ratio of births.</span><a class="headerlink" href="#fig-posterioruninformativelinearregression" title="Permalink to this image">¶</a></p>
</div>
<p>Nominally we will assume births are equally split between males and
females, and that attractiveness has no effect on sex ratio. This
translates to setting the mean of the prior for intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> to
be 50 and the prior mean for the coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> to be 0. We also
set a wide dispersion to express our lack of knowledge about both the
intercept and the effect of attractiveness on sex ratio. This is not a
fully <em>uninformative prior</em>, of which we covered in Section
<a class="reference internal" href="chp_01.html#make-prior-count"><span class="std std-ref">A Few Options to Quantify Your Prior Information</span></a>, however, a very wide prior.
Given these choices we can write our model in Code Block
<a class="reference internal" href="#uninformative-prior-sex-ratio"><span class="std std-ref">uninformative_prior_sex_ratio</span></a>,
run inference, and generate samples to estimate posterior distribution.
From the data and model we estimate that the mean of <span class="math notranslate nohighlight">\(\beta_1\)</span> to be
1.4, meaning the least attractive group when compared to the most
attractive group the birth ratio will differ by 7.4% on average. In
<a class="reference internal" href="#fig-posterioruninformativelinearregression"><span class="std std-numref">Fig. 3.24</span></a> if we include the
uncertainty, the ratio can vary by over 20% per unit of attractiveness
<a class="footnote-reference brackets" href="#id44" id="id34">10</a> from a random sample of 50 possible “lines of fit” prior to
conditioning the parameters to data.</p>
<p>From a mathematical lens this result is valid. But from the lens of our
general knowledge and our understanding of birth sex ratio outside of
this studies, these results are suspect. The “natural” sex ratio at
birth has been measured to be around 105 boys per 100 girls (ranging
from around 103 to 107 boys), which means the sex ratio at birth is
48.5% female, with a standard deviation of 0.5. Moreover, even factors
that are more intrinsically tied to human biology do not affect birth
ratios to this magnitude, weakening the notion that attractiveness,
which is subjective, should have this magnitude of effect. Given this
information a change of 8% between two groups would require
extraordinary observations.</p>
<p>Let us run our model again but this time set more informative priors
shown in Code Block
<a class="reference internal" href="#informative-prior-sex-ratio"><span class="std std-ref">informative_prior_sex_ratio</span></a>
that are consistent with this general knowledge. Plotting our posterior
samples the concentration of coefficients is smaller and the plotted
posterior lines fall into bounds that are more reasonable when considering
possible ratios.</p>
<div class="literal-block-wrapper docutils container" id="informative-prior-sex-ratio">
<div class="code-block-caption"><span class="caption-number">Listing 3.30 </span><span class="caption-text">informative_prior_sex_ratio</span><a class="headerlink" href="#informative-prior-sex-ratio" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_informative_prior_sex_ratio</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)</span>

    <span class="c1"># Note the now more informative priors</span>
    <span class="n">β_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_1&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)</span>
    <span class="n">β_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β_0&quot;</span><span class="p">,</span> <span class="mf">48.5</span><span class="p">,</span> <span class="mf">.5</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β_0</span> <span class="o">+</span> <span class="n">β_1</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;ratio&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

    <span class="n">prior_predictive_informative_prior_sex_ratio</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">(</span>
        <span class="n">samples</span><span class="o">=</span><span class="mi">10000</span>
    <span class="p">)</span>
    <span class="n">trace_informative_prior_sex_ratio</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">inf_data_informative_prior_sex_ratio</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_informative_prior_sex_ratio</span><span class="p">,</span>
        <span class="n">prior</span><span class="o">=</span><span class="n">prior_predictive_informative_prior_sex_ratio</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-posteriorinformativelinearregression">
<a class="reference internal image-reference" href="../_images/PosteriorInformativeLinearRegression.png"><img alt="../_images/PosteriorInformativeLinearRegression.png" src="../_images/PosteriorInformativeLinearRegression.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 3.25 </span><span class="caption-text">With priors informed from other papers and domain expertise the mean
posterior hardly changes across attractiveness ratio indicating that if
there is a belief there is an effect on birth ratio from the parents
attractiveness more data should be collected to showcase the effect.</span><a class="headerlink" href="#fig-posteriorinformativelinearregression" title="Permalink to this image">¶</a></p>
</div>
<p>This time we see that estimated effect of attractiveness on gender is
negligible, there simply was not enough information to affect the
posterior. As we mentioned in Section <a class="reference internal" href="chp_01.html#make-prior-count"><span class="std std-ref">A Few Options to Quantify Your Prior Information</span></a>
choosing a prior is both a burden and a blessing. Regardless of which
you believe it is, it is important to use this statistical tool with an
explainable and principled choice.</p>
</div>
<div class="section" id="exercises">
<span id="exercises3"></span><h2><span class="section-number">3.6. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>E1.</strong> Comparisons are part of everyday life. What is
something you compare on a daily basis and answer the following
question:</p>
<ul class="simple">
<li><p>What is the numerical quantification you use for comparison?</p></li>
<li><p>How do you decide on the logical groupings for observations? For
example in the penguin model we use species or sex</p></li>
<li><p>What point estimate would you use to compare them?</p></li>
</ul>
<p><strong>E2.</strong> Referring to Model
<a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a> complete the following tasks.</p>
<ol class="simple">
<li><p>Compute the values of Monte Carlo Standard Error Mean using
<code class="docutils literal notranslate"><span class="pre">az.summary</span></code>. Given the computed values which of the following
reported values of <span class="math notranslate nohighlight">\(\mu\)</span> would not be well supported as a point
estimate? 3707.235, 3707.2, or 3707.</p></li>
<li><p>Plot the ESS and MCSE per quantiles and describe the results.</p></li>
<li><p>Resample the model using a low number of draws until you get bad
values of <span class="math notranslate nohighlight">\(\hat R\)</span>, and ESS</p></li>
<li><p>Report the HDI 50% numerically and using <code class="docutils literal notranslate"><span class="pre">az.plot_posterior</span></code></p></li>
</ol>
<p><strong>E3.</strong> In your own words explain how regression can be used
to do the following:</p>
<ol class="simple">
<li><p>Covariate estimation</p></li>
<li><p>Prediction</p></li>
<li><p>Counterfactual analysis</p></li>
</ol>
<p>Explain how they are different, the steps to perform each, and
situations where they would be useful. Use the penguin example or come
up with your own.</p>
<p><strong>E4.</strong> In Code Block
<a class="reference internal" href="#flipper-centering"><span class="std std-ref">flipper_centering</span></a> and Code Block
<a class="reference internal" href="#tfp-penguins-centered-predictor"><span class="std std-ref">tfp_penguins_centered_predictor</span></a>
we centered the flipper length covariate. Refit the model, but instead
of centering, subtract the minimum observed flipped length. Compare the
posterior estimates of the slope and intercept parameters of the
centered model. What is different, what is the same. How does the
interpretation of this model change when compared to the centered model?</p>
<p><strong>E5.</strong> Translate the following primitives from PyMC3 to
TFP. Assume the model name is <code class="docutils literal notranslate"><span class="pre">pymc_model</span></code></p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pm.StudentT(&quot;x&quot;,</span> <span class="pre">0,</span> <span class="pre">10,</span> <span class="pre">20)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pm.sample(chains=2)</span></code></p></li>
</ol>
<p>Hint: write the model and inference first in PyMC3, and find the similar
primitives in TFP using the code shown in this chapter.</p>
<p><strong>E6.</strong> PyMC3 and TFP use different argument names for their
distribution parameterizations. For example in PyMC3 the Uniform
Distribution is parameterized as <code class="docutils literal notranslate"><span class="pre">pm.Uniform.dist(lower=,</span> <span class="pre">upper=)</span></code>
whereas in TFP it is <code class="docutils literal notranslate"><span class="pre">tfd.Uniform(low=,</span> <span class="pre">high=)</span></code>. Use the online
documentation to identify the difference in argument names for the
following distributions.</p>
<ol class="simple">
<li><p>Normal</p></li>
<li><p>Poisson</p></li>
<li><p>Beta</p></li>
<li><p>Binomial</p></li>
<li><p>Gumbel</p></li>
</ol>
<p><strong>E7.</strong> A common modeling technique for parameterizing
Bayesian multiple regressions is to assign a wide prior to the
intercept, and assign more informative prior to the slope coefficients.
Try modifying the <code class="docutils literal notranslate"><span class="pre">model_logistic_penguins_bill_length_mass</span></code> model in
Code Block
<a class="reference internal" href="#model-logistic-penguins-bill-length-mass"><span class="std std-ref">model_logistic_penguins_bill_length_mass</span></a>.
Do you get better inference results? Note that there are divergences with
the original parameterization.</p>
<p><strong>E8.</strong> In linear regression models we have two terms. The
mean linear function and the noise term. Write down these two terms in
mathematical notation, referring to the equations in this chapter for
guidance. Explain in your own words what the purpose of these two parts
of regression are. In particular why are they useful when there is
random noise in any part of the data generating or data collection
process.</p>
<p><strong>E9.</strong> Simulate the data using the formula y = 10 + 2x +
<span class="math notranslate nohighlight">\(\mathcal{N}(0, 5)\)</span> with integer covariate x generated np.linspace(-10,
20, 100). Fit a linear model of the form <span class="math notranslate nohighlight">\(b_0 + b_1*X + \sigma\)</span>. Use a
Normal distribution for the likelihood and covariate priors and a Half
Student’s T prior for the noise term as needed. Recover the parameters
verifying your results using both a posterior plot and a forest plot.</p>
<p><strong>E10.</strong> Generate diagnostics for the model in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> to
verify the results shown in the chapter can be trusted. Use a
combination of visual and numerical diagnostics.</p>
<p><strong>E11.</strong> Refit the model in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> on
Gentoo penguins and Chinstrap penguins. How are the posteriors different
from each other? How are they different from the Adelie posterior
estimation? What inferences can you make about the relationship between
flipper length and mass for these other species of penguins? What does
the change in <span class="math notranslate nohighlight">\(\sigma\)</span> tell you about the ability of flipper length to
estimate mass?</p>
<p><strong>M12.</strong> Using the model in Code Block
<a class="reference internal" href="#tfp-flipper-bill-sex-counterfactuals"><span class="std std-ref">tfp_flipper_bill_sex_counterfactuals</span></a>
run a counterfactual analysis for female penguin flipper length with
mean flipper length and a bill length of 20mm. Plot a kernel density
estimate of the posterior predictive samples.</p>
<p><strong>M13.</strong> Duplicate the flipper length covariate in Code
Block <a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a>
by adding a <span class="math notranslate nohighlight">\(\beta_2\)</span> coefficient and rerun the model. What do
diagnostics such as ESS and rhat indicate about this model with a
duplicated coefficient?</p>
<p><strong>M14.</strong> Translate the PyMC3 model in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> into
Tensorflow Probability. List three of the syntax differences.</p>
<p><strong>M15.</strong> Translate the TFP model in Code Block
<a class="reference internal" href="#tfp-penguins-centered-predictor"><span class="std std-ref">tfp_penguins_centered_predictor</span></a>
into PyMC3. List three of the syntax differences.</p>
<p><strong>M16.</strong> Use a logistic regression with increasing number of
covariates to reproduce the prior predictive distributions in <a class="reference internal" href="chp_02.html#fig-prior-predictive-check-01"><span class="std std-numref">Fig. 2.3</span></a>.
Explain why its the case that a logistic regression with many covariates
generate a prior response with extreme values.</p>
<p><strong>H17.</strong> Translate the PyMC3 model in Code Block
<a class="reference internal" href="#model-logistic-penguins-bill-length-mass"><span class="std std-ref">model_logistic_penguins_bill_length_mass</span></a>
into TFP to classify Adelie and Chinstrap penguins. Reuse the same model
to classify Chinstrap and Gentoo penguins. Compare the coefficients, how
do they differ?</p>
<p><strong>H18.</strong> In Code Block
<a class="reference internal" href="#penguin-mass"><span class="std std-ref">penguin_mass</span></a> our model allowed for
negative values mass. Change the model so negative values are no longer
possible. Run a prior predictive check to verify that your change was
effective. Perform MCMC sampling and plot the posterior. Has the
posterior changed from the original model? Given the results why would
you choose one model over the other and why?</p>
<p><strong>H19.</strong> The Palmer Penguin dataset includes additional data
for the observed penguins such as island and bill depth. Include these
covariates into the linear regression model defined in Code Block
<a class="reference internal" href="#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> in two
parts, first adding bill depth, and then adding the island covariates.
Do these covariates help estimate Adelie mass more precisely? Justify
your answer using the parameter estimates and model comparison tools.</p>
<p><strong>H20.</strong> Similar the exercise 2H19, see if adding bill depth
or island covariates to the penguin logistic regression help classify
Adelie and Gentoo penguins more precisely. Justify if the additional
covariates helped using the numerical and visual tools shown in this
chapter.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>You can find more information in the TensorFlow tutorials and
documentations. For example,
<a class="reference external" href="https://www.tensorflow.org/probability/examples/JointDistributionAutoBatched_A_Gentle_Tutorial">https://www.tensorflow.org/probability/examples/JointDistributionAutoBatched_A_Gentle_Tutorial</a>
and
<a class="reference external" href="https://www.tensorflow.org/probability/examples/Modeling_with_JointDistribution">https://www.tensorflow.org/probability/examples/Modeling_with_JointDistribution</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">tfd.Sample</span></code> and <code class="docutils literal notranslate"><span class="pre">tfd.Independent</span></code> are distribution constructors
that takes other distributions as input and return a new
distribution. There are other meta distribution but with different
purposes like <code class="docutils literal notranslate"><span class="pre">tfd.Mixture</span></code>, <code class="docutils literal notranslate"><span class="pre">tfd.TransformedDistribution</span></code>, and
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code>. A more comprehensive introduction to
<code class="docutils literal notranslate"><span class="pre">tfp.distributions</span></code> can be found in
<a class="reference external" href="https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial">https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial</a></p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id7">3</a></span></dt>
<dd><p><a class="reference external" href="https://mc-stan.org/docs/2_23/reference-manual/hmc-algorithm-parameters.html#automatic-parameter-tuning">https://mc-stan.org/docs/2_23/reference-manual/hmc-algorithm-parameters.html#automatic-parameter-tuning</a></p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id18">4</a></span></dt>
<dd><p>If wanted exactly the same model we could specify the priors in
Bambi, not shown here. For our purposes however, the models are
“close enough”.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id19">5</a></span></dt>
<dd><p>You can also parse the design matrix differently so that
covariates represents the contrast between 2 categories within a
column.</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id22">6</a></span></dt>
<dd><p>Maybe because collecting more data is expensive or difficult or
even impossible</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id23">7</a></span></dt>
<dd><p>Unless we are talking about large systems like rain forests, where
the presence of plants actually have an impact in the weather.
Nature can be hard to grasp with simple statements.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id25">8</a></span></dt>
<dd><p>Traditionally people apply functions like <span class="math notranslate nohighlight">\(\phi\)</span> to the left side
of Equation <a class="reference internal" href="#equation-eq-generalized-linear-model">(3.5)</a>, and call them link
functions. We instead prefer to apply them to the right-hand side
and then to avoid confusion we use term inverse link function.</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id26">9</a></span></dt>
<dd><p>Usually in the traditional Generalized Linear Models Literature,
the likelihood of the observation need to be from the Exponential
family, but being Bayesian we are actually not restricted by that
and can use any likelihood that can be parameterized by the expected
value.</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id34">10</a></span></dt>
<dd><p>Estimate shown in corresponding notebook.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_02.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Exploratory Analysis of Bayesian Models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="chp_04.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Extending Linear Models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Martin, Kumar, Lao<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>