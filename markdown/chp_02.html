
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2. Exploratory Analysis of Bayesian Models &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Linear Models and Probabilistic Programming Languages" href="chp_03.html" />
    <link rel="prev" title="1. Bayesian Inference" href="chp_01.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_02.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#there-is-life-after-inference-and-before-too">
   2.1. There is Life After Inference, and Before Too!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#understanding-your-assumptions">
   2.2. Understanding Your Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#understanding-your-predictions">
   2.3. Understanding Your Predictions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagnosing-numerical-inference">
   2.4. Diagnosing Numerical Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#effective-sample-size">
     2.4.1. Effective Sample Size
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#potential-scale-reduction-factor-hat-r">
     2.4.2. Potential Scale Reduction Factor
     <span class="math notranslate nohighlight">
      \(\hat R\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-standard-error">
     2.4.3. Monte Carlo Standard Error
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trace-plots">
     2.4.4. Trace Plots
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autocorrelation-plots">
     2.4.5. Autocorrelation Plots
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rank-plots">
     2.4.6. Rank Plots
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#divergences">
     2.4.7. Divergences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampler-parameters-and-other-diagnostics">
     2.4.8. Sampler Parameters and Other Diagnostics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-comparison">
   2.5. Model Comparison
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-and-loo">
     2.5.1. Cross-validation and LOO
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expected-log-predictive-density">
     2.5.2. Expected Log Predictive Density
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pareto-shape-parameter">
     2.5.3. Pareto Shape Parameter
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-p-loo-when-pareto-hat-kappa-is-large">
     2.5.4. Interpreting p_loo When Pareto
     <span class="math notranslate nohighlight">
      \(\hat \kappa\)
     </span>
     is Large
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loo-pit">
     2.5.5. LOO-PIT
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-averaging">
     2.5.6. Model Averaging
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   2.6. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Exploratory Analysis of Bayesian Models</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#there-is-life-after-inference-and-before-too">
   2.1. There is Life After Inference, and Before Too!
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#understanding-your-assumptions">
   2.2. Understanding Your Assumptions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#understanding-your-predictions">
   2.3. Understanding Your Predictions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#diagnosing-numerical-inference">
   2.4. Diagnosing Numerical Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#effective-sample-size">
     2.4.1. Effective Sample Size
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#potential-scale-reduction-factor-hat-r">
     2.4.2. Potential Scale Reduction Factor
     <span class="math notranslate nohighlight">
      \(\hat R\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#monte-carlo-standard-error">
     2.4.3. Monte Carlo Standard Error
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trace-plots">
     2.4.4. Trace Plots
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autocorrelation-plots">
     2.4.5. Autocorrelation Plots
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rank-plots">
     2.4.6. Rank Plots
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#divergences">
     2.4.7. Divergences
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampler-parameters-and-other-diagnostics">
     2.4.8. Sampler Parameters and Other Diagnostics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-comparison">
   2.5. Model Comparison
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation-and-loo">
     2.5.1. Cross-validation and LOO
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expected-log-predictive-density">
     2.5.2. Expected Log Predictive Density
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pareto-shape-parameter">
     2.5.3. Pareto Shape Parameter
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpreting-p-loo-when-pareto-hat-kappa-is-large">
     2.5.4. Interpreting p_loo When Pareto
     <span class="math notranslate nohighlight">
      \(\hat \kappa\)
     </span>
     is Large
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loo-pit">
     2.5.5. LOO-PIT
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-averaging">
     2.5.6. Model Averaging
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   2.6. Exercises
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="exploratory-analysis-of-bayesian-models">
<span id="chap1bis"></span><h1><span class="section-number">2. </span>Exploratory Analysis of Bayesian Models<a class="headerlink" href="#exploratory-analysis-of-bayesian-models" title="Permalink to this headline">¶</a></h1>
<p>As we saw in Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a>, Bayesian inference is about
conditioning models to the available data and obtaining posterior
distributions. We can do this using pen and paper, computers, or other
devices <a class="footnote-reference brackets" href="#id52" id="id1">1</a>. Additionally we can include, as part of the inference
process, the computation of other quantities like the prior and
posterior predictive distributions. However, Bayesian modeling is wider
than inference. While it would be nice if Bayesian modeling was as
simple as specifying model and calculating a posterior, it is typically
not. The reality is that other equally important tasks are needed for
successful Bayesian data analysis. In this chapter we will discuss some
of these tasks, including checking model assumptions, diagnosing
inference results, and model comparison.</p>
<section id="there-is-life-after-inference-and-before-too">
<span id="id2"></span><h2><span class="section-number">2.1. </span>There is Life After Inference, and Before Too!<a class="headerlink" href="#there-is-life-after-inference-and-before-too" title="Permalink to this headline">¶</a></h2>
<p>A successful Bayesian modeling approach requires performing additional
tasks beyond inference <a class="footnote-reference brackets" href="#id53" id="id3">2</a>. Such as:</p>
<ul class="simple">
<li><p>Diagnosing the quality of the inference results obtained using
numerical methods.</p></li>
<li><p>Model criticism, including evaluations of both model assumptions and
model predictions.</p></li>
<li><p>Comparison of models, including model selection or model averaging.</p></li>
<li><p>Preparation of the results for a particular audience.</p></li>
</ul>
<p>These tasks require both numerical and visual summaries to help
practitioners analyze their models. We collectively call these tasks
<strong>Exploratory Analysis of Bayesian Models</strong>. The name is taken from the
statistical approach known as Exploratory Data Analysis (EDA)
<span id="id4">[<a class="reference internal" href="references.html#id34" title="John W. Tukey. Exploratory Data Analysis. Addison-Wesley, 1977.">14</a>]</span>. This approach to data analysis aims at summarizing the main
characteristics of a data set, often with visual methods. In the words
of Persi Diaconis <span id="id5">[<a class="reference internal" href="references.html#id27" title="Persi Diaconis. Theories of Data Analysis: From Magical Thinking Through Classical Statistics, chapter 1, pages 1-36. John Wiley &amp; Sons, Ltd, 2006.">15</a>]</span>:</p>
<blockquote>
<div><p>Exploratory data analysis (EDA) seeks to reveal structure, or simple
descriptions, in data. We look at numbers or graphs and try to find
patterns. We pursue leads suggested by background information,
imagination, patterns perceived, and experience with other data
analyses.</p>
</div></blockquote>
<p>EDA is generally performed before, or even instead of, an inferential
step. We, as many others before us
<span id="id6">[<a class="reference internal" href="references.html#id20" title="Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization in bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2):389–402, 2019.">16</a>, <a class="reference internal" href="references.html#id78" title="Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. Bayesian workflow. arXiv preprint arXiv:2011.01808, 2020.">17</a>]</span>, think that many of the ideas
from EDA can be used, reinterpreted and expanded into a robust Bayesian
modeling approach. In this book we will mainly use the Python library
ArviZ <a class="footnote-reference brackets" href="#id54" id="id7">3</a> <span id="id8">[<a class="reference internal" href="references.html#id11" title="Ravin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo Martin. Arviz a unified library for exploratory analysis of bayesian models in python. Journal of Open Source Software, 4(33):1143, 2019.">3</a>]</span> to help us perform exploratory analysis of
Bayesian models.</p>
<p>In a real life setting, Bayesian inference and exploratory analysis of
Bayesian models get tangled into an iterative workflow, that includes
silly coding mistakes, computational problems, doubts about the adequacy
of models, doubts about our current understanding of the data,
non-linear model building, model checking, etc. Trying to emulate this
intricate workflow in a book is challenging. Thus throughout these pages
we may omit some or even all the exploratory analysis steps or maybe
leave them as exercises. This is not because they are not necessary or
not important. On the contrary during writing this book we have
performed many iterations “behind the scenes”. However, we omit them in
certain areas so focus on other relevant aspects such as model details,
computational features, or fundamental mathematics.</p>
</section>
<section id="understanding-your-assumptions">
<span id="prior-predictive-checks"></span><h2><span class="section-number">2.2. </span>Understanding Your Assumptions<a class="headerlink" href="#understanding-your-assumptions" title="Permalink to this headline">¶</a></h2>
<p>As we discussed in Section <a class="reference internal" href="chp_01.html#make-prior-count"><span class="std std-ref">A Few Options to Quantify Your Prior Information</span></a>, “what is
the best-ever prior?” is a tempting question to ask. However, it is
difficult to give a straight satisfying answer other than: “it
depends”. We can certainly find default priors for a given model or
family of models that will yield good results for a wide range of
datasets. But we can also find ways to outperform them for particular
problems if we can generate more informative priors for those specific
problems. Good default priors can serve as good priors for quick/default
analysis and also as good placeholder for better priors if we can invest
the time and effort to move into an iterative, exploratory Bayesian
modelling workflow.</p>
<p>One problem when choosing priors is that it may be difficult to
understand their effect as they propagate down the model into the data.
The choices we made in the parameter space may induce something
unexpected in the observable data space. A very helpful tool to better
understand our assumptions is the prior predictive distribution, which
we presented in Section <a class="reference internal" href="chp_01.html#id6"><span class="std std-ref">Bayesian Inference</span></a> and
Equation <a class="reference internal" href="chp_01.html#equation-eq-prior-pred-dist">(1.7)</a>. In practice we
can compute a prior predictive distribution by sampling from the model,
but without conditioning on the observed data. By sampling from the
prior predictive distribution, the computer does the work of translating
choices we made in the parameter space into the observed data space.
Using these samples to evaluate priors is known as <strong>prior predictive
checks</strong>.</p>
<p>Let us assume we want to build a model of football (or soccer for the
people in USA). Specifically we are interested in the probability of
scoring goals from the penalty point. After thinking for a while we
decide to use a geometric model <a class="footnote-reference brackets" href="#id55" id="id9">4</a>. Following the sketch in
<a class="reference internal" href="#fig-football-sketch"><span class="std std-numref">Fig. 2.1</span></a> and a little bit of trigonometry we come
up with the following formula for the probability of scoring a goal:</p>
<div class="math notranslate nohighlight" id="equation-eq-geometric-football">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-eq-geometric-football" title="Permalink to this equation">¶</a></span>\[p\left(|\alpha| &lt; \tan^{-1}\left(\frac{L}{x}\right)\right) = 2\Phi\left(\frac{\tan^{-1}\left(\frac{L}{x}\right)}{\sigma}\right) - 1\]</div>
<p>The intuition behind Equation <a class="reference internal" href="#equation-eq-geometric-football">(2.1)</a> is that we are
assuming the probability of scoring a goal is given by the absolute
value of the angle <span class="math notranslate nohighlight">\(\alpha\)</span> being less than a threshold
<span class="math notranslate nohighlight">\(\tan^{-1}\left(\frac{L}{x}\right)\)</span>. Furthermore we are assuming that
the player is trying to kick the ball straight, i.e at a zero angle, but
then there are other factors that result in a trajectory with a
deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<figure class="align-default" id="fig-football-sketch">
<a class="reference internal image-reference" href="../_images/football_sketch.png"><img alt="../_images/football_sketch.png" src="../_images/football_sketch.png" style="width: 4in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.1 </span><span class="caption-text">Sketch of a penalty shot. The dashed lines represent the angle <span class="math notranslate nohighlight">\(\alpha\)</span>
within which the ball must be kicked to score a goal. <span class="math notranslate nohighlight">\(x\)</span> represents the
reglementary distance for a penalty shot (11 meters) and <span class="math notranslate nohighlight">\(L\)</span> half the
length of a reglementary goal mouth (3.66 meters).</span><a class="headerlink" href="#fig-football-sketch" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The only unknown quantity in Equation <a class="reference internal" href="#equation-eq-geometric-football">(2.1)</a> is
<span class="math notranslate nohighlight">\(\sigma\)</span>. We can get the values of <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(x\)</span> from the rules of
football. As good Bayesians, when we do not know a quantity, we assign a
prior to it and then try to build a Bayesian model, for example, we
could write:</p>
<div class="math notranslate nohighlight" id="equation-eq-geometric-model">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-eq-geometric-model" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}\begin{split}\begin{split}
\sigma &amp;\sim \mathcal{HN}(\sigma_{\sigma}) \\
\text{p_goal} &amp;= 2\Phi\left(\frac{\tan^{-1}\left(\frac{L}{x}\right)}{\sigma}\right) - 1 \\
Y &amp;\sim \text{Bin}(n=1, p=\text{p_goal})\end{split}\\\end{split}\end{aligned}\end{align} \]</div>
<p>At this point we are not entirely certain how well our model encodes our
domain knowledge about football, so we decide to first sample from the
prior predictive to gain some intuition.
<a class="reference internal" href="#fig-prior-predictive-check-00"><span class="std std-numref">Fig. 2.2</span></a> shows the results for three
priors (encoded as three values of <span class="math notranslate nohighlight">\(\sigma_{\sigma}\)</span>, 5, 20, and 60
degrees). The gray circular area represents the set of angles that
should lead to scoring a goal assuming that the player kicks the ball
completely straight and no other factors come in play like wind,
friction, etc. We can see that our model assumes that it is possible to
score goals even if the player kicks the ball with a greater angle than
the gray area. Interestingly enough for large values of
<span class="math notranslate nohighlight">\(\sigma_{\sigma}\)</span> the model thinks that kicking in the opposite
direction is not that bad idea.</p>
<figure class="align-default" id="fig-prior-predictive-check-00">
<a class="reference internal image-reference" href="../_images/prior_predictive_distributions_00.png"><img alt="../_images/prior_predictive_distributions_00.png" src="../_images/prior_predictive_distributions_00.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.2 </span><span class="caption-text">Prior predictive checks for the model in Equation
<a class="reference internal" href="#equation-eq-geometric-model">(2.2)</a>. Each subplot corresponds to a different value
of the prior for <span class="math notranslate nohighlight">\(\sigma\)</span>. The black dot at the center of each circular
plot represents the penalty point. The dots at the edges represent
shots, the position is the value of the angle <span class="math notranslate nohighlight">\(\alpha\)</span> (see
<a class="reference internal" href="#fig-football-sketch"><span class="std std-numref">Fig. 2.1</span></a>), and the color represents the probability
of scoring a goal.</span><a class="headerlink" href="#fig-prior-predictive-check-00" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>At this point we have a few alternatives: we can rethink our model to
incorporate further geometric insights. Alternatively we can use a prior
that reduces the chance of nonsensical results even if we do not exclude
them completely, or we can just fit the data and see if the data is
informative enough to estimate a posterior that excludes nonsensical
values. <a class="reference internal" href="#fig-prior-predictive-check-01"><span class="std std-numref">Fig. 2.3</span></a> shows another example of
what we may consider unexpected <a class="footnote-reference brackets" href="#id56" id="id10">5</a>. The example shows how for a
logistic regression <a class="footnote-reference brackets" href="#id58" id="id11">6</a> with binary predictors and priors
<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> on the regression coefficients. As we increase the
number of predictors, the mean of the prior predictive distributions
shifts from being more concentrated around 0.5 (first panel) to Uniform
(middle) to favor extreme, 0 or 1, values (last panel). This example
shows us that as the number of predictors increase, the induced prior
predictive distribution puts more mass on extreme values. Thus we need a
<em>stronger regularizing prior</em> (e.g., Laplace distribution) in order to
keep the model away from those extreme values.</p>
<figure class="align-default" id="fig-prior-predictive-check-01">
<a class="reference internal image-reference" href="../_images/prior_predictive_distributions_01.png"><img alt="../_images/prior_predictive_distributions_01.png" src="../_images/prior_predictive_distributions_01.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.3 </span><span class="caption-text">Prior predictive distribution for a logistic regression models with 2,
5, or 15 binary predictors and with 100 data points. The KDE represents
the distributions of the mean of the simulated data over 10000
simulations. Even when the prior for each coefficient,
<span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>, is the same for all 3 panels, increasing the
numbers of the predictor is effectively equivalent to using a prior
favoring extreme values.</span><a class="headerlink" href="#fig-prior-predictive-check-01" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Both previous examples show that priors can not be understood in
isolation, we need to put them in the context of a particular model. As
thinking in terms of observed values is generally easier than thinking
in terms of the model’s parameters, prior predictive distributions can
help make model evaluation easier. This becomes even more useful for
complex models where parameters get transformed through many
mathematical operations, or multiple priors interact with each other.
Additionally, prior predictive distributions can be used to present
results or discuss models in a more intuitive way to a wide audience. A
domain expert may not be familiar with statistical notation or code and
thus using those devices may not lead to a productive discussion, but if
you show them the implications of one or more models, you provide them
more material to discuss. This can provide valuable insight both for
your domain partner and yourself. And again computing the prior
predictive has other advantages, such as helping us debug models,
ensuring they are properly written and able to run in our computational
environment.</p>
</section>
<section id="understanding-your-predictions">
<span id="posterior-pd"></span><h2><span class="section-number">2.3. </span>Understanding Your Predictions<a class="headerlink" href="#understanding-your-predictions" title="Permalink to this headline">¶</a></h2>
<p>As we can use synthetic data, that is generated data, from the prior
predictive distribution to help us inspect our model, we can perform a
similar analysis with the posterior predictive distribution, introduced
in Section <a class="reference internal" href="chp_01.html#id6"><span class="std std-ref">Bayesian Inference</span></a> and Equation
<a class="reference internal" href="chp_01.html#equation-eq-post-pred-dist">(1.8)</a>. This procedure is generally
referred as <strong>posterior predictive checks</strong>. The basic idea is to
evaluate how close the synthetic observations are to the actual
observations. Ideally the way we assess closeness should be
problem-dependent, but we can also use some general rules. We may even
want to use more than one metric in order to assess different ways our
models (mis)match the data.</p>
<p><a class="reference internal" href="#fig-posterior-predictive-check"><span class="std std-numref">Fig. 2.4</span></a> shows a very simple example for
a binomial model and data. On the left panel we are comparing the number
of observed successes in our data (blue line) with the number of
predicted successes over 1000 samples from the posterior predictive
distribution. On the right panel is an alternative way of representing
the results, this time showing the proportion of success and failures in
our data (blue line) against 1000 samples from the posterior
distribution. As we can see the model is doing a very good job at
capturing the mean value in this case, even when the model recognizes
there is a lot of uncertainty. We should not be surprised that the model
is doing a good job at capturing the mean. The reason is that we are
directly modeling the mean of the binomial distribution. In the next
chapters we will see examples when posterior predictive checks provide
less obvious and thus more valuable information about our model’s fit to
the data.</p>
<figure class="align-default" id="fig-posterior-predictive-check">
<a class="reference internal image-reference" href="../_images/posterior_predictive_check.png"><img alt="../_images/posterior_predictive_check.png" src="../_images/posterior_predictive_check.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.4 </span><span class="caption-text">Posterior predictive check for a Beta-Binomial model. On the left panel
we have the number of predicted success (gray histogram), the dashed
black line represents the mean predicted success. The blue line is the
mean computed from the data. On the right panel we have the same
information but represented in an alternative way. Instead of the number
of success we are plotting the probability of getting 0’s or 1’s. We are
using a line to represent that the probability of <span class="math notranslate nohighlight">\(p(y=0) = 1-p(y=1)\)</span>.
the dashed black line is the mean predicted probability, and the blue
line is the mean computed from the data.</span><a class="headerlink" href="#fig-posterior-predictive-check" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Posterior predictive checks are not restricted to plots. We can also
perform numerical tests <span id="id12">[<a class="reference internal" href="references.html#id3" title="A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. Bayesian Data Analysis, Third Edition. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2013. ISBN 9781439840955.">18</a>]</span>. One way of
doing this is by computing:</p>
<div class="math notranslate nohighlight" id="equation-eq-post-pred-test-quantity">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-eq-post-pred-test-quantity" title="Permalink to this equation">¶</a></span>\[p_{B} = p(T_{sim} \leq T_{obs} \mid \tilde Y)\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{B}\)</span> is a Bayesian p-value and is defined as the probability
the simulated test statistic <span class="math notranslate nohighlight">\(T_{sim}\)</span> is less or equal than the
observed statistic <span class="math notranslate nohighlight">\(T_{obs}\)</span>. The statistic <span class="math notranslate nohighlight">\(T\)</span> is basically any metric
we may want to use to assess our models fit to the data. Following the
binomial example we can choose <span class="math notranslate nohighlight">\(T_{obs}\)</span> as the observed success rate
and then compare it against the posterior predictive distribution
<span class="math notranslate nohighlight">\(T_{sim}\)</span>. The ideal value of <span class="math notranslate nohighlight">\(p_{B}=0.5\)</span> means that we compute a
<span class="math notranslate nohighlight">\(T_{sim}\)</span> statistic that half the time is below and half the time above
the observed statistics <span class="math notranslate nohighlight">\(T_{obs}\)</span>, which is the expected outcome for a
good fit.</p>
<p>Because we love plots, we can also create plots with Bayesian p-values.
The first panel of <a class="reference internal" href="#fig-posterior-predictive-check-pu-values"><span class="std std-numref">Fig. 2.5</span></a>
shows the distribution of Bayesian p-values in black solid line, the
dashed line represents the expected distribution for a dataset of the
same size. We can obtain such a plot with ArviZ
<code class="docutils literal notranslate"><span class="pre">az.plot_bpv(.,</span> <span class="pre">kind=&quot;p_value&quot;)</span></code>. The second panel is conceptually
similar, the twist is that we evaluate how many of the simulations are
below (or above) the observed data “per observation”. For a
well-calibrated model, all observations should be equally well
predicted, that is the expected number of predictions above or below
should be the same. Thus we should get a Uniform distribution. As for
any finite dataset, even a perfectly calibrated model will show
deviations from a Uniform distribution, we plot a band where we expected
to see 94% of the Uniform-like curves.</p>
<div class="admonition-bayesian-p-values admonition">
<p class="admonition-title">Bayesian p-values</p>
<p>We call <span class="math notranslate nohighlight">\(p_{B}\)</span> Bayesian p-values as the quantity in
Equation <a class="reference internal" href="#equation-eq-post-pred-test-quantity">(2.3)</a> is essentially the definition
of a p-value, and we say they are Bayesian because, instead of using the
distribution of the statistic <span class="math notranslate nohighlight">\(T\)</span> under the null hypothesis as the
sampling distribution we are using the posterior predictive
distribution. Notice that we are not conditioning on any null
hypothesis. Neither are we using any predefined threshold to declare
statistical significance or to perform hypothesis testing.</p>
</div>
<figure class="align-default" id="fig-posterior-predictive-check-pu-values">
<a class="reference internal image-reference" href="../_images/posterior_predictive_check_pu_values.png"><img alt="../_images/posterior_predictive_check_pu_values.png" src="../_images/posterior_predictive_check_pu_values.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.5 </span><span class="caption-text">Posterior predictive distribution for a Beta-Binomial model. In the
first panel, the curve with the solid line is the KDE of the proportion
of predicted values that are less or equal than the observed data. The
dashed lines represent the expected distribution for a dataset of the
same size as the observed data. On the second panel, the black line is a
KDE for the proportion of predicted values that are less or equal than
the observed computed per observation instead of over each simulation as
in the first panel. The white line represents the ideal case, a standard
Uniform distribution, and the gray band deviations of that Uniform
distribution that we expect to see for a dataset of the same size.</span><a class="headerlink" href="#fig-posterior-predictive-check-pu-values" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As we said before we can choose from many <span class="math notranslate nohighlight">\(T\)</span> statistics to summarize
observations and predictions.
<a class="reference internal" href="#fig-posterior-predictive-check-tstat"><span class="std std-numref">Fig. 2.6</span></a> shows two examples, in
the first panel <span class="math notranslate nohighlight">\(T\)</span> is the mean and in the second one <span class="math notranslate nohighlight">\(T\)</span> is the
standard deviation. The curves are KDEs representing the distribution of
the <span class="math notranslate nohighlight">\(T\)</span> statistics from the posterior predictive distribution and the
dot is the value for the observed data.</p>
<figure class="align-default" id="fig-posterior-predictive-check-tstat">
<a class="reference internal image-reference" href="../_images/posterior_predictive_check_tstat.png"><img alt="../_images/posterior_predictive_check_tstat.png" src="../_images/posterior_predictive_check_tstat.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.6 </span><span class="caption-text">Posterior predictive distribution for a Beta-Binomial model. In the
first panel, the curve with the solid line is the KDE of the proportion
of simulations of predicted values with mean values less or equal than
the observed data. On the second panel, the same but for the standard
deviation. The black dot represented the mean (first panel) or standard
deviation (second panel) computed from the observed data.</span><a class="headerlink" href="#fig-posterior-predictive-check-tstat" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Before continue reading you should take a moment to carefully inspect
<a class="reference internal" href="#fig-posterior-predictive-many-examples"><span class="std std-numref">Fig. 2.7</span></a> and try to understand
why the plots look like they do. In this figure we have a series of
simple examples to help us gain intuition about how to interpret
posterior predictive checks plots<a class="footnote-reference brackets" href="#id59" id="id13">7</a>. In all these examples the
observed data (in blue) follows a Gaussian distribution.</p>
<ol class="simple">
<li><p>On the first row, the model predicts observations that are
systematically shifted to higher values with respect to the observed
data.</p></li>
<li><p>On the second row, the model is making predictions that are more
spread than the observed data.</p></li>
<li><p>On the third row we have the opposite scenario, the model is not
generating enough predictions at the tails.</p></li>
<li><p>On the last row shows a model making predictions following a mixture
of Gaussians.</p></li>
</ol>
<p>We are now going to pay special attention to the third column from
<a class="reference internal" href="#fig-posterior-predictive-many-examples"><span class="std std-numref">Fig. 2.7</span></a>. Plots in this column
are very useful but at the same time they can be confusing at first.
From top to bottom, you can read them as:</p>
<ol class="simple">
<li><p>The model is missing observations on the left tail (and making more
on the right).</p></li>
<li><p>The model is making less predictions at the middle (and more at the
tails).</p></li>
<li><p>The model is making less predictions for both tails.</p></li>
<li><p>The model is making more or less well-calibrated predictions, but I
am a skeptical person so I should run another posterior predictive
check to confirm.</p></li>
</ol>
<p>If this way of reading the plots still sounds confusing to you, we could
try from a different perspective that is totally equivalent, but it may
be more intuitive, as long as you remember that you can change the
model, not the observations <a class="footnote-reference brackets" href="#id60" id="id14">8</a>. From top to bottom, you can read them
as:</p>
<ol class="simple">
<li><p>There are more observations on the left</p></li>
<li><p>There are more observations on the middle</p></li>
<li><p>There are more observations at the tails.</p></li>
<li><p>Observations seem well distributed (at least within the expected
boundaries), but you should not trust me. I am just a platonic model
in a platonic world.</p></li>
</ol>
<p>We hope <a class="reference internal" href="#fig-posterior-predictive-many-examples"><span class="std std-numref">Fig. 2.7</span></a> and the
accompanying discussion provide you with enough intuition to better
perform model checking in real scenarios.</p>
<figure class="align-default" id="fig-posterior-predictive-many-examples">
<a class="reference internal image-reference" href="../_images/posterior_predictive_many_examples.png"><img alt="../_images/posterior_predictive_many_examples.png" src="../_images/posterior_predictive_many_examples.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.7 </span><span class="caption-text">Posterior predictive checks for a set of simple hypothetical models. On
the first column, the blue solid line represents the observed data and
the light-gray ones predictions from an hypothetical model. On the
second column, the solid line is the KDE of the proportion of predicted
values that are less or equal than the observed data. The dashed lines
represent the expected distribution for a dataset of the same size as
the observed data. On the third panel, the KDE is for the proportion of
predicted values that are less or equal than the observed computed per
each observation. The white lines represent the expected Uniform
distribution and the gray band the expected deviations from uniformity
for a dataset of the same size as the one used. This figure was made
using the ArviZ’s functions <code class="docutils literal notranslate"><span class="pre">az.plot_ppc(.)</span></code>,
<code class="docutils literal notranslate"><span class="pre">az.plot_bpv(.,</span> <span class="pre">kind=&quot;p_values&quot;)</span></code> and <code class="docutils literal notranslate"><span class="pre">az.plot_bpv(.,</span> <span class="pre">kind=&quot;u_values&quot;)</span></code>.</span><a class="headerlink" href="#fig-posterior-predictive-many-examples" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Posterior predictive checks, either using plots or numerical summaries,
or even a combination of both, is a very flexible idea. This concept is
general enough to let the practitioner use their imagination to come up
with different ways to explore, evaluate, and better understand model
through their predictions and how well a model (or models) work for a
particular problem.</p>
</section>
<section id="diagnosing-numerical-inference">
<span id="diagnosing-inference"></span><h2><span class="section-number">2.4. </span>Diagnosing Numerical Inference<a class="headerlink" href="#diagnosing-numerical-inference" title="Permalink to this headline">¶</a></h2>
<p>Using numerical methods to approximate the posterior distribution allows
us to solve Bayesian models that could be tedious to solve with pen and
paper or that could be mathematically intractable. Unfortunately, they
do not always work as expected. For that reason we must always evaluate
if the results they offer are of any use. We can do that using a
collection of numerical and visual diagnostic tools. In this section we
will discuss the most common and useful diagnostics tools for Markov
chain Monte Carlo methods.</p>
<p>To help us understand these diagnostic tools, we are going to create
three <em>synthetic posteriors</em>. The first one is a sample from a
<span class="math notranslate nohighlight">\(\text{Beta}(2, 5)\)</span>. We generate it using SciPy, and we call it
<code class="docutils literal notranslate"><span class="pre">good_chains</span></code>. This is an example of a “good” sample because we are
generating independent and identically distributed (iid) draws and
ideally this is what we want in order to approximate the posterior. The
second one is called <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> and represents a poor sample from the
posterior. We generate it by sorting <code class="docutils literal notranslate"><span class="pre">good_chains</span></code> and then adding a
small Gaussian error. <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> is a poor sample for two reasons:</p>
<ul class="simple">
<li><p>The values are not independent. On the contrary they are highly
autocorrelated, meaning that given any number at any position in the
sequence we can compute the values coming before and after with high
precision.</p></li>
<li><p>The values are not identically distributed, as we are reshaping a
previously flattened and sorted array into a 2D array, representing
two chains.</p></li>
</ul>
<p>The third <em>synthetic posterior</em> called <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> is generated from
<code class="docutils literal notranslate"><span class="pre">good_chains</span></code>, and we are turning it into a representation of a poor
sample from the posterior, by randomly introducing portions where
consecutive samples are highly correlated to each other. <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code>
represents a very common scenario, a sampler can resolve a region of the
parameter space very well, but one or more regions are difficult to
sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">good_chains</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2000</span><span class="p">))</span>
<span class="n">bad_chains0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">good_chains</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="mf">0.05</span><span class="p">,</span>
                               <span class="n">size</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">bad_chains1</span> <span class="o">=</span> <span class="n">good_chains</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1900</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">bad_chains1</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="mi">2</span><span class="p">:,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">950</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">chains</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;good_chains&quot;</span><span class="p">:</span><span class="n">good_chains</span><span class="p">,</span>
          <span class="s2">&quot;bad_chains0&quot;</span><span class="p">:</span><span class="n">bad_chains0</span><span class="p">,</span>
          <span class="s2">&quot;bad_chains1&quot;</span><span class="p">:</span><span class="n">bad_chains1</span><span class="p">}</span>
</pre></div>
</div>
<p>Notice that the 3 synthetic posteriors are samples from a scalar (single
parameter) posterior distribution. This is enough for our current
discussion as all the diagnostics we will see are computed per parameter
in the model.</p>
<section id="effective-sample-size">
<span id="ess"></span><h3><span class="section-number">2.4.1. </span>Effective Sample Size<a class="headerlink" href="#effective-sample-size" title="Permalink to this headline">¶</a></h3>
<p>When using MCMC sampling methods, it is reasonable to wonder if a
particular sample is large enough to confidently compute the quantities
of interest, like a mean or an HDI. This is something we can not
directly answer just by looking at the number of samples, the reason is
that samples from MCMC methods will have some degree of
<strong>autocorrelation</strong>, thus the actual <em>amount of information</em> contained
in that sample will be less than the one we would get from an iid sample
of the same size. We say a series of values are autocorrelated when we
can observe a similarity between them as a function of the time lag
between them. For example, if today the sunset was at 6:03 am, you know
tomorrow the sunset will be about the same time. In fact the closer you
are to the equator the longer you will be able to predict the time for
future sunsets given the value of today. That is, the autocorrelation is
larger at the equator than closer to the poles <a class="footnote-reference brackets" href="#id61" id="id15">9</a>.</p>
<p>We can think of the effective sample size (ESS) as an estimator that
takes autocorrelation into account and provides the number of draws we
would have if our sample was actually iid. This interpretation is
appealing, but we have to be careful about not over-interpreting it as
we will see next.</p>
<p>Using ArviZ we can compute the effective sample size for the mean with
<code class="docutils literal notranslate"><span class="pre">az.ess()</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">ess</span><span class="p">(</span><span class="n">chains</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;xarray.Dataset&gt;
Dimensions:      ()
Data variables:
    good_chains  float64 4.389e+03
    bad_chains0  float64 2.436
    bad_chains1  float64 111.1
</pre></div>
</div>
<p>We can see that even when the count of actual samples in our synthetic
posteriors is 4000, <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> has efficiency equivalent to an iid
sample of size <span class="math notranslate nohighlight">\(\approx 2\)</span>. This is certainly a low number indicating a
problem with the sampler. Given the method used by ArviZ to compute the
ESS and how we created <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code>, this result is totally expected.
<code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> is a bimodal distribution with each chain stuck in each
mode. For such cases the ESS will be approximately equal to the number
of modes the MCMC chains explored. For <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> we also get a low
number <span class="math notranslate nohighlight">\(\approx 111\)</span> and only ESS for <code class="docutils literal notranslate"><span class="pre">good_chains</span></code> is close to the
actual number of samples.</p>
<div class="admonition-on-the-effectiveness-of-effective-samples admonition">
<p class="admonition-title">On the effectiveness of effective samples</p>
<p>If you rerun the generation of these synthetic posteriors, using a different random seed,
you will see that the effective sample size you get will be different each time. This
is expected as the samples will not be exactly the same, they are after
all samples. For <code class="docutils literal notranslate"><span class="pre">good_chains</span></code>, on average, the value of effective
sample size will be lower than the number of samples. But notice that
ESS could be in fact larger! When using the NUTS sampler (see Section
<a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">Inference Methods</span></a>) values of ESS larger than
the total number of samples can happen for parameters which posterior
distributions are close to Gaussian and which are almost independent of
other parameters in the model.</p>
</div>
<p>Convergence of Markov chains is not uniform across the parameter space
<span id="id16">[<a class="reference internal" href="references.html#id13" title="Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. Bayesian Analysis, pages 1 – 38, 2021. URL: https://doi.org/10.1214/20-BA1221, doi:10.1214/20-BA1221.">19</a>]</span>, intuitively it is easier to get a good
approximation from the bulk of a distribution than from the tails,
simply because the tails are dominated by rare events. The default value
returned by <code class="docutils literal notranslate"><span class="pre">az.ess()</span></code> is <code class="docutils literal notranslate"><span class="pre">bulk-ESS</span></code> which mainly assesses how well the
<em>center</em> of the distribution was resolved. If you also want to report
posterior intervals or you are interested in rare events, you should
check the value of <code class="docutils literal notranslate"><span class="pre">tail-ESS</span></code>, which corresponds to the minimum ESS at
the percentiles 5 and 95. If you are interested in specific quantiles,
you can ask ArviZ for those specific values using
<code class="docutils literal notranslate"><span class="pre">az.ess(.,</span> <span class="pre">method='quantile')</span></code>.</p>
<p>As the ESS values vary across the parameter space, we may find it useful
to visualize this variation in a single plot. We have at least two ways
to do it. Plotting the ESS for specifics quantiles
<code class="docutils literal notranslate"><span class="pre">az.plot_ess(.,</span> <span class="pre">kind=&quot;quantiles&quot;)</span></code> or for small intervals defined between
two quantiles <code class="docutils literal notranslate"><span class="pre">az.plot_ess(.,</span> <span class="pre">kind=&quot;local&quot;)</span></code> as shown in
<a class="reference internal" href="#fig-plot-ess"><span class="std std-numref">Fig. 2.8</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_ess</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;local&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_ess</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;quantile&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
</pre></div>
</div>
<figure class="align-default" id="fig-plot-ess">
<a class="reference internal image-reference" href="../_images/plot_ess.png"><img alt="../_images/plot_ess.png" src="../_images/plot_ess.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.8 </span><span class="caption-text">Top: Local ESS of small interval probability estimates. Bottom: Quantile
ESS estimates. The dashed lines represent the minimum suggested value of
400 at which we would consider the effective sample size to be
sufficient. Ideally, we want the local and quantile ESS to be high
across all regions of the parameter space.</span><a class="headerlink" href="#fig-plot-ess" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As a general rule of thumb we recommend a value of ESS greater than 400,
otherwise, the estimation of the ESS itself and the estimation of other
quantities, like <span class="math notranslate nohighlight">\(\hat R\)</span> that we will see next, will be basically
unreliable <span id="id17">[<a class="reference internal" href="references.html#id13" title="Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. Bayesian Analysis, pages 1 – 38, 2021. URL: https://doi.org/10.1214/20-BA1221, doi:10.1214/20-BA1221.">19</a>]</span>. Finally, we said that the ESS provides
the number of draws we would have if our sample was actually iid.
Nevertheless, we have to be careful with this interpretation as the
actual value of ESS will not be the same for different regions of the
parameter space. Taking that detail into account, the intuition still
seems useful.</p>
</section>
<section id="potential-scale-reduction-factor-hat-r">
<span id="id18"></span><h3><span class="section-number">2.4.2. </span>Potential Scale Reduction Factor <span class="math notranslate nohighlight">\(\hat R\)</span><a class="headerlink" href="#potential-scale-reduction-factor-hat-r" title="Permalink to this headline">¶</a></h3>
<p>Under very general conditions Markov chain Monte Carlo methods have
theoretical guarantees that they will get the right answer irrespective
of the starting point. Unfortunately, the fine print says that the
guarantees are valid only for infinite samples. Thus in practice we need
ways to estimate convergence for finite samples. One pervasive idea is
to run more than one chain, starting from very different points and then
check the resulting chains to see if they <em>look similar</em> to each other.
This intuitive notion can be formalized into a numerical diagnostic
known as <span class="math notranslate nohighlight">\(\hat R\)</span>. There are many versions of this estimator, as it has
been refined over the years <span id="id19">[<a class="reference internal" href="references.html#id13" title="Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. Bayesian Analysis, pages 1 – 38, 2021. URL: https://doi.org/10.1214/20-BA1221, doi:10.1214/20-BA1221.">19</a>]</span>. Originally the
<span class="math notranslate nohighlight">\(\hat R\)</span> diagnostic was interpreted as the overestimation of variance
due to MCMC finite sampling. Meaning that if you continue sampling
infinitely you should get a reduction of the variance of your estimation
by a <span class="math notranslate nohighlight">\(\hat R\)</span> factor. And hence the name “potential scale reduction
factor”, with the target value of 1 meaning that increasing the number
of samples will not reduce the variance of the estimation further.
Nevertheless, in practice it is better to just think of it as a
diagnostic tool without trying to over-interpret it.</p>
<p>The <span class="math notranslate nohighlight">\(\hat R\)</span> for the parameter <span class="math notranslate nohighlight">\(\theta\)</span> is computed as the standard
deviation of all the samples of <span class="math notranslate nohighlight">\(\theta\)</span>, that is including all chains
together, divided by the root mean square of the separated within-chain
standard deviations. The actual computation is a little bit more
involved but the overall idea remains true <span id="id20">[<a class="reference internal" href="references.html#id13" title="Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. Bayesian Analysis, pages 1 – 38, 2021. URL: https://doi.org/10.1214/20-BA1221, doi:10.1214/20-BA1221.">19</a>]</span>. Ideally
we should get a value of 1, as the variance between chains should be the
same as the variance within-chain. From a practical point of view values
of <span class="math notranslate nohighlight">\(\hat R \lessapprox 1.01\)</span> are considered safe.</p>
<p>Using ArviZ we can compute the <span class="math notranslate nohighlight">\(\hat R\)</span> diagnostics with the <code class="docutils literal notranslate"><span class="pre">az.rhat()</span></code>
function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">rhat</span><span class="p">(</span><span class="n">chains</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;xarray.Dataset&gt;
Dimensions:      ()
Data variables:
    good_chains  float64 1.000
    bad_chains0  float64 2.408
    bad_chains1  float64 1.033
</pre></div>
</div>
<p>From this result we can see that <span class="math notranslate nohighlight">\(\hat R\)</span> correctly identifies
<code class="docutils literal notranslate"><span class="pre">good_chains</span></code> as a good sample and <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> and <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> as
samples with different degree of problems. While <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> is a
total disaster, <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> seems to be closer to reaching the
<em>ok-chain status</em>, but still off.</p>
</section>
<section id="monte-carlo-standard-error">
<span id="id21"></span><h3><span class="section-number">2.4.3. </span>Monte Carlo Standard Error<a class="headerlink" href="#monte-carlo-standard-error" title="Permalink to this headline">¶</a></h3>
<p>When using MCMC methods we introduce an additional layer of uncertainty
as we are approximating the posterior with a finite number of samples.
We can estimate the amount of error introduced using the Monte Carlo
standard error (MCSE), which is based on Markov chain central limit
theorem (see Section <a class="reference internal" href="chp_11.html#markov-chains"><span class="std std-ref">Markov Chains</span></a>). The MCSE takes into
account that the samples are not truly independent of each other and are
in fact computed from the ESS <span id="id22">[<a class="reference internal" href="references.html#id13" title="Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. Bayesian Analysis, pages 1 – 38, 2021. URL: https://doi.org/10.1214/20-BA1221, doi:10.1214/20-BA1221.">19</a>]</span>. While the values of
ESS and <span class="math notranslate nohighlight">\(\hat R\)</span> are independent of the scale of the parameters,
interpreting whether MCSE is small enough requires domain expertise. If
we want to report the value of an estimated parameter to the second
decimal, we need to be sure the MCSE is below the second decimal
otherwise, we will be, wrongly, reporting a higher precision than we
really have. We should check the MCSE only once we are sure ESS is high
enough and <span class="math notranslate nohighlight">\(\hat R\)</span> is low enough; otherwise, MCSE is of no use.</p>
<p>Using ArviZ we can compute the MCSE with the function <code class="docutils literal notranslate"><span class="pre">az.mcse()</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">mcse</span><span class="p">(</span><span class="n">chains</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;xarray.Dataset&gt;
Dimensions:      ()
Data variables:
    good_chains  float64 0.002381
    bad_chains0  float64 0.1077
    bad_chains1  float64 0.01781
</pre></div>
</div>
<p>As with the ESS the MCSE varies across the parameter space and then we
may also want to evaluate it for different regions, like specific
quantiles. Additionally, we may also want to visualize several values at
once as in <a class="reference internal" href="#fig-plot-mcse"><span class="std std-numref">Fig. 2.9</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_mcse</span><span class="p">(</span><span class="n">chains</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="fig-plot-mcse">
<a class="reference internal image-reference" href="../_images/plot_mcse.png"><img alt="../_images/plot_mcse.png" src="../_images/plot_mcse.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.9 </span><span class="caption-text">Local MCSE for quantiles. Subplots y-axis share the same scale to ease
comparison between them. Ideally, we want the MCSE to be small across
all regions of the parameter space. Note how the MCSE values for
<code class="docutils literal notranslate"><span class="pre">good_chains</span></code> is relatively low across all values compared to MCSE of
both bad chains.</span><a class="headerlink" href="#fig-plot-mcse" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Finally, the ESS, <span class="math notranslate nohighlight">\(\hat R\)</span>, and MCSE can all be computed with a single
call to the <code class="docutils literal notranslate"><span class="pre">az.summary(.)</span></code> function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;diagnostics&quot;</span><span class="p">)</span>
</pre></div>
</div>
<table class="table">
<colgroup>
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mcse_mean</strong>*</p></td>
<td><p><strong>mcse_sd</strong></p></td>
<td><p><strong>ess_bulk</strong></p></td>
<td><p><strong>ess_tail</strong></p></td>
<td><p><strong>r_hat</strong></p></td>
</tr>
<tr class="row-even"><td><p>good_chains</p></td>
<td><p>0.002</p></td>
<td><p>0.002</p></td>
<td><p>4389.0</p></td>
<td><p>3966.0</p></td>
<td><p>1.00</p></td>
</tr>
<tr class="row-odd"><td><p>bad_chains0</p></td>
<td><p>0.108</p></td>
<td><p>0.088</p></td>
<td><p>2.0</p></td>
<td><p>11.0</p></td>
<td><p>2.41</p></td>
</tr>
<tr class="row-even"><td><p>bad_chains1</p></td>
<td><p>0.018</p></td>
<td><p>0.013</p></td>
<td><p>111.0</p></td>
<td><p>105.0</p></td>
<td><p>1.03</p></td>
</tr>
</tbody>
</table>
<p>The first column is the Monte Carlo standard error for the mean or
(expectation), the second one is the Monte Carlo standard error for the
standard deviation <a class="footnote-reference brackets" href="#id62" id="id23">10</a>. Then we have the bulk and tail effective
sample size and finally the <span class="math notranslate nohighlight">\(\hat R\)</span> diagnostic.</p>
</section>
<section id="trace-plots">
<span id="id24"></span><h3><span class="section-number">2.4.4. </span>Trace Plots<a class="headerlink" href="#trace-plots" title="Permalink to this headline">¶</a></h3>
<p>Trace plots are probably the most popular plots in Bayesian literature.
They are often the first plot we make after inference, to visually check
<em>what we got</em>. A trace plot is made by drawing the sampled values at
each iteration step. From these plots we should be able to see if
different chains converge to the same distribution, we can get a <em>sense</em>
of the degree of autocorrelation, etc. In ArviZ by calling the function
<code class="docutils literal notranslate"><span class="pre">az.plot_trace(.)</span></code> we get a trace plot on the right plus a
representation of the distribution of the sample values, using a KDE for
continuous variables and a histogram for discrete ones on the left.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">chains</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="fig-trace-plots">
<a class="reference internal image-reference" href="../_images/trace_plots.png"><img alt="../_images/trace_plots.png" src="../_images/trace_plots.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.10 </span><span class="caption-text">On the left column, we see one KDE per chain. On the right, column we
see the values of the sampled values per chain per step. Note the
differences in the KDE and trace plots between each of the example
chains, particularly the <em>fuzzy caterpillar</em> appearance in <code class="docutils literal notranslate"><span class="pre">good_chains</span></code>
versus the irregularities in the other two.</span><a class="headerlink" href="#fig-trace-plots" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-trace-plots"><span class="std std-numref">Fig. 2.10</span></a> shows the trace plots for <code class="docutils literal notranslate"><span class="pre">chains</span></code>. From it,
we can see that the draws in <code class="docutils literal notranslate"><span class="pre">good_chains</span></code> belong to the same
distribution as there are only small (random) differences between both
chains. When we see the draws ordered by iteration (i.e. the trace
itself) we can see that chains look rather <em>noisy</em> with no apparent
trend or pattern, it is also difficult to distinguish one chain from the
other. This is in clear contrast to what we get for <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code>. For
this sample we clearly see two different distributions with only some
overlap. This is easy to see both from the KDE and from the trace. The
chains are exploring two different regions of the parameter space. The
situation for <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> is a little bit more subtle. The KDE shows
distributions that seem to be similar to those from <code class="docutils literal notranslate"><span class="pre">good_chains</span></code> the
differences between the two chains are more clear. Do we have 2 or 3
peaks? The distributions do not seem to agree, maybe we just have one
mode and the extra peaks are artifacts! Peaks generally look suspicious
unless we have reasons to believe in multi-modal distributions arising,
for example, from sub-populations in our data. The trace also seems to
be somehow similar to the one from <code class="docutils literal notranslate"><span class="pre">good_chains</span></code>, but a more careful
inspection reveals the presence of long regions of monotonicity (the
lines parallel to the x-axis). This is a clear indication that the
sampler is getting stuck in some regions of the parameter space, maybe
because we have a multimodal posterior with barrier between modes of
very low probability or perhaps because we have some regions of the
parameter space with a curvature that is too different from the rest.</p>
</section>
<section id="autocorrelation-plots">
<span id="autocorr-plot"></span><h3><span class="section-number">2.4.5. </span>Autocorrelation Plots<a class="headerlink" href="#autocorrelation-plots" title="Permalink to this headline">¶</a></h3>
<p>As we saw when we discussed the effective sample size, autocorrelation
decreases the actual amount of information contained in a sample and
thus something we want to keep at a minimum. We can directly inspect the
autocorrelation with <code class="docutils literal notranslate"><span class="pre">az.plot_autocorr</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_autocorr</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="fig-autocorrelation-plot">
<a class="reference internal image-reference" href="../_images/autocorrelation_plot.png"><img alt="../_images/autocorrelation_plot.png" src="../_images/autocorrelation_plot.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.11 </span><span class="caption-text">Bar plot of the autocorrelation function over a 100 steps window. The
height of the bars for <code class="docutils literal notranslate"><span class="pre">good_chains</span></code> is close to zero (and mostly inside
the gray band) for the entire plot, which indicates very low
autocorrelation. The tall bars in <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> and <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code>
indicate large values of autocorrelation, which is undesirable. The gray
band represents the 95% confidence interval.</span><a class="headerlink" href="#fig-autocorrelation-plot" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>What we see in <a class="reference internal" href="#fig-autocorrelation-plot"><span class="std std-numref">Fig. 2.11</span></a> is at least
qualitatively expected after seeing the results from <code class="docutils literal notranslate"><span class="pre">az.ess</span></code>.
<code class="docutils literal notranslate"><span class="pre">good_chains</span></code> shows essentially zero autocorrelation, <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> is
highly correlated and <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> is not that bad, but autocorrelation
is still noticeable and is long-range, i.e. it does not drop quickly.</p>
</section>
<section id="rank-plots">
<span id="id25"></span><h3><span class="section-number">2.4.6. </span>Rank Plots<a class="headerlink" href="#rank-plots" title="Permalink to this headline">¶</a></h3>
<p>Rank plots are another visual diagnostic we can use to compare the
sampling behavior both within and between chains. Rank plots, simply
put, are histograms of the ranked samples. The ranks are computed by
first combining all chains but then plotting the results separately for
each chain. If all of the chains are targeting the same distribution, we
expect the ranks to have a Uniform distribution. Additionally if rank
plots of all chains look similar, this indicates good mixing of the
chains <span id="id26">[<a class="reference internal" href="references.html#id13" title="Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. Bayesian Analysis, pages 1 – 38, 2021. URL: https://doi.org/10.1214/20-BA1221, doi:10.1214/20-BA1221.">19</a>]</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_rank</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;bars&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="fig-rank-plot-bars">
<a class="reference internal image-reference" href="../_images/rank_plot_bars.png"><img alt="../_images/rank_plot_bars.png" src="../_images/rank_plot_bars.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.12 </span><span class="caption-text">Rank plots using the <code class="docutils literal notranslate"><span class="pre">bar</span></code> representation. In particular, compare the
height of the bar to the dashed line representing a Uniform
distribution. Ideally, the bars should follow a Uniform distribution.</span><a class="headerlink" href="#fig-rank-plot-bars" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>One alternative to the “bars” representation is vertical lines,
shortened to “vlines”.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_rank</span><span class="p">(</span><span class="n">chains</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;vlines&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="fig-rank-plot-vlines">
<a class="reference internal image-reference" href="../_images/rank_plot_vlines.png"><img alt="../_images/rank_plot_vlines.png" src="../_images/rank_plot_vlines.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.13 </span><span class="caption-text">Rank plots using the <code class="docutils literal notranslate"><span class="pre">vline</span></code> representation. The shorter the vertical
lines the better. vertical lines above the dashed line indicate an
excess sampled values for a particular rank, vertical lines below
indicate a lack of sampled values.</span><a class="headerlink" href="#fig-rank-plot-vlines" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We can see in Figures <a class="reference internal" href="#fig-rank-plot-bars"><span class="std std-numref">Fig. 2.12</span></a> and
<a class="reference internal" href="#fig-rank-plot-vlines"><span class="std std-numref">Fig. 2.13</span></a> that for <code class="docutils literal notranslate"><span class="pre">good_chains</span></code> the ranks are very
close to Uniform and that both chains look similar to each other with
not distinctive pattern. This is in clear contrast with the results for
<code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> where the chains depart from uniformity, and they are
exploring two separate sets of value, with some overlap over the middle
ranks. Notice how this is consistent to the way we created <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code>
and also with what we saw in its trace plot. <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code> is somehow
Uniform but with a few large deviations here and there, reflecting that
the problems are more <em>local</em> than those from <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code>.</p>
<p>Rank plots can be more sensitive than trace plots, and thus we recommend
them over the latter. We can obtain them using
<code class="docutils literal notranslate"><span class="pre">az.plot_trace(.,</span> <span class="pre">kind=&quot;rank_bars&quot;)</span></code> or
<code class="docutils literal notranslate"><span class="pre">az.plot_trace(.,</span> <span class="pre">kind=&quot;rank_vlines&quot;)</span></code>. These functions not only plot
the ranks but also the marginal distributions of the posterior. This
kind of plot can be useful to quickly get a sense of what the posterior
<em>looks like</em>, which in many cases can help us to spot problems with
sampling or model definition, especially during the early phases of
modeling when we are most likely not sure about what we really want to
do and as a result, we need to explore many different alternatives. As
we progress and the model or models start to make more sense we can then
check that the ESS, <span class="math notranslate nohighlight">\(\hat R\)</span>, and MCSE are okay and if not okay know
that our model needs further refinements.</p>
</section>
<section id="divergences">
<span id="id27"></span><h3><span class="section-number">2.4.7. </span>Divergences<a class="headerlink" href="#divergences" title="Permalink to this headline">¶</a></h3>
<p>So far we have been diagnosing how well a sampler works by studying the
generated samples. Another way to perform a diagnostic is by monitoring
the behavior of the inner workings of the sampling method. One prominent
example of such diagnostics is the concept of divergences present in
some <strong>Hamiltonian Monte Carlo</strong> (HMC) methods <a class="footnote-reference brackets" href="#id63" id="id28">11</a>. Divergences, or
more correctly divergent transitions, are a powerful and sensitive way
of diagnosing samples and works as complement to the diagnostics we saw
in the previous sections.</p>
<p>Let us discuss divergences in the context of a very simple model, we
will find more realistic examples later through the book. Our model
consists of a parameter <span class="math notranslate nohighlight">\(\theta2\)</span> following a Uniform distribution in
the interval <span class="math notranslate nohighlight">\([-\theta1, \theta1]\)</span>, and <span class="math notranslate nohighlight">\(\theta1\)</span> is sampled from a
normal distribution. When <span class="math notranslate nohighlight">\(\theta1\)</span> is large <span class="math notranslate nohighlight">\(\theta2\)</span> will follow a
Uniform distribution spanning a wide range, and when <span class="math notranslate nohighlight">\(\theta1\)</span>
approaches zero, the width of <span class="math notranslate nohighlight">\(\theta2\)</span> will also approach zero. Using
PyMC3 we can write this model as:</p>
<div class="literal-block-wrapper docutils container" id="divm0">
<div class="code-block-caption"><span class="caption-number">Listing 2.1 </span><span class="caption-text">divm0</span><a class="headerlink" href="#divm0" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_0</span><span class="p">:</span>
    <span class="n">θ1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;θ1&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">θ2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;θ2&quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">θ1</span><span class="p">,</span> <span class="n">θ1</span><span class="p">)</span>
    <span class="n">idata_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition-the-arviz-inferencedata-format admonition">
<p class="admonition-title">The ArviZ InferenceData format</p>
<p><code class="docutils literal notranslate"><span class="pre">az.InferenceData</span></code> is a specialized data format designed for MCMC Bayesian users.
It is based on xarray
<span id="id29">[<a class="reference internal" href="references.html#id171" title="Stephan Hoyer and Joe Hamman. Xarray: nd labeled arrays and datasets in python. Journal of Open Research Software, 2017.">20</a>]</span>, a flexible N dimensional array package. The main purpose
of the InferenceData object is to provide a convenient way to store and
manipulate the information generated during a Bayesian workflow,
including samples from distributions like the posterior, prior,
posterior predictive, prior predictive and other information and
diagnostics generated during sampling. InferenceData objects keeps all
this information organized using a concept called groups.</p>
<p>In this book we extensively utilize <code class="docutils literal notranslate"><span class="pre">az.InferenceData</span></code>. We use it to
store Bayesian inference results, calculate diagnostics, generate plots,
and read and write from disk. Refer to the ArviZ documentation for a
full technical specification and API.</p>
</div>
<p>Notice how the model in Code Block <a class="reference internal" href="#divm0"><span class="std std-ref">divm0</span></a> is not
conditioned on any observations, which means <code class="docutils literal notranslate"><span class="pre">model_0</span></code> specifies a
posterior distribution parameterized by two unknowns (<code class="docutils literal notranslate"><span class="pre">θ1</span></code> and <code class="docutils literal notranslate"><span class="pre">θ2</span></code>).
You may have also noticed that we have included the argument
<code class="docutils literal notranslate"><span class="pre">testval=0.1</span></code>. We do this to instruct PyMC3 to start sampling from a
particular value (<span class="math notranslate nohighlight">\(0.1\)</span> in this example), instead of from its default.
The default value is <span class="math notranslate nohighlight">\(\theta1 = 0\)</span> and for that value the probability
density function of <span class="math notranslate nohighlight">\(\theta2\)</span> is a Dirac delta <a class="footnote-reference brackets" href="#id64" id="id30">12</a>, which will produce
an error. Using <code class="docutils literal notranslate"><span class="pre">testval=0.1</span></code> only affects how the sampling is
initialized.</p>
<p>In <a class="reference internal" href="#fig-divergences-trace"><span class="std std-numref">Fig. 2.14</span></a> we can see vertical bars at the
bottom of the KDEs for <code class="docutils literal notranslate"><span class="pre">model0</span></code>. Each one of these bars represents a
divergence, indicating that something went wrong during sampling. We can
see something similar using other plots, like with
<code class="docutils literal notranslate"><span class="pre">az.plot_pair(.,</span> <span class="pre">divergences=True)</span></code> as shown in
<a class="reference internal" href="#fig-divergences-pair"><span class="std std-numref">Fig. 2.15</span></a>, here the divergences are the blue dots,
which are everywhere!</p>
<figure class="align-default" id="fig-divergences-trace">
<a class="reference internal image-reference" href="../_images/divergences_trace.png"><img alt="../_images/divergences_trace.png" src="../_images/divergences_trace.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.14 </span><span class="caption-text">KDEs and rank plots for Model 0 Code <a class="reference internal" href="#divm0"><span class="std std-ref">divm0</span></a>, Model
1 <a class="reference internal" href="#divm1"><span class="std std-ref">divm1</span></a> and Model 1bis, which is the same as
Model 1 <a class="reference internal" href="#divm1"><span class="std std-ref">divm1</span></a> but with
<code class="docutils literal notranslate"><span class="pre">pm.sample(.,</span> <span class="pre">target_accept=0.95)</span></code>. The black vertical bars represent
divergences.</span><a class="headerlink" href="#fig-divergences-trace" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-divergences-pair">
<a class="reference internal image-reference" href="../_images/divergences_pair.png"><img alt="../_images/divergences_pair.png" src="../_images/divergences_pair.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.15 </span><span class="caption-text">Scatter plot of the posterior samples from Model 0 from Code
<a class="reference internal" href="#divm0"><span class="std std-ref">divm0</span></a>, Model 1 from Code Block
<a class="reference internal" href="#divm1"><span class="std std-ref">divm1</span></a>, and Model 1bis, which is the same as Model
1 in Code Block <a class="reference internal" href="#divm1"><span class="std std-ref">divm1</span></a> but with
<code class="docutils literal notranslate"><span class="pre">pm.sample(.,</span> <span class="pre">target_accept=0.95)</span></code>. The blue dots represent divergences.</span><a class="headerlink" href="#fig-divergences-pair" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Something is definitely problematic with <code class="docutils literal notranslate"><span class="pre">model0</span></code>. Upon inspection of
the model definition in Code Block <a class="reference internal" href="#divm0"><span class="std std-ref">divm0</span></a> we may
realize that we defined it in a weird way. <span class="math notranslate nohighlight">\(\theta1\)</span> is a Normal
distribution centered at 0, and thus we should expect half of the values
to be negative, but for negative values <span class="math notranslate nohighlight">\(\theta2\)</span> will be defined in the
interval <span class="math notranslate nohighlight">\([\theta1, -\theta1]\)</span>, which is at least a little bit weird.
So, let us try to <strong>reparameterize</strong> the model, i.e. express the model
in a different but mathematically equivalent way. For example, we can
do:</p>
<div class="literal-block-wrapper docutils container" id="divm1">
<div class="code-block-caption"><span class="caption-number">Listing 2.2 </span><span class="caption-text">divm1</span><a class="headerlink" href="#divm1" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_1</span><span class="p">:</span>
    <span class="n">θ1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;θ1&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">θ2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;θ2&quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">θ1</span><span class="p">,</span> <span class="n">θ1</span><span class="p">)</span>
    <span class="n">idata_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now <span class="math notranslate nohighlight">\(\theta1\)</span> will always provide reasonable values we can feed into the
definition of <span class="math notranslate nohighlight">\(\theta2\)</span>. Notice that we have defined the standard
deviation of <span class="math notranslate nohighlight">\(\theta1\)</span> as <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{(1-\frac{2}{\pi})}}\)</span> instead
of just 1. This is because the standard deviation of the half-normal is
<span class="math notranslate nohighlight">\(\sigma \sqrt{(1-\frac{2}{\pi})}\)</span> where <span class="math notranslate nohighlight">\(\sigma\)</span> is the scale parameter
of the half-normal. In other words, <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation
of the <em>unfolded</em> Normal, not the Half-normal distribution.</p>
<p>Anyway, let us see how these reparameterized models do with respect to
divergences. <a class="reference internal" href="#fig-divergences-trace"><span class="std std-numref">Fig. 2.14</span></a> and
<a class="reference internal" href="#fig-divergences-pair"><span class="std std-numref">Fig. 2.15</span></a> show that the number of divergences has
been reduced dramatically for <code class="docutils literal notranslate"><span class="pre">model1</span></code>, but we still can see a few of
them. One easy option we can try to reduce divergences is increasing the
value of <code class="docutils literal notranslate"><span class="pre">target_accept</span></code> as shown in Code Block
<a class="reference internal" href="#divm2"><span class="std std-ref">divm2</span></a>, by default this value is 0.8 and the
maximum valid value is 1 (see Section <a class="reference internal" href="chp_11.html#hmc"><span class="std std-ref">Hamiltonian Monte Carlo</span></a> for details).</p>
<div class="literal-block-wrapper docutils container" id="divm2">
<div class="code-block-caption"><span class="caption-number">Listing 2.3 </span><span class="caption-text">divm2</span><a class="headerlink" href="#divm2" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_1bis</span><span class="p">:</span>
    <span class="n">θ1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;θ1&quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">θ2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;θ2&quot;</span><span class="p">,</span> <span class="o">-</span><span class="n">θ1</span><span class="p">,</span> <span class="n">θ1</span><span class="p">)</span>
    <span class="n">idata_1bis</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">target_accept</span><span class="o">=</span><span class="mf">.95</span><span class="p">,</span> <span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model1bis</span></code> in <a class="reference internal" href="#fig-divergences-trace"><span class="std std-numref">Fig. 2.14</span></a> and
<a class="reference internal" href="#fig-divergences-pair"><span class="std std-numref">Fig. 2.15</span></a> is the same as <code class="docutils literal notranslate"><span class="pre">model1</span></code> but with we have
changed the default value of one of the sampling parameters
<code class="docutils literal notranslate"><span class="pre">pm.sample(.,</span> <span class="pre">target_accept=0.95)</span></code>. We can see that finally we have
removed all the divergences. This is already good news, but in order to
trust these samples, we still need to check the value of <span class="math notranslate nohighlight">\(\hat R\)</span> and
ESS as explained in previous sections.</p>
<div class="admonition-reparameterization admonition">
<p class="admonition-title">Reparameterization</p>
<p>Reparameterization can be useful to turn a difficult
to sample posterior geometry into an easier one. This could help to
remove divergences, but it can also help even if no divergences are
present. For example, we can use it to speed up sampling or increase the
number of effective samples, without having to increase the
computational cost. Additionally, reparameterization can also help to
better interpret or communicate models and their results (see Alice and
Bob example in Section <a class="reference internal" href="chp_01.html#conjugate-priors"><span class="std std-ref">Conjugate Priors</span></a>).</p>
</div>
</section>
<section id="sampler-parameters-and-other-diagnostics">
<span id="id31"></span><h3><span class="section-number">2.4.8. </span>Sampler Parameters and Other Diagnostics<a class="headerlink" href="#sampler-parameters-and-other-diagnostics" title="Permalink to this headline">¶</a></h3>
<p>Most sampler methods have hyperparameters that affect the sampler
performance. While most PPLs try to use sensible defaults, in practice,
they will not work for every possible combination of data and models. As
we saw in the previous sections, divergences can sometimes be removed by
increasing the parameter <code class="docutils literal notranslate"><span class="pre">target_accept</span></code>, for example, if the
divergences originated from numerical imprecision. There are other
sampler parameters that can also help with sampling issues, for example,
we may want to increase the number of iterations used to tune MCMC
samplers. In PyMC3 we have <code class="docutils literal notranslate"><span class="pre">pm.sample(.,tune=1000)</span></code> by default. During
the tuning phase sampler parameters get automatically adjusted. Some
models are more complex and require more interactions for the sampler to
learn better parameters. Thus increasing the tuning steps can help to
increase the ESS or lower the <span class="math notranslate nohighlight">\(\hat R\)</span>. Increasing the number of draws
can also help with convergence but in general other routes are more
productive. If a model is failing to converge with a few thousands of
draws, it will generally still fail with 10 times more draws or the
slight improvement will not justify the extra computational cost.
Reparameterization, improved model structure, more informative priors,
or even changing the model will most often be much more effective <a class="footnote-reference brackets" href="#id65" id="id32">13</a>.
We want to note that in the early stages of modeling we could use a
relatively low number of draws to test that the model runs, that we have
actually written the intended model, that we broadly get reasonable
results. For this initial check around 200 or 300 is typically
sufficient., Then we can increase the number of draws to a few thousand,
maybe around 2000 or 4000, when we are more confident about the model.</p>
<p>In addition to the diagnostics shown in this chapter, additional
diagnostics exist, such as parallel plots, and separation plots. All
these diagnostics are useful and have their place but for the brevity of
this text we have omitted them from this section. To see others we
suggest visiting the ArviZ documentation and plot gallery which contains
many more examples.</p>
</section>
</section>
<section id="model-comparison">
<span id="model-cmp"></span><h2><span class="section-number">2.5. </span>Model Comparison<a class="headerlink" href="#model-comparison" title="Permalink to this headline">¶</a></h2>
<p>Usually we want to build models that are not too simple that they miss
valuable information in our data nor too complex that they fit the noise
in the data. Finding this <em>sweet spot</em> is a complex task. Partially
because there is not a single criteria to define an optimal solution,
partly because such optimal solution may not exist, and partly because
in practice we need to choose from a limited set of models evaluated
over a finite dataset.</p>
<p>Nevertheless, we can still try to find good general strategies. One
useful solution is to compute the generalization error, also known as
out-of-sample predictive accuracy. This is an estimate of how well a
model behaves at predicting data not used to fit it. Ideally, any
measure of predictive accuracy should take into account the details of
the problem we are trying to solve, including the benefits and costs
associated with the model’s predictions. That is, we should apply a
decision theoretic approach. However, we can also rely on general
devices that are applicable to a wide range of models and problems. Such
devices are sometimes referred to as scoring rules, as they help us to
score and rank models. From the many possible scoring rules it has been
shown that the logarithmic scoring rule has very nice theoretical
properties <span id="id33">[<a class="reference internal" href="references.html#id18" title="Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359–378, 2007.">21</a>]</span>, and thus is widely used. Under a Bayesian
setting the log scoring rule can be computed as.</p>
<div class="math notranslate nohighlight" id="equation-eq-elpd">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-eq-elpd" title="Permalink to this equation">¶</a></span>\[\text{ELPD} = \sum_{i=1}^{n} \int p_t(\tilde y_i) \; \log p(\tilde y_i \mid y_i) \; d\tilde y_i\]</div>
<p>where <span class="math notranslate nohighlight">\(p_t(\tilde y_i)\)</span> is the distribution of the true data-generating
process for <span class="math notranslate nohighlight">\(\tilde y_i\)</span> and <span class="math notranslate nohighlight">\(p(\tilde y_i \mid y_i)\)</span> is the posterior
predictive distribution. The quantity defined in Equation <a class="reference internal" href="#equation-eq-elpd">(2.4)</a>
is known as the <strong>expected log pointwise predictive density</strong> (ELPD).
Expected because we are integrating over the true data-generating
process i.e over all the possible datasets that could be generated from
that process, and pointwise because we perform the computations per
observation (<span class="math notranslate nohighlight">\(y_i\)</span>), over the <span class="math notranslate nohighlight">\(n\)</span> observations. For simplicity we use
the term density for both continuous and discrete models <a class="footnote-reference brackets" href="#id66" id="id34">14</a>.</p>
<p>For real problems we do not know <span class="math notranslate nohighlight">\(p_t(\tilde y_i)\)</span> and thus the ELPD as
defined in Equation <a class="reference internal" href="#equation-eq-elpd">(2.4)</a> is of no immediate use, in practice we
can instead compute:</p>
<div class="math notranslate nohighlight" id="equation-eq-elpd-practice">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-eq-elpd-practice" title="Permalink to this equation">¶</a></span>\[\sum_{i=1}^{n} \log \int \ p(y_i \mid \boldsymbol{\theta}) \; p(\boldsymbol{\theta} \mid y) d\boldsymbol{\theta}\]</div>
<p>The quantity defined by Equation <a class="reference internal" href="#equation-eq-elpd-practice">(2.5)</a> (or that
quantity multiplied by some constant) is usually known as the deviance,
and it is used in both Bayesians and non-Bayesians contexts <a class="footnote-reference brackets" href="#id67" id="id35">15</a>. When
the likelihood is Gaussian, then Equation <a class="reference internal" href="#equation-eq-elpd-practice">(2.5)</a> will be
proportional to the quadratic mean error.</p>
<p>To compute Equation <a class="reference internal" href="#equation-eq-elpd-practice">(2.5)</a> we used the same data used to
fit the model and thus we will, on average, overestimate the ELPD
(Equation <a class="reference internal" href="#equation-eq-elpd">(2.4)</a>) which will lead us to choose models prone to
overfitting. Fortunately, there are a few ways to produce better
estimations of the ELPD. One of them is cross-validation as we will see
in the next section.</p>
<section id="cross-validation-and-loo">
<span id="cv-and-loo"></span><h3><span class="section-number">2.5.1. </span>Cross-validation and LOO<a class="headerlink" href="#cross-validation-and-loo" title="Permalink to this headline">¶</a></h3>
<p>Cross-validation (CV) is a method of estimating out-of-sample predictive
accuracy. This method requires re-fitting a model many times, each time
excluding a different portion of the data. The excluded portion is then
used to measure the accuracy of the model. This process is repeated many
times and the estimated accuracy of the model will be the average over
all runs. Then the entire dataset is used to fit the model one more time
and this is the model used for further analysis and/or predictions. We
can see CV as a way to simulate or approximate out-of-sample statistics,
while still using all the data.</p>
<p>Leave-one-out cross-validation (LOO-CV) is a particular type of
cross-validation when the data excluded is a single data-point. The ELPD
computed using LOO-CV is <span class="math notranslate nohighlight">\(\text{ELPD}_\text{LOO-CV}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-elpd-loo-cv">
<span class="eqno">(2.6)<a class="headerlink" href="#equation-eq-elpd-loo-cv" title="Permalink to this equation">¶</a></span>\[\text{ELPD}_\text{LOO-CV} = \sum_{i=1}^{n} \log
    \int \ p(y_i \mid \boldsymbol{\theta}) \; p(\boldsymbol{\theta} \mid y_{-i}) d\boldsymbol{\theta}\]</div>
<p>Computing Equation <a class="reference internal" href="#equation-eq-elpd-loo-cv">(2.6)</a> can easily become too costly as
in practice we do not know <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> and thus we need to
compute <span class="math notranslate nohighlight">\(n\)</span> posteriors, i.e. as many values of
<span class="math notranslate nohighlight">\(\boldsymbol{\theta_{-i}}\)</span> as observations we have in our dataset.
Fortunately, we can approximate <span class="math notranslate nohighlight">\(\text{ELPD}_\text{LOO-CV}\)</span> from a
single fit to the data by using a method known as Pareto smoothed
importance sampling leave-one-out cross validation PSIS-LOO-CV (see Section
<a class="reference internal" href="chp_11.html#loo-depth"><span class="std std-ref">LOO in Depth</span></a> for details). For brevity, and for
consistency with ArviZ, in this book we call this method LOO. It is
important to remember we are talking about PSIS-LOO-CV and unless we
state it otherwise when we refer to ELPD we are talking about the ELPD
as estimated by this method.</p>
<p>ArviZ provides many LOO-related functions, using them is very simple but
understanding the results may require a little bit of care. Thus, to
illustrate how to interpret the output of these functions we are going
to use 3 simple models. The models are defined in Code Block
<a class="reference internal" href="#pymc3-models-for-loo"><span class="std std-ref">pymc3_models_for_loo</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="pymc3-models-for-loo">
<div class="code-block-caption"><span class="caption-number">Listing 2.4 </span><span class="caption-text">pymc3_models_for_loo</span><a class="headerlink" href="#pymc3-models-for-loo" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">idatas_cmp</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c1"># Generate data from Skewnormal likelihood model</span>
<span class="c1"># with fixed mean and skewness and random standard deviation</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mA</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">SkewNormal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">idataA</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># add_groups modifies an existing az.InferenceData</span>
<span class="n">idataA</span><span class="o">.</span><span class="n">add_groups</span><span class="p">({</span><span class="s2">&quot;posterior_predictive&quot;</span><span class="p">:</span>
                  <span class="p">{</span><span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idataA</span><span class="p">)[</span><span class="s2">&quot;y&quot;</span><span class="p">][</span><span class="kc">None</span><span class="p">,:]}})</span>
<span class="n">idatas_cmp</span><span class="p">[</span><span class="s2">&quot;mA&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idataA</span>

<span class="c1"># Generate data from Normal likelihood model</span>
<span class="c1"># with fixed mean with random standard deviation</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mB</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">idataB</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">idataB</span><span class="o">.</span><span class="n">add_groups</span><span class="p">({</span><span class="s2">&quot;posterior_predictive&quot;</span><span class="p">:</span>
                  <span class="p">{</span><span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idataB</span><span class="p">)[</span><span class="s2">&quot;y&quot;</span><span class="p">][</span><span class="kc">None</span><span class="p">,:]}})</span>
<span class="n">idatas_cmp</span><span class="p">[</span><span class="s2">&quot;mB&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idataB</span>

<span class="c1"># Generate data from Normal likelihood model</span>
<span class="c1"># with random mean and random standard deviation</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">mC</span><span class="p">:</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_obs</span><span class="p">)</span>
    <span class="n">idataC</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">idataC</span><span class="o">.</span><span class="n">add_groups</span><span class="p">({</span><span class="s2">&quot;posterior_predictive&quot;</span><span class="p">:</span>
                  <span class="p">{</span><span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">idataC</span><span class="p">)[</span><span class="s2">&quot;y&quot;</span><span class="p">][</span><span class="kc">None</span><span class="p">,:]}})</span>
<span class="n">idatas_cmp</span><span class="p">[</span><span class="s2">&quot;mC&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">idataC</span>
</pre></div>
</div>
</div>
<p>To compute LOO we just need samples from the posterior <a class="footnote-reference brackets" href="#id68" id="id36">16</a>. Then we
can call <code class="docutils literal notranslate"><span class="pre">az.loo(.)</span></code>, which allows us to compute LOO for a single model.
In practice it is common to compute LOO for two or more models, and thus
a commonly used function is <code class="docutils literal notranslate"><span class="pre">az.compare(.)</span></code>.
<a class="reference internal" href="#table-compare-00"><span class="std std-numref">Table 2.1</span></a> was generated using <code class="docutils literal notranslate"><span class="pre">az.compare(idatas_cmp)</span></code>.</p>
<table class="table" id="table-compare-00">
<caption><span class="caption-number">Table 2.1 </span><span class="caption-text">Summary of model comparison. Models are ranked from lowest to highest ELPD values (loo column).</span><a class="headerlink" href="#table-compare-00" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>rank</strong></p></td>
<td><p><strong>loo</strong></p></td>
<td><p><strong>p_loo</strong></p></td>
<td><p><strong>d_loo</strong></p></td>
<td><p><strong>weight</strong></p></td>
<td><p><strong>se</strong></p></td>
<td><p><strong>dse</strong></p></td>
<td><p><strong>warning</strong></p></td>
<td><p><strong>loo_scale</strong></p></td>
</tr>
<tr class="row-even"><td><p>mB</p></td>
<td><p>0</p></td>
<td><p>-137.87</p></td>
<td><p>0.96</p></td>
<td><p>0.00</p></td>
<td><p>1.0</p></td>
<td><p>7.06</p></td>
<td><p>0.00</p></td>
<td><p>False</p></td>
<td><p>log</p></td>
</tr>
<tr class="row-odd"><td><p>mC</p></td>
<td><p>1</p></td>
<td><p>-138.61</p></td>
<td><p>2.03</p></td>
<td><p>0.74</p></td>
<td><p>0.0</p></td>
<td><p>7.05</p></td>
<td><p>0.85</p></td>
<td><p>False</p></td>
<td><p>log</p></td>
</tr>
<tr class="row-even"><td><p>mA</p></td>
<td><p>2</p></td>
<td><p>-168.06</p></td>
<td><p>1.35</p></td>
<td><p>30.19</p></td>
<td><p>0.0</p></td>
<td><p>10.32</p></td>
<td><p>6.54</p></td>
<td><p>False</p></td>
<td><p>log</p></td>
</tr>
</tbody>
</table>
<p>There are many columns in <a class="reference internal" href="#table-compare-00"><span class="std std-numref">Table 2.1</span></a>  so let us detail their meaning one by one:</p>
<ol class="simple">
<li><p>The first column is the index which lists the names of the models
taken from the keys of the dictionary passed to <code class="docutils literal notranslate"><span class="pre">az.compare(.)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank</span></code>: The ranking of the models starting from 0 (the model with
the highest predictive accuracy) to the number of models.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loo</span></code>: The list of ELPD values. The DataFrame is always sorted from
best ELPD to worst.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">p_loo</span></code>: The list of values for the penalization term. We can roughly
think of this value as the estimated effective number of parameters
(but do not take that too seriously). This value can be lower than
the actual number of parameters in a model that <em>has more structure</em>
like a hierarchical model or can be much higher than the actual
number when the model has very weak predictive capability and may
indicate a severe model misspecification.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d_loo</span></code>: The list of relative differences between the value of LOO
for the top-ranked model and the value of LOO for each model. For
this reason we will always get a value of 0 for the first model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight</span></code>: The weights assigned to each model. These weights can be
loosely interpreted as the probability of each model (among the
compared models) given the data. See Section <a class="reference internal" href="#model-averaging"><span class="std std-ref">Model Averaging</span></a> for
details.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">se</span></code>: The standard error for the ELPD computations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dse</span></code>: The standard errors of the difference between two values of
the ELPD. <code class="docutils literal notranslate"><span class="pre">dse</span></code> is not necessarily the same as the <code class="docutils literal notranslate"><span class="pre">se</span></code> because the
uncertainty about the ELPD can be correlated between models. The
value of <code class="docutils literal notranslate"><span class="pre">dse</span></code> is always 0 for the top-ranked model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warning</span></code>: If <code class="docutils literal notranslate"><span class="pre">True</span></code> this is a warning that the LOO approximation
may not be reliable (see Section <a class="reference internal" href="#k-paretto"><span class="std std-ref">Pareto Shape Parameter</span></a> for details).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loo_scale</span></code>: The scale of the reported values. The default is the
log scale. Other options are deviance, this is the log-score
multiplied by -2 (this will reverse the order: a lower ELPD will be
better). And negative-log, this is the log-score multiplied by -1,
as with the deviance scale, a lower value is better.</p></li>
</ol>
<p>We can also represent part of the information in
<a class="reference internal" href="#table-compare-00"><span class="std std-numref">Table 2.1</span></a>  graphically in <a class="reference internal" href="#fig-compare-dummy"><span class="std std-numref">Fig. 2.16</span></a>.
Models are also ranked from higher predictive accuracy to lower. The
open dots represent the values of <code class="docutils literal notranslate"><span class="pre">loo</span></code>, the black dots are the
predictive accuracy without the <code class="docutils literal notranslate"><span class="pre">p_loo</span></code> penalization term. The black
segments represent the standard error for the LOO computations <code class="docutils literal notranslate"><span class="pre">se</span></code>. The
grey segments, centered at the triangles, represent the standard errors
of the difference <code class="docutils literal notranslate"><span class="pre">dse</span></code> between the values of LOO for each model and the
best ranked model. We can see that <code class="docutils literal notranslate"><span class="pre">mB</span></code> <span class="math notranslate nohighlight">\(\approx\)</span> <code class="docutils literal notranslate"><span class="pre">mC</span></code> <span class="math notranslate nohighlight">\(&gt;\)</span> <code class="docutils literal notranslate"><span class="pre">mA</span></code>.</p>
<p>From <a class="reference internal" href="#table-compare-00"><span class="std std-numref">Table 2.1</span></a>  and <a class="reference internal" href="#fig-compare-dummy"><span class="std std-numref">Fig. 2.16</span></a> we
can see that model <code class="docutils literal notranslate"><span class="pre">mA</span></code> is ranked as the lowest one and clearly
separated from the other two. We will now discuss the other two as their
differences are more subtle. <code class="docutils literal notranslate"><span class="pre">mB</span></code> is the one with the highest predictive
accuracy, but the difference is negligible when compared with <code class="docutils literal notranslate"><span class="pre">mC</span></code>. As a
rule of thumb a difference of LOO (<code class="docutils literal notranslate"><span class="pre">d_loo</span></code>) below 4 is considered small.
The difference between these two models is that for <code class="docutils literal notranslate"><span class="pre">mB</span></code> the mean is
fixed at 0 and for <code class="docutils literal notranslate"><span class="pre">mC</span></code> the mean has a prior distribution. LOO penalizes
the addition of this prior, indicated by the value of <code class="docutils literal notranslate"><span class="pre">p_loo</span></code> which is
larger for <code class="docutils literal notranslate"><span class="pre">mC</span></code> than <code class="docutils literal notranslate"><span class="pre">mB</span></code>, and the distance between the black dot
(unpenalized ELPD) and open dot (<span class="math notranslate nohighlight">\(\text{ELPD}_\text{LOO-CV}\)</span>) is larger
for <code class="docutils literal notranslate"><span class="pre">mC</span></code> than <code class="docutils literal notranslate"><span class="pre">mB</span></code>. We can also see that <code class="docutils literal notranslate"><span class="pre">dse</span></code> between these two models
is much lower than their respective <code class="docutils literal notranslate"><span class="pre">se</span></code>, indicating their predictions
are highly correlated.</p>
<p>Given the small difference between <code class="docutils literal notranslate"><span class="pre">mB</span></code> and <code class="docutils literal notranslate"><span class="pre">mC</span></code>, it is expected that
under a slightly different dataset the rank of these model could swap,
with <code class="docutils literal notranslate"><span class="pre">mC</span></code> becoming the highest ranked model. Also the values of the
weights are expected to change (see Section <a class="reference internal" href="#model-averaging"><span class="std std-ref">Model Averaging</span></a>). We can
easily check this is true by changing the random seed and refitting the
model a few times.</p>
<figure class="align-default" id="fig-compare-dummy">
<a class="reference internal image-reference" href="../_images/compare_dummy.png"><img alt="../_images/compare_dummy.png" src="../_images/compare_dummy.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.16 </span><span class="caption-text">Model comparison using LOO. The open dots represent the values of <code class="docutils literal notranslate"><span class="pre">loo</span></code>,
the black dots are the predictive accuracy without the <code class="docutils literal notranslate"><span class="pre">p_loo</span></code>
penalization term. The black segments represent the standard error for
the LOO computations <code class="docutils literal notranslate"><span class="pre">se</span></code>. The grey segments, centered at the triangles,
represent the standard errors of the difference <code class="docutils literal notranslate"><span class="pre">dse</span></code> between the values
of LOO for each model and the best ranked model.</span><a class="headerlink" href="#fig-compare-dummy" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="expected-log-predictive-density">
<span id="elpd-plots"></span><h3><span class="section-number">2.5.2. </span>Expected Log Predictive Density<a class="headerlink" href="#expected-log-predictive-density" title="Permalink to this headline">¶</a></h3>
<p>In the previous section we computed a value of the ELPD for each model.
Since this is a <em>global</em> comparison it reduces a model, and data, to a
single number. But from Equation <a class="reference internal" href="#equation-eq-elpd-practice">(2.5)</a> and
<a class="reference internal" href="#equation-eq-elpd-loo-cv">(2.6)</a> we can see that LOO is computed as a sum of
point-wise values, one for each observation. Thus we can also perform
<em>local</em> comparisons. We can think of the individual values of the ELPD
as an indicator of how difficult it is for the model to predict a
particular observation.</p>
<p>To compare models based on the per-observation ELPD, ArviZ offers the
<code class="docutils literal notranslate"><span class="pre">az.plot_elpd(.)</span></code> function. <a class="reference internal" href="#fig-elpd-dummy"><span class="std std-numref">Fig. 2.17</span></a> shows the
comparison between models <code class="docutils literal notranslate"><span class="pre">mA</span></code>, <code class="docutils literal notranslate"><span class="pre">mB</span></code> and <code class="docutils literal notranslate"><span class="pre">mC</span></code> in a pairwise fashion.
Positive values indicate that observations are better resolved by the
first model than by the second. For example, if we observed the first
plot (<code class="docutils literal notranslate"><span class="pre">mA-</span> <span class="pre">mB</span></code>), observation 49 and 72 are better resolved by model <code class="docutils literal notranslate"><span class="pre">mA</span></code>
than model <code class="docutils literal notranslate"><span class="pre">mB</span></code>, and the opposite happens for observations 75 and 95. We
can see that the first two plots <code class="docutils literal notranslate"><span class="pre">mA-</span> <span class="pre">mB</span></code> and <code class="docutils literal notranslate"><span class="pre">mA-</span> <span class="pre">mC</span></code> are very similar,
the reason is that model <code class="docutils literal notranslate"><span class="pre">mB</span></code> and model <code class="docutils literal notranslate"><span class="pre">mC</span></code> are in fact very similar to
each other. <a class="reference internal" href="#fig-elpd-and-khat"><span class="std std-numref">Fig. 2.19</span></a> shows that observations 34, 49,
72, 75 and 82 are in fact the five most <em>extreme</em> observations.</p>
<figure class="align-default" id="fig-elpd-dummy">
<a class="reference internal image-reference" href="../_images/elpd_dummy.png"><img alt="../_images/elpd_dummy.png" src="../_images/elpd_dummy.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.17 </span><span class="caption-text">Pointwise ELPD differences. Annotated points correspond to observations
with an ELPD difference 2 times the standard deviation of the computed
ELPD differences. Differences are small in all 3 examples, especially
between <code class="docutils literal notranslate"><span class="pre">mB</span></code> and <code class="docutils literal notranslate"><span class="pre">mC</span></code>. Positive values indicate that observations are
better resolved for the first model than the second.</span><a class="headerlink" href="#fig-elpd-dummy" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="pareto-shape-parameter">
<span id="k-paretto"></span><h3><span class="section-number">2.5.3. </span>Pareto Shape Parameter<a class="headerlink" href="#pareto-shape-parameter" title="Permalink to this headline">¶</a></h3>
<p>As we already mentioned we use LOO to approximate
<span class="math notranslate nohighlight">\(\text{ELPD}_\text{LOO-CV}\)</span>. This approximation involves the computation
of a Pareto distribution (see details in Section <a class="reference internal" href="chp_11.html#loo-depth"><span class="std std-ref">LOO in Depth</span></a>),
the main purpose is to obtain a more robust estimation, the side-effect
of this computation is that the <span class="math notranslate nohighlight">\(\hat \kappa\)</span> parameter of such Pareto
distribution can be used to detect highly influential observations, i.e.
observations that have a large effect on the predictive distribution
when they are left out. In general, higher values of <span class="math notranslate nohighlight">\(\hat \kappa\)</span> can
indicate problems with the data or model, especially when
<span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.7\)</span> <span id="id37">[<a class="reference internal" href="references.html#id20" title="Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization in bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2):389–402, 2019.">16</a>, <a class="reference internal" href="references.html#id19" title="Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto smoothed importance sampling. arXiv preprint arXiv:1507.02646, 2021.">22</a>]</span>.
When this is the case the recommendations are <span id="id38">[<a class="reference internal" href="references.html#id170" title="Aki. Vehtari and Jonah. Gabry. Loo glossary. https://mc-stan.org/loo/reference/loo-glossary.html.">23</a>]</span>:</p>
<ul class="simple">
<li><p>Use the matching moment method <span id="id39">[<a class="reference internal" href="references.html#id147" title="Topi Paananen, Juho Piironen, Paul-Christian Bürkner, and Aki Vehtari. Implicitly adaptive importance sampling. Statistics and Computing, 31(2):1–19, 2021.">24</a>]</span> <a class="footnote-reference brackets" href="#id69" id="id40">17</a>. With some
additional computations, it is possible to transform the MCMC draws
from the posterior distribution to obtain more reliable importance
sampling estimates.</p></li>
<li><p>Perform exact leave-one-out cross validation for the problematic
observations or use k-fold cross-validation.</p></li>
<li><p>Use a model that is more robust to anomalous observations.</p></li>
</ul>
<p>When we get at least one value of <span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.7\)</span> we will get a
warning when calling <code class="docutils literal notranslate"><span class="pre">az.loo(.)</span></code> or <code class="docutils literal notranslate"><span class="pre">az.compare(.)</span></code>. The <code class="docutils literal notranslate"><span class="pre">warning</span></code>
column in <a class="reference internal" href="#table-compare-00"><span class="std std-numref">Table 2.1</span></a>  has only <code class="docutils literal notranslate"><span class="pre">False</span></code> values because
all the computed values of <span class="math notranslate nohighlight">\(\hat \kappa\)</span> are <span class="math notranslate nohighlight">\(&lt; 0.7\)</span> which we can check
by ourselves from <a class="reference internal" href="#fig-loo-k-dummy"><span class="std std-numref">Fig. 2.18</span></a>. We have annotated the
observations with <span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.09\)</span> values in
<a class="reference internal" href="#fig-loo-k-dummy"><span class="std std-numref">Fig. 2.18</span></a>, <span class="math notranslate nohighlight">\(0.09\)</span> is just an arbitrary number we picked,
you can try with other cutoff value if you want. Comparing
<a class="reference internal" href="#fig-elpd-dummy"><span class="std std-numref">Fig. 2.17</span></a> against <a class="reference internal" href="#fig-loo-k-dummy"><span class="std std-numref">Fig. 2.18</span></a> we can see
that the highest values of <span class="math notranslate nohighlight">\(\hat \kappa\)</span> are not necessarily the ones
with the highest values of ELPD or vice versa.</p>
<figure class="align-default" id="fig-loo-k-dummy">
<a class="reference internal image-reference" href="../_images/loo_k_dummy.png"><img alt="../_images/loo_k_dummy.png" src="../_images/loo_k_dummy.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.18 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\hat \kappa\)</span> values. Annotated points correspond to observations with
<span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.09\)</span>, a totally arbitrary threshold.</span><a class="headerlink" href="#fig-loo-k-dummy" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-elpd-and-khat">
<a class="reference internal image-reference" href="../_images/elpd_and_khat.png"><img alt="../_images/elpd_and_khat.png" src="../_images/elpd_and_khat.png" style="width: 4.5in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.19 </span><span class="caption-text">Kernel density estimate of the observations fitted with <code class="docutils literal notranslate"><span class="pre">mA</span></code>, <code class="docutils literal notranslate"><span class="pre">mB</span></code> and
<code class="docutils literal notranslate"><span class="pre">mC</span></code>. The black lines represent the values of each observation. The
annotated observations are the same one highlighted in
<a class="reference internal" href="#fig-elpd-dummy"><span class="std std-numref">Fig. 2.17</span></a> except for the observation 78, annotated in
boldface, which is only highlighted in <a class="reference internal" href="#fig-loo-k-dummy"><span class="std std-numref">Fig. 2.18</span></a>.</span><a class="headerlink" href="#fig-elpd-and-khat" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="interpreting-p-loo-when-pareto-hat-kappa-is-large">
<span id="id41"></span><h3><span class="section-number">2.5.4. </span>Interpreting p_loo When Pareto <span class="math notranslate nohighlight">\(\hat \kappa\)</span> is Large<a class="headerlink" href="#interpreting-p-loo-when-pareto-hat-kappa-is-large" title="Permalink to this headline">¶</a></h3>
<p>As previously said p_loo can be loosely interpreted as the estimated
effective number of parameters in a model. Nevertheless, for models with
large values of <span class="math notranslate nohighlight">\(\hat \kappa\)</span> we can obtain some additional
information. If <span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.7\)</span> then comparing p_loo to the number
of parameters <span class="math notranslate nohighlight">\(p\)</span> can provide us with some additional information
<span id="id42">[<a class="reference internal" href="references.html#id170" title="Aki. Vehtari and Jonah. Gabry. Loo glossary. https://mc-stan.org/loo/reference/loo-glossary.html.">23</a>]</span>:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(p\_loo &lt;&lt; p\)</span>, then the model is likely to be misspecified. You
usually also see problems in the posterior predictive checks that
the posterior predictive samples match the observations poorly.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(p\_loo &lt; p\)</span> and <span class="math notranslate nohighlight">\(p\)</span> is relatively large compared to the number
of observations (e.g., <span class="math notranslate nohighlight">\(p &gt; \frac{N}{5}\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the total
number of observations), it is usually an indication that the model
is too flexible or the priors are too uninformative. Thus it becomes
difficult to predict the left out observation.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(p\_loo &gt; p\)</span>, then the model is also likely to be badly
misspecified. If the number of parameters is <span class="math notranslate nohighlight">\(p &lt;&lt; N\)</span>, then
posterior predictive checks are also likely to already reveal some
problem <a class="footnote-reference brackets" href="#id70" id="id43">18</a>. However, if <span class="math notranslate nohighlight">\(p\)</span> is relatively large compared to the
number of observations, say <span class="math notranslate nohighlight">\(p &gt; \frac{N}{5}\)</span>, it is possible you do
not see problems in the posterior predictive checks.</p></li>
</ul>
<p>A few heuristics for fixing model misspecification you may try are:
adding more structure to the model, for example, adding nonlinear
components; using a different likelihood, for example, using an
overdispersed likelihood like a NegativeBinomial instead of a Poisson
distribution, or using mixture likelihood.</p>
</section>
<section id="loo-pit">
<span id="id44"></span><h3><span class="section-number">2.5.5. </span>LOO-PIT<a class="headerlink" href="#loo-pit" title="Permalink to this headline">¶</a></h3>
<p>As we just saw in Sections <a class="reference internal" href="#elpd-plots"><span class="std std-ref">Expected Log Predictive Density</span></a> and <a class="reference internal" href="#k-paretto"><span class="std std-ref">Pareto Shape Parameter</span></a> model
comparison, and LOO in particular, can be used for purposes other than
declaring a model is <em>better</em> than another model. We can compare models
as a way to better understand them. As the complexity of a model
increases it becomes more difficult to understand it just by looking at
its mathematical definition or the code we use to implement it. Thus,
comparing models using LOO or other tools like posterior predictive
checks, can help us to better understand them.</p>
<p>One criticism of posterior predictive checks is that we are using the
data twice, once to fit the model and once to criticize it. The LOO-PIT
plot offers an answer to this concern. The main idea is that we can use
LOO as a fast and reliable approximation to cross-validation in order to
avoid using the data twice. The “PIT part”, stands for Probability
Integral Transform<a class="footnote-reference brackets" href="#id71" id="id45">19</a>, which is a transformation in 1D where we can get
a <span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span> distribution from any continuous random variable
if we transform that random variable using its own CDF (for details see Section
<a class="reference internal" href="chp_11.html#loo-depth"><span class="std std-ref">LOO in Depth</span></a>). In LOO-PIT we do not know the true CDF, but
we approximate it with the empirical CDF. Putting aside these
mathematical details for a moment, the take-home-message is that for a
well calibrated model we should expect an approximately Uniform
distribution. If you are experiencing a Déjà vu, do not worry you do not
have extrasensory powers nor is this a glitch in the matrix. This may
sound familiar because this is in fact the very same idea we discussed
in <a class="reference internal" href="#posterior-pd"><span class="std std-ref">Understanding Your Predictions</span></a> with the function
<code class="docutils literal notranslate"><span class="pre">az.plot_bpv(idata,</span> <span class="pre">kind=&quot;u_value&quot;)</span></code>.</p>
<p>LOO-PIT is obtained by comparing the observed data <span class="math notranslate nohighlight">\(y\)</span> to posterior
predicted data <span class="math notranslate nohighlight">\(\tilde y\)</span>. The comparison is done point-wise. We have:</p>
<div class="math notranslate nohighlight" id="equation-loo-pit">
<span class="eqno">(2.7)<a class="headerlink" href="#equation-loo-pit" title="Permalink to this equation">¶</a></span>\[p_i = P(\tilde y_i \leq y_i \mid y_{-i})\]</div>
<p>Intuitively, LOO-PIT is computing the probability that the posterior
predicted data <span class="math notranslate nohighlight">\(\tilde y_i\)</span> has lower value than the observed data
<span class="math notranslate nohighlight">\(y_i\)</span>, when we remove the <span class="math notranslate nohighlight">\(i\)</span> observation. Thus the difference between
<code class="docutils literal notranslate"><span class="pre">az.plot_bpv(idata,</span> <span class="pre">kind=&quot;u_value&quot;)</span></code> and LOO-PIT is that with the latter
we are approximately avoiding using the data twice, but the overall
interpretation of the plots is the same.</p>
<p><a class="reference internal" href="#fig-loo-pit-dummy"><span class="std std-numref">Fig. 2.20</span></a> shows the LOO-PIT for models <code class="docutils literal notranslate"><span class="pre">mA</span></code>, <code class="docutils literal notranslate"><span class="pre">mB</span></code> and
<code class="docutils literal notranslate"><span class="pre">mC</span></code>. We can observe that from the perspective of model <code class="docutils literal notranslate"><span class="pre">mA</span></code> there is
more observed data than expected for low values and less data for high
values, i.e. the model is biased. On the contrary, models <code class="docutils literal notranslate"><span class="pre">mB</span></code> and <code class="docutils literal notranslate"><span class="pre">mC</span></code>
seem to be very well calibrated.</p>
<figure class="align-default" id="fig-loo-pit-dummy">
<a class="reference internal image-reference" href="../_images/loo_pit_dummy.png"><img alt="../_images/loo_pit_dummy.png" src="../_images/loo_pit_dummy.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2.20 </span><span class="caption-text">The black lines are a KDE of LOO-PIT, i.e. the proportion of predicted
values that are less or equal than the observed data, computed per each
observation. The white lines represent the expected Uniform distribution
and the gray band the expected deviation for a dataset of the same size
as the one used.</span><a class="headerlink" href="#fig-loo-pit-dummy" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="model-averaging">
<span id="id46"></span><h3><span class="section-number">2.5.6. </span>Model Averaging<a class="headerlink" href="#model-averaging" title="Permalink to this headline">¶</a></h3>
<p>Model averaging can be justified as being Bayesian about model
uncertainty as we are Bayesian about parameter uncertainty. If we can
not absolutely be sure that <em>a</em> model is <em>the</em> model (and generally we
can not), then we should somehow take that uncertainty into account for
our analysis. One way of taking into account model uncertainty is by
performing a weighted average of all the considered models, giving more
weight to the models that seem to explain or predict the data better.</p>
<p>A <em>natural</em> way to weight Bayesian models is by their marginal
likelihoods, this is known as Bayesian Model Averaging
<span id="id47">[<a class="reference internal" href="references.html#id26" title="Jennifer A Hoeting, David Madigan, Adrian E Raftery, and Chris T Volinsky. Bayesian model averaging: a tutorial (with comments by m. clyde, david draper and ei george, and a rejoinder by the authors. Statistical science, 14(4):382–417, 1999.">25</a>]</span>. While this is theoretically appealing, it is
problematic in practice (see Section <a class="reference internal" href="chp_11.html#marginal-likelihood"><span class="std std-ref">Marginal Likelihood</span></a>) for details). An
alternative is to use the values of LOO to estimate weights for each
model. We can do this by using the following formula:</p>
<div class="math notranslate nohighlight" id="equation-eq-pseudo-avg">
<span class="eqno">(2.8)<a class="headerlink" href="#equation-eq-pseudo-avg" title="Permalink to this equation">¶</a></span>\[w_i = \frac {e^{-\Delta_i }} {\sum_j^k e^{-\Delta_j }}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta_i\)</span> is the difference between the <span class="math notranslate nohighlight">\(i\)</span> value of LOO and the
highest one, assuming we are using the log- scale, which is the default
in ArviZ.</p>
<p>This approach is called pseudo Bayesian model averaging, or Akaike-like
<a class="footnote-reference brackets" href="#id72" id="id48">20</a> weighting and is an heuristic way to compute the relative
probability of each model (given a fixed set of models) from LOO<a class="footnote-reference brackets" href="#id73" id="id49">21</a>.
See how the denominator is just a normalization term ensuring that the
weights sum up to one. The solution offered by Equation
<a class="reference internal" href="#equation-eq-pseudo-avg">(2.8)</a> for computing weights is a very nice and simple
approach. One major caveat is that it does not take into account the
uncertainty in the computation of the values of LOO. We could compute
the standard error assuming a Gaussian approximation and modify Equation
<a class="reference internal" href="#equation-eq-pseudo-avg">(2.8)</a> accordingly. Or we can do something more robust,
like using Bayesian Bootstrapping.</p>
<p>Yet another option for model averaging is stacking of predictive
distributions <span id="id50">[<a class="reference internal" href="references.html#id22" title="Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using stacking to average bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3):917–1007, 2018.">26</a>]</span>. The main idea is to combine several
models in a meta-model in such a way that we minimize the divergence
between the meta-model and the <em>true</em> generating model. When using a
logarithmic scoring rule this is equivalent to computing:</p>
<div class="math notranslate nohighlight" id="equation-eq-stacking">
<span class="eqno">(2.9)<a class="headerlink" href="#equation-eq-stacking" title="Permalink to this equation">¶</a></span>\[\max_{n} \frac{1}{n} \sum_{i=1}^{n}log\sum_{j=1}^{k} w_j p(y_i \mid y_{-i}, M_j)\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of data points and <span class="math notranslate nohighlight">\(k\)</span> the number of models. To
enforce a solution we constrain <span class="math notranslate nohighlight">\(w\)</span> to be <span class="math notranslate nohighlight">\(w_j \ge 0\)</span> and
<span class="math notranslate nohighlight">\(\sum_{j=1}^{k} w_j = 1\)</span>. The quantity <span class="math notranslate nohighlight">\(p(y_i \mid y_{-i}, M_j)\)</span> is the
leave-one-out predictive distribution for the <span class="math notranslate nohighlight">\(M_j\)</span> model. As we already
said computing it can bee too costly and thus in practice we can use LOO
to approximate it.</p>
<p>Stacking has more interesting properties than pseudo Bayesian model
averaging. We can see this from their definitions, Equation
<a class="reference internal" href="#equation-eq-pseudo-avg">(2.8)</a> is just a normalization of weights that have been
computed for each model independently of the rest of the models. Instead
in Equation <a class="reference internal" href="#equation-eq-stacking">(2.9)</a> the weights are computed by maximizing the
combined log-score, i.e. even when the models have been fitted
independently as in pseudo Bayesian model averaging, the computation of
the weights takes into account all models together. This helps to explain
why model <code class="docutils literal notranslate"><span class="pre">mB</span></code> gets a weight of 1 and <code class="docutils literal notranslate"><span class="pre">mC</span></code> a weight of 0 (see
<a class="reference internal" href="#table-compare-00"><span class="std std-numref">Table 2.1</span></a> ), even if they are very similar models. Why are
the weights not around 0.5 for each one of them? The reason is that
according to the stacking procedure once <code class="docutils literal notranslate"><span class="pre">mB</span></code> is included in our set of
compared models, <code class="docutils literal notranslate"><span class="pre">mC</span></code> does not provide new information. In other words
including it will be redundant.</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">pm.sample_posterior_predictive_w(.)</span></code> accepts a list of
traces and a list of weights allowing us to easily generate weighted
posterior predictive samples. The weights can be taken from anywhere,
but using the weights computed with <code class="docutils literal notranslate"><span class="pre">az.compare(.,</span> <span class="pre">method=&quot;stacking&quot;)</span></code>
makes a lot of sense.</p>
</section>
</section>
<section id="exercises">
<span id="exercises2"></span><h2><span class="section-number">2.6. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>2E1.</strong> Using your own words, what are the main
differences between prior predictive checks and posterior predictive
checks? How are these empirical evaluations related to Equations
<a class="reference internal" href="chp_01.html#equation-eq-prior-pred-dist">(1.7)</a> and
<a class="reference internal" href="chp_01.html#equation-eq-post-pred-dist">(1.8)</a>.</p>
<p><strong>2E2.</strong> Using your own words explain: ESS, <span class="math notranslate nohighlight">\(\hat R\)</span> and
MCSE. Focus your explanation on what these quantities are measuring and
what potential issue with MCMC they are identifying.</p>
<p><strong>2E3.</strong> ArviZ includes precomputed InferenceData objects
for a few models. We are going to load an InferenceData object generated
from a classical example in Bayesian statistic, the eight schools model
<span id="id51">[<a class="reference internal" href="references.html#id115" title="Donald B Rubin. Estimation in parallel randomized experiments. Journal of Educational Statistics, 6(4):377–401, 1981.">27</a>]</span>. The InferenceData object includes prior samples, prior
predictive samples and posterior samples. We can load the InferenceData
object using the command <code class="docutils literal notranslate"><span class="pre">az.load_arviz_data(&quot;centered_eight&quot;)</span></code>. Use
ArviZ to:</p>
<ol class="simple">
<li><p>List all the groups available on the InferenceData object.</p></li>
<li><p>Identify the number of chains and the total number of posterior
samples.</p></li>
<li><p>Plot the posterior.</p></li>
<li><p>Plot the posterior predictive distribution.</p></li>
<li><p>Calculate the estimated mean of the parameters, and the Highest
Density Intervals.</p></li>
</ol>
<p>If necessary check the ArviZ documentation to help you do these tasks
<a class="reference external" href="https://arviz-devs.github.io/arviz/">https://arviz-devs.github.io/arviz/</a></p>
<p><strong>2E4.</strong> Load <code class="docutils literal notranslate"><span class="pre">az.load_arviz_data(&quot;non_centered_eight&quot;)</span></code>,
which is a reparametrized version of the “centered_eight” model in the
previous exercise. Use ArviZ to assess the MCMC sampling convergence for
both models by using:</p>
<ol class="simple">
<li><p>Autocorrelation plots</p></li>
<li><p>Rank plots.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat R\)</span> values.</p></li>
</ol>
<p>Focus on the plots for the mu and tau parameters. What do these three
different diagnostics show? Compare these to the InferenceData results
loaded from <code class="docutils literal notranslate"><span class="pre">az.load_arviz_data(&quot;centered_eight&quot;)</span></code>. Do all three
diagnostics tend to agree on which model is preferred? Which one of the
models has better convergence diagnostics?</p>
<p><strong>2E5.</strong> InferenceData object can store statistics related
to the sampling algorithm. You will find them in the <code class="docutils literal notranslate"><span class="pre">sample_stats</span></code>
group, including divergences (<code class="docutils literal notranslate"><span class="pre">diverging</span></code>):</p>
<ol class="simple">
<li><p>Count the number of divergences for “centered_eight” and
“non_centered_eight” models.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">az.plot_parallel</span></code> to identify where the divergences tend to
concentrate in the parameter space.</p></li>
</ol>
<p><strong>2E6.</strong> In the GitHub repository we have included an
InferenceData object with a Poisson model and one with a
NegativeBinomial, both models are fitted to the same dataset. Use
<code class="docutils literal notranslate"><span class="pre">az.load_arviz_data(.)</span></code> to load them, and then use ArviZ functions to
answer the following questions:</p>
<ol class="simple">
<li><p>Which model provides a better fit to the data? Use the functions
<code class="docutils literal notranslate"><span class="pre">az.compare(.)</span></code> and <code class="docutils literal notranslate"><span class="pre">az.plot_compare(.)</span></code></p></li>
<li><p>Explain why one model provides a better fit than the other. Use
<code class="docutils literal notranslate"><span class="pre">az.plot_ppc(.)</span></code> and <code class="docutils literal notranslate"><span class="pre">az.plot_loo_pit(.)</span></code></p></li>
<li><p>Compare both models in terms of their pointwise ELPD values.
Identify the 5 observations with the largest (absolute) difference.
Which model is predicting them better? For which model p_loo is
closer to the actual number of parameters? Could you explain why?
Hint: the Poisson model has a single parameter that controls both
the variance and mean. Instead, the NegativeBinomial has two
parameters.</p></li>
<li><p>Diagnose LOO using the <span class="math notranslate nohighlight">\(\hat \kappa\)</span> values. Is there any reason to
be concerned about the accuracy of LOO for this particular case?</p></li>
</ol>
<p><strong>2E7.</strong> Reproduce
<a class="reference internal" href="#fig-posterior-predictive-many-examples"><span class="std std-numref">Fig. 2.7</span></a>, but using
<code class="docutils literal notranslate"><span class="pre">az.plot_loo_pit(ecdf=True)</span></code> in place of <code class="docutils literal notranslate"><span class="pre">az.plot_bpv(.)</span></code>. Interpret the
results. Hint: when using the option <code class="docutils literal notranslate"><span class="pre">ecdf=True</span></code>, instead of the LOO-PIT
KDE you will get a plot of the difference between the LOO-PIT Empirical
Cumulative Distribution Function (ECDF) and the Uniform CDF. The ideal
plot will be one with a difference of zero.</p>
<p><strong>2E8.</strong> In your own words explain why MCMC posterior
estimation techniques need convergence diagnostics. In particular
contrast these to the conjugate methods described in Section
<a class="reference internal" href="chp_01.html#conjugate-priors"><span class="std std-ref">Conjugate Priors</span></a> which do not need those
diagnostics. What is different about the two inference methods?</p>
<p><strong>2E9.</strong> Visit the ArviZ plot gallery at
<a class="reference external" href="https://arviz-devs.github.io/arviz/examples/index.html">https://arviz-devs.github.io/arviz/examples/index.html</a>. What diagnoses
can you find there that are not covered in this chapter? From the
documentation what is this diagnostic assessing?</p>
<p><strong>2E10.</strong> List some plots and numerical quantities that
are useful at each step during the Bayesian workflow (shown visually in
<a class="reference internal" href="chp_09.html#fig-bayesianworkflow"><span class="std std-numref">Fig. 9.1</span></a>). Explain how they work
and what they are assessing. Feel free to use anything you have seen in
this chapter or in the ArviZ documentation.</p>
<ol class="simple">
<li><p>Prior selection.</p></li>
<li><p>MCMC sampling.</p></li>
<li><p>Posterior predictions.</p></li>
</ol>
<p><strong>2M11.</strong> We want to model a football league with <span class="math notranslate nohighlight">\(N\)</span>
teams. As usual, we start with a simpler version of the model in mind,
just a single team. We assume the scores are Poisson distributed
according to a scoring rate <span class="math notranslate nohighlight">\(\mu\)</span>. We choose the prior
<span class="math notranslate nohighlight">\(\text{Gamma}(0.5, 0.00001)\)</span> because this is sometimes recommend as an
“objective” prior.</p>
<div class="literal-block-wrapper docutils container" id="poisson-football">
<div class="code-block-caption"><span class="caption-number">Listing 2.5 </span><span class="caption-text">poisson_football</span><a class="headerlink" href="#poisson-football" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Gamma</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.00001</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Poisson</span><span class="p">(</span><span class="s2">&quot;score&quot;</span><span class="p">,</span> <span class="n">μ</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_prior_predictive</span><span class="p">()</span>
</pre></div>
</div>
</div>
<ol>
<li><p>Generate and plot the prior predictive distribution. How reasonable
it looks to you?</p></li>
<li><p>Use your knowledge of sports in order to refine the prior choice.</p></li>
<li><p>Instead of soccer you now want to model basketball. Could you come
with a reasonable prior for that instance? Define the prior in a
model and generate a prior predictive distribution to validate your
intuition.</p>
<p>Hint: You can parameterize the Gamma distribution using the rate and
shape parameters as in Code Block
<a class="reference internal" href="#poisson-football"><span class="std std-ref">poisson_football</span></a> or alternatively
using the mean and standard deviation.</p>
</li>
</ol>
<p><strong>2M12.</strong> In Code Block <a class="reference internal" href="chp_01.html#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> from
Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a>, change the value of <code class="docutils literal notranslate"><span class="pre">can_sd</span></code> and run the
Metropolis sampler. Try values like 0.2 and 1.</p>
<ol class="simple">
<li><p>Use ArviZ to compare the sampled values using diagnostics such as
the autocorrelation plot, trace plot and the ESS. Explain the
observed differences.</p></li>
<li><p>Modify Code Block <a class="reference internal" href="chp_01.html#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> so you get more than one
independent chain. Use ArviZ to compute rank plots and <span class="math notranslate nohighlight">\(\hat R\)</span>.</p></li>
</ol>
<p><strong>2M13.</strong> Generate a random sample using
<code class="docutils literal notranslate"><span class="pre">np.random.binomial(n=1,</span> <span class="pre">p=0.5,</span> <span class="pre">size=200)</span></code> and fit it using a
Beta-Binomial model.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">pm.sample(.,</span> <span class="pre">step=pm.Metropolis())</span></code> (Metropolis-Hastings sampler)
and <code class="docutils literal notranslate"><span class="pre">pm.sample(.)</span></code> (the standard sampler). Compare the results in terms
of the ESS, <span class="math notranslate nohighlight">\(\hat R\)</span>, autocorrelation, trace plots and rank plots.
Reading the PyMC3 logging statements what sampler is autoassigned? What
is your conclusion about this sampler performance compared to
Metropolis-Hastings?</p>
<p><strong>2M14.</strong> Generate your own example of a synthetic
posterior with convergence issues, let us call it <code class="docutils literal notranslate"><span class="pre">bad_chains3</span></code>.</p>
<ol class="simple">
<li><p>Explain why the synthetic posterior you generated is “bad”. What
about it would we not want to see in an actual modeling scenario?</p></li>
<li><p>Run the same diagnostics we run in the book for <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> and
<code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code>. Compare your results with those in the book and
explain the differences and similarities.</p></li>
<li><p>Did the results of the diagnostics from the previous point made you
reconsider why <code class="docutils literal notranslate"><span class="pre">bad_chains3</span></code> is a “bad chain”?</p></li>
</ol>
<p><strong>2H15.</strong> Generate a random sample using
<code class="docutils literal notranslate"><span class="pre">np.random.binomial(n=1,</span> <span class="pre">p=0.5,</span> <span class="pre">size=200)</span></code> and fit it using a
Beta-Binomial model.</p>
<ol class="simple">
<li><p>Check that LOO-PIT is approximately Uniform.</p></li>
<li><p>Tweak the prior to make the model a bad fit and get a LOO-PIT that
is low for values closer to zero and high for values closer to one.
Justify your prior choice.</p></li>
<li><p>Tweak the prior to make the model a bad fit and get a LOO-PIT that
is high for values closer to zero and low for values closer to one.
Justify your prior choice.</p></li>
<li><p>Tweak the prior to make the model a bad fit and get a LOO-PIT that
is high for values close to 0.5 and low for values closer to zero
and one. Could you do it? Explain why.</p></li>
</ol>
<p><strong>2H16.</strong> Use PyMC3 to write a model with Normal
likelihood. Use the following random samples as data and the following
priors for the mean. Fix the standard deviation parameter in the
likelihood at 1.</p>
<ol class="simple">
<li><p>A random sample of size 200 from a <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> and prior
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(0,20)\)</span></p></li>
<li><p>A random sample of size 2 from a <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> and prior
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(0,20)\)</span></p></li>
<li><p>A random sample of size 200 from a <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> and prior
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(20, 1)\)</span></p></li>
<li><p>A random sample of size 200 from a <span class="math notranslate nohighlight">\(\mathcal{U}(0,1)\)</span> and prior
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(10, 20)\)</span></p></li>
<li><p>A random sample of size 200 from a <span class="math notranslate nohighlight">\(\mathcal{HN}(0,1)\)</span> and a prior
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(10,20)\)</span></p></li>
</ol>
<p>Assess convergence by running the same diagnostics we run in the book
for <code class="docutils literal notranslate"><span class="pre">bad_chains0</span></code> and <code class="docutils literal notranslate"><span class="pre">bad_chains1</span></code>. Compare your results with those in
the book and explain the differences and similarities.</p>
<p><strong>2H17.</strong> Each of the four sections in this chapter, prior
predictive checks, posterior predictive checks, numerical inference
diagnostics, and model comparison, detail a specific step in the
Bayesian workflow. In your own words explain what the purpose of each
step is, and conversely what is lacking if the step is omitted. What
does each tell us about our statistical models?</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id52"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego">https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego</a></p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>We are omitting tasks related to obtaining the data in the first
place, but experimental design can be as critical if not more than
other aspects in the statistical analysis, see Chapter
<a class="reference internal" href="chp_10.html#chap10"><span class="std std-ref">9</span></a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id7">3</a></span></dt>
<dd><p><a class="reference external" href="https://arviz-devs.github.io/arviz/">https://arviz-devs.github.io/arviz/</a></p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id9">4</a></span></dt>
<dd><p>This example has been adapted from
<a class="reference external" href="https://mc-stan.org/users/documentation/case-studies/golf.html">https://mc-stan.org/users/documentation/case-studies/golf.html</a> and
<a class="reference external" href="https://docs.pymc.io/notebooks/putting_workflow.html">https://docs.pymc.io/notebooks/putting_workflow.html</a></p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id10">5</a></span></dt>
<dd><p>The example has been adapted from <span id="id57">[<a class="reference internal" href="references.html#id78" title="Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. Bayesian workflow. arXiv preprint arXiv:2011.01808, 2020.">17</a>]</span>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id11">6</a></span></dt>
<dd><p>See Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a> for details of the logistic
regression model.</p>
</dd>
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id13">7</a></span></dt>
<dd><p>Posterior predictive checks are a very general idea. These figures
do not try to show the only available choices, just some of the
options offered by ArviZ.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id14">8</a></span></dt>
<dd><p>Unless you realize you need to collect data again, but that is
another story.</p>
</dd>
<dt class="label" id="id61"><span class="brackets"><a class="fn-backref" href="#id15">9</a></span></dt>
<dd><p>Try <a class="reference external" href="https://www.timeanddate.com/sun/ecuador/quito">https://www.timeanddate.com/sun/ecuador/quito</a></p>
</dd>
<dt class="label" id="id62"><span class="brackets"><a class="fn-backref" href="#id23">10</a></span></dt>
<dd><p>Do not confuse with the standard deviation of the MCSE for the
mean.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id28">11</a></span></dt>
<dd><p>Most useful and commonly used sampling methods for Bayesian
inference are variants of HMC, including for example, the default
method for continuous variables in PyMC3. For more details of this
method, see Section <a class="reference internal" href="chp_11.html#hmc"><span class="std std-ref">Hamiltonian Monte Carlo</span></a>).</p>
</dd>
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id30">12</a></span></dt>
<dd><p>A function which is zero everywhere and infinite at zero.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id32">13</a></span></dt>
<dd><p>For a sampler like Sequential Monte Carlo, increasing the number
of draws also increases the number of particles, and thus it could
actually provide better convergence. See Section <a class="reference internal" href="chp_11.html#smc-details"><span class="std std-ref">Sequential Monte Carlo</span></a>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id34">14</a></span></dt>
<dd><p>Strictly speaking we should use probabilities for discrete
models, but that distinction rapidly becomes annoying in practice.</p>
</dd>
<dt class="label" id="id67"><span class="brackets"><a class="fn-backref" href="#id35">15</a></span></dt>
<dd><p>In non-Bayesians contexts <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is a point
estimate obtained, for example, by maximizing the likelihood.</p>
</dd>
<dt class="label" id="id68"><span class="brackets"><a class="fn-backref" href="#id36">16</a></span></dt>
<dd><p>We are also computing samples from the posterior predictive
distribution to use them to compute LOO-PIT.</p>
</dd>
<dt class="label" id="id69"><span class="brackets"><a class="fn-backref" href="#id40">17</a></span></dt>
<dd><p>At time of writing this book the method has not been yet
implemented in ArviZ, but it may be already available by the time
you are reading this.</p>
</dd>
<dt class="label" id="id70"><span class="brackets"><a class="fn-backref" href="#id43">18</a></span></dt>
<dd><p>See the case study
<a class="reference external" href="https://avehtari.github.io/modelselection/roaches.html">https://avehtari.github.io/modelselection/roaches.html</a> for an
example.</p>
</dd>
<dt class="label" id="id71"><span class="brackets"><a class="fn-backref" href="#id45">19</a></span></dt>
<dd><p>A deeper give into Probability Integral Transform can be found in Section
<a class="reference internal" href="chp_11.html#probability-integral-transform-pit"><span class="std std-ref">Probability Integral Transform (PIT)</span></a>.</p>
</dd>
<dt class="label" id="id72"><span class="brackets"><a class="fn-backref" href="#id48">20</a></span></dt>
<dd><p>The Akaike information criterion (AIC) is an estimator of the
generalization error, it is commonly used in frequentists
statistics, but their assumptions are generally not adequate enough
for general use with Bayesians models.</p>
</dd>
<dt class="label" id="id73"><span class="brackets"><a class="fn-backref" href="#id49">21</a></span></dt>
<dd><p>This formula also works for WAIC [^22] and other information
criteria</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_01.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1. </span>Bayesian Inference</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="chp_03.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Linear Models and Probabilistic Programming Languages</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Martin, Kumar, Lao<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>