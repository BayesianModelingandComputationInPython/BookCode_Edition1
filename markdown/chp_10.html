
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10. Probabilistic Programming Languages &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Appendiceal Topics" href="chp_11.html" />
    <link rel="prev" title="9. End to End Bayesian Workflows" href="chp_09.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_10.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-systems-engineering-perspective-of-a-ppl">
   10.1. A Systems Engineering Perspective of a PPL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-rainier">
     10.1.1. Example: Rainier
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#posterior-computation">
   10.2. Posterior Computation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#getting-the-gradient">
     10.2.1. Getting the Gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-near-real-time-inference">
     10.2.2. Example: Near Real Time Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#application-programming-interfaces">
   10.3. Application Programming Interfaces
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-stan-and-slicstan">
     10.3.1. Example: Stan and Slicstan
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-pymc3-and-pymc4">
     10.3.2. Example: PyMC3 and PyMC4
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ppl-driven-transformations">
   10.4. PPL Driven Transformations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-probabilities">
     10.4.1. Log Probabilities
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-variables-and-distributions-transformations">
     10.4.2. Random Variables and Distributions Transformations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-sampling-comparison-between-bounded-and-unbounded-random-variables">
     10.4.3. Example: Sampling Comparison between Bounded and Unbounded Random Variables
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#operation-graphs-and-automatic-reparameterization">
   10.5. Operation Graphs and Automatic Reparameterization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#effect-handling">
   10.6. Effect handling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-effect-handling-in-tfp-and-numpyro">
     10.6.1. Example: Effect Handling in TFP and Numpyro
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#base-language-code-ecosystem-modularity-and-everything-else">
   10.7. Base Language, Code Ecosystem, Modularity and Everything Else
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#designing-a-ppl">
   10.8. Designing a PPL
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#shape-handling-in-ppls">
     10.8.1. Shape Handling in PPLs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#takeaways-for-the-applied-bayesian-practitioner">
   10.9. Takeaways for the Applied Bayesian Practitioner
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   10.10. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="probabilistic-programming-languages">
<span id="chap10"></span><h1><span class="section-number">10. </span>Probabilistic Programming Languages<a class="headerlink" href="#probabilistic-programming-languages" title="Permalink to this headline">¶</a></h1>
<p>In Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a> Section <a class="reference internal" href="chp_01.html#bayesian-modeling"><span class="std std-ref">Bayesian Modeling</span></a>, we used cars as analogy to
understand applied Bayesian concepts. We will revisit this analogy but
this time to understand Probabilistic Programming Languages. If we treat
a car as a system, their purpose is to move people or cargo to a chosen
destination with wheels connected to a power source. This whole system
is presented to users with an interface, typically a steering wheel and
pedals. Like all physical objects cars have to obey the laws of physics,
but within those bounds human designers have a multitude of components
to choose from. Cars can have big engines, or small tires, 1 seat or 8
seats. The end result however, is informed by the specific purpose. Some
cars are designed to go fast with a single person around a racetrack,
such as Formula 1 cars. Others are designed for family life like
carrying families and groceries home from the store. No matter what the
purpose, someone needs to pick the components, for the right car, for
the right purpose.</p>
<p>The story of Probabilistic Programming Languages (PPL) is similar. The
purpose of PPLs is help Bayesian practitioners build generative models
to solve problem at hand, for example, perform inference of a Bayesian
model through estimating the posterior distribution with MCMC. The power
source for computational Bayesians is, well, a computer which are bound
by computer science fundamentals. Within those bounds however, PPL
designers can choose different components and interfaces, with the
specifics being determined by anticipated user need and preference. In
this chapter we will focus our discussion on what the components of PPLs
are, and different design choices that can be made within those
components. This knowledge will help you as Bayesian practitioner when
you need to pick a PPL when starting a project or debug that arises
during your statistical workflow. This understanding ultimately will
lead to better experience for you, the modern Bayesian practitioner.</p>
<div class="section" id="a-systems-engineering-perspective-of-a-ppl">
<span id="id1"></span><h2><span class="section-number">10.1. </span>A Systems Engineering Perspective of a PPL<a class="headerlink" href="#a-systems-engineering-perspective-of-a-ppl" title="Permalink to this headline">¶</a></h2>
<p>Wikipedia defines systems engineering as “an interdisciplinary field of
engineering and engineering management that focuses on how to design,
integrate, and manage complex systems over their life cycles”. PPLs are
by this definition a complex system. PPLs span across computational
backends, algorithms, and base languages. As the definition also states
the integration of components is a key part of systems engineering,
which is true of PPLs as well. A choice of computational backend can
have an effect on the interface, or the choice of base language can
limit the available inference algorithms. In some PPLs the user can make
component selections themselves. For example, Stan users can choose
their base language between R, Python, or the command line interface
among others, whereas PyMC3 users cannot change base languages, they
must use Python.</p>
<p>In addition to the PPL itself there is the consideration of the
organization and manner in which it will be used. A PhD student using a
PPL in a research lab has different needs than an engineer working in a
corporation. This is also relevant to the lifecycle of a PPL.
Researchers may only need a model once or twice in a short period to
write a paper, whereas an engineer in a corporation may maintain and run
the model over the course of years.</p>
<p>In a PPL the two necessary components are: an application programming
interface for the user to define the model <a class="footnote-reference brackets" href="#id64" id="id2">1</a>, and algorithms to
performs inference and manage the computation. Other components exist
but largely to improve the system in some fashion, such as computational
speed or ease of use. Regardless of the choice components, when a system
is designed well the day to day user need not be aware of the
complexity, just as most drivers are able to use cars without
understanding the details of each part. In the ideal case a PPL the user
should just feel as though things work just the way they want them. This
is the challenge PPL designers must meet.</p>
<p>In the remaining of the chapter, we will give some overview of some
general components of a PPL, with examples of design choice from
different PPLs. We are not aiming to provide an exhaustive descriptions
of all PPLs <a class="footnote-reference brackets" href="#id65" id="id3">2</a>, and we are also not trying to convince you to develop
a PPL <a class="footnote-reference brackets" href="#id66" id="id4">3</a>. Rather, by understanding the implementation consideration,
we hope that you will gain a better understanding of how to write more
performative Bayesian models, and to diagnose computation bottlenecks
and errors when they occur.</p>
<div class="section" id="example-rainier">
<span id="id5"></span><h3><span class="section-number">10.1.1. </span>Example: Rainier<a class="headerlink" href="#example-rainier" title="Permalink to this headline">¶</a></h3>
<p>Consider the development of Rainier <a class="footnote-reference brackets" href="#id68" id="id6">4</a>, a PPL written in Scala
developed at Stripe. Stripe is a payments processing company that
handles finances for many thousands of partner business. In Stripe, they
need to estimate the distribution of risk associated with each partnered
business, ideally with a PPL that is able to support many parallel
inferences (one per each business partner) and easy to deploy in
Stripe’s compute cluster. As Stripe’s compute clusters included a Java
run time environment, they choose Scala as it can be compiled to Java
bytecode. Rainier. In this case PyMC3 and Stan were considered as well,
but due to either the restriction of Python use (PyMC3), or the
requirement for a C++ compiler (Stan), creating a PPL for their
particular use case was the best choice.</p>
<p>Most users will not need to develop their own PPL but we present this
case study to highlight how considerations of both the environment in
which you are using the code, and the functionality of the available
PPLs can help inform a decision for a smoother experience as a
computational Bayesian.</p>
</div>
</div>
<div class="section" id="posterior-computation">
<span id="id7"></span><h2><span class="section-number">10.2. </span>Posterior Computation<a class="headerlink" href="#posterior-computation" title="Permalink to this headline">¶</a></h2>
<p>Inference is defined as a conclusion reached on the basis of evidence
and reasoning and the posterior computational methodology is the engine
that gets us to the conclusion. The posterior computation method can
largely be thought of as two parts, the computation algorithm, and the
software and hardware that makes the calculation, often referred to as
the computational backend. When either designing or selecting a PPL, the
available posterior computation methods ends up being a key decision
that informs many factors of the workflow, from the speed of inference,
hardware needed, complexity of PPL, and breadth of applicability. There
are numerous algorithms to compute the posterior <a class="footnote-reference brackets" href="#id69" id="id8">5</a>, from exact
computations when using conjugate models, to numerical approximations
like grid search to Hamiltonian Monte Carlo (HMC), to model
approximation like Laplace approximation and variational inference
(covered in more detail in Section <a class="reference internal" href="chp_11.html#vi-details"><span class="std std-ref">Variational Inference</span></a>). When selecting
an inference algorithms both the PPL designer and the user need to make
a series of choices. For the PPL designer the algorithms have different
levels of implementation complexity. For example, conjugate methods are
quite easy to implement, as there exists analytical formulas that can be
written in a couple lines of code, whereas MCMC samplers are more
complex, typically requiring a PPL designer to write much more code than
an analytical solution. Another tradeoff in computational complexity,
conjugate methods do not require much computation power and can return a
posterior in sub millisecond on all modern hardware, even a cell phone.
By comparison, HMC is slow and require a system that can compute
gradients, such as the one we will present in Section <a class="reference internal" href="#auto-grad"><span class="std std-ref">Getting the Gradient</span></a>. This
limits HMC computation to relatively powerful computers, sometimes with
specialized hardware.</p>
<p>The user faces a similar dilemma, more advanced posterior computation
methods are more general and require less mathematical expertise, but
require more knowledge to assess and ensure correct fit. We have seen
this throughout this book, where visual and numerical diagnostics are
necessary to ensure our MCMC samplers have converged to an <em>estimate</em> of
the posterior. Conjugate models do not need any convergence diagnostics
due to the fact they calculate the posterior <em>exactly</em>, every time if
the right mathematics are used.</p>
<p>For all these reasons there is no universal recommendation for an
inference algorithm that suits every situation. At time of writing MCMC
methods, especially adaptive Dynamic Hamiltonian Monte Carlo, are the
most flexible, but not useful in all situations. As a user it is
worthwhile understanding the availability and tradeoffs of each
algorithm to be able to make an assessment for each situation.</p>
<div class="section" id="getting-the-gradient">
<span id="auto-grad"></span><h3><span class="section-number">10.2.1. </span>Getting the Gradient<a class="headerlink" href="#getting-the-gradient" title="Permalink to this headline">¶</a></h3>
<p>An incredibly useful piece of information in computational mathematics
is the gradient. Also known as the slope, or the derivative for one
dimensional functions, it indicates how fast a function output value is
changing at any point in its domain. By utilizing the gradient many
algorithms are developed to more efficiently achieve their goal. With
inference algorithms we have seen this difference when comparing the
Metropolis Hasting algorithm, which does not need a gradient when
sampling, to Hamiltonian Monte Carlo, which does use the gradient and
usually returns high quality samples faster <a class="footnote-reference brackets" href="#id70" id="id9">6</a>.</p>
<p>Just as Markov chain Monte Carlo was originally developed in the sub
field of statistical mechanics before computational Bayesians adopted
it, most of the gradient evaluation libraries were originally developed
as part of “Deep Learning” libraries mostly intended for
backpropagation computation to train Neural Networks. These include
Theano, TensorFlow and PyTorch. Bayesians however, learned to use them
as computational backends for Bayesian Inference. An example of
computational gradient evaluation using JAX <span id="id10">[<a class="reference internal" href="references.html#id155">116</a>]</span>, a
dedicated autograd library, shown in Code Block
<a class="reference internal" href="#jax-grad-small"><span class="std std-ref">jax_grad_small</span></a>. In this Code Block the
gradient of <span class="math notranslate nohighlight">\(x^2\)</span> is computed at a value of <span class="math notranslate nohighlight">\(4\)</span>. We can solve this
analytically with the rule <span class="math notranslate nohighlight">\(rx^{r-1}\)</span>, and we can then calculate
<span class="math notranslate nohighlight">\(2*4=8\)</span>. However, with autograd libraries users do not need to think
about closed form solutions. All that is needed is an expression of the
function itself and the computer can automatically calculate the
gradient, as implied by “auto” in autograd.</p>
<div class="literal-block-wrapper docutils container" id="jax-grad-small">
<div class="code-block-caption"><span class="caption-number">Listing 10.1 </span><span class="caption-text">jax_grad_small</span><a class="headerlink" href="#jax-grad-small" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>

<span class="n">simple_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">simple_grad</span><span class="p">(</span><span class="mf">4.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>8.0
</pre></div>
</div>
<p>Methods such us Adaptive Dynamic Hamiltonian Monte Carlo or Variational
Inference use gradients to estimate posterior distributions. Being able
to obtain gradient easily becomes even more important when we realize
that in posterior computation the gradient typically gets computed
thousands of times. We show one such calculation in Code Block
<a class="reference internal" href="#jax-model-grad"><span class="std std-ref">jax_model_grad</span></a> using JAX for a small
“hand built” model.</p>
<div class="literal-block-wrapper docutils container" id="jax-model-grad">
<div class="code-block-caption"><span class="caption-number">Listing 10.2 </span><span class="caption-text">jax_model_grad</span><a class="headerlink" href="#jax-model-grad" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">jax.scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">test_point</span><span class="p">,</span> <span class="n">observed</span><span class="p">):</span>
    <span class="n">z_pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">test_point</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">x_pdf</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">test_point</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">logpdf</span> <span class="o">=</span> <span class="n">z_pdf</span> <span class="o">+</span> <span class="n">x_pdf</span>
    <span class="k">return</span> <span class="n">logpdf</span>

<span class="n">model_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">observed</span><span class="p">,</span> <span class="n">test_point</span> <span class="o">=</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.5</span> 
<span class="n">logp_val</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_point</span><span class="p">,</span> <span class="n">observed</span><span class="p">)</span>
<span class="n">grad</span> <span class="o">=</span> <span class="n">model_grad</span><span class="p">(</span><span class="n">test_point</span><span class="p">,</span> <span class="n">observed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;log_p_val: </span><span class="si">{</span><span class="n">logp_val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;grad: </span><span class="si">{</span><span class="n">grad</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>log_p_val: -6.697315216064453
grad: 2.4000000953674316
</pre></div>
</div>
<p>For comparison we can make the same calculation using a PyMC3 model and
computing the gradient using Theano in Code Block
<a class="reference internal" href="#pymc3-model-grad"><span class="std std-ref">pymc3_model_grad</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="pymc3-model-grad">
<div class="code-block-caption"><span class="caption-number">Listing 10.3 </span><span class="caption-text">pymc3_model_grad</span><a class="headerlink" href="#pymc3-model-grad" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>

<span class="n">func</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">logp_dlogp_function</span><span class="p">()</span>
<span class="n">func</span><span class="o">.</span><span class="n">set_extra_values</span><span class="p">({})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">test_point</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[array(-6.69731498), array([2.4])]
</pre></div>
</div>
<p>From the output we can see the PyMC3 model returns the same logp and
gradient as the JAX model.</p>
</div>
<div class="section" id="example-near-real-time-inference">
<span id="conjugate-case-study"></span><h3><span class="section-number">10.2.2. </span>Example: Near Real Time Inference<a class="headerlink" href="#example-near-real-time-inference" title="Permalink to this headline">¶</a></h3>
<p>As a hypothetical example consider a statistician at a credit card
company that is concerned with detecting credit card fraud quickly so it
can disable cards before the thief can make more transactions. A
secondary system classifies transactions as fraudulent or legitimate but
the company wants to ensure it does not block cards with a low number of
events and wants to be able to set priors for different customers to
control the sensitivity. It is decided that the users accounts will be
disabled when mean of the posterior distribution is above a probability
threshold of 50%. In this near real time scenario inference needs to be
performed in less than a second so fraudulent activity can be detected
before the transaction clears. The statistician recognizes that this can
be analytically expressed using a conjugate model which she then writes
in Equation <a class="reference internal" href="#equation-eq-conjugate-beta-fraud">(10.1)</a>, where the <span class="math notranslate nohighlight">\(\alpha\)</span> and
<span class="math notranslate nohighlight">\(\beta\)</span> parameters, representing the prior of fraud and non-fraud
transactions directly. As transactions are observed they are used fairly
directly compute the posterior parameters.</p>
<div class="math notranslate nohighlight" id="equation-eq-conjugate-beta-fraud">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-eq-conjugate-beta-fraud" title="Permalink to this equation">¶</a></span>\[ \begin{align}\begin{aligned}\begin{split}\begin{split}
    \alpha_{post} &amp;= \alpha_{prior} + fraud\_observations \\
    \beta_{post} &amp;=  \beta_{prior} + non\_fraud\_observations \\
   % p(\theta)  &amp;= Beta(fraud\_prior, non\_fraud\_prior) \\
    p(\theta \mid y)  &amp;= Beta(\alpha_{post}, \beta_{post}) \\
    \mathop{\mathbb{E}}[p(\theta \mid y)] &amp;= \frac{\alpha_{post}}{\alpha_{post} + \beta_{post}}\end{split}\\\end{split}\end{aligned}\end{align} \]</div>
<p>She can then fairly trivially express these calculations in Python as
shown in Code Block <a class="reference internal" href="#fraud-detector"><span class="std std-ref">fraud_detector</span></a>. No
external libraries needed either, making this function quite easy to
deploy.</p>
<div class="literal-block-wrapper docutils container" id="fraud-detector">
<div class="code-block-caption"><span class="caption-number">Listing 10.4 </span><span class="caption-text">fraud_detector</span><a class="headerlink" href="#fraud-detector" title="Permalink to this code">¶</a></div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fraud_detector</span><span class="p">(</span><span class="n">obs_fraud</span><span class="p">,</span> <span class="n">obs_non_fraud</span><span class="p">,</span> <span class="n">fraud_prior</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">non_fraud_prior</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Conjugate beta binomial model for fraud detection&quot;&quot;&quot;</span>
    <span class="n">expectation</span> <span class="o">=</span> <span class="p">(</span><span class="n">fraud_prior</span><span class="o">+</span><span class="n">observed_fraud</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
        <span class="n">fraud_prior</span><span class="o">+</span><span class="n">observed_fraud</span><span class="o">+</span><span class="n">non_fraud_prior</span><span class="o">+</span><span class="n">obs_non_fraud</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">expectation</span> <span class="o">&gt;</span> <span class="mf">.5</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;suspend_card&quot;</span><span class="p">:</span><span class="kc">True</span><span class="p">}</span>

<span class="o">%</span><span class="k">timeit</span> fraud_detector(2, 0)
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>152 ns ± 0.969 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)
</pre></div>
</div>
<p>To meet the sensitivity and probability computation time requirements of
less than one second a conjugate prior is selected, and the model
posterior is calculated in Code Block
<a class="reference internal" href="#fraud-detector"><span class="std std-ref">fraud_detector</span></a>. The calculations took
about 152 ns, in contrast an MCMC sampler will take around 2 seconds on
the same machine which is over 6 orders of magnitude. It is unlikely
that any MCMC sampler would meet the time requirement needed from this
system, making a conjugate prior the clear choice.</p>
<div class="admonition-hardware-and-sampling-speed admonition">
<p class="admonition-title">Hardware and Sampling Speed</p>
<p>From a hardware perspective, there are three
ways to increase MCMC sampling speed. The first is typically the clock
speed of the processing unit, often measured in hertz, or Gigahertz for
modern computers. This is the speed at which the instructions are
executed, so generally speaking a 4 Gigahertz computer can execute twice
as many instructions in a second than a 2 Gigahertz computer. In MCMC
sampling the clock speed will correlate with the number of samples that
can be taken in a timespan for a single chain. The other is
parallelization across multiple cores in a processing unit. In MCMC
sampling each chains can be sampled in parallel on computers with
multiple cores. This coincidentally is convenient as many convergence
metrics use multiple chains. On a modern desktop computer anywhere from
2 to 16 cores are typically available. The last method is specialized
hardware such as the Graphics Processing Units (GPUs) and Tensor
Processing Units (TPUs). If paired with the correct software and
algorithms these are able to both sample each chain more quickly, but
also sample more chains in parallel.</p>
</div>
</div>
</div>
<div class="section" id="application-programming-interfaces">
<span id="id11"></span><h2><span class="section-number">10.3. </span>Application Programming Interfaces<a class="headerlink" href="#application-programming-interfaces" title="Permalink to this headline">¶</a></h2>
<p>Application Programming Interfaces (API) “define interactions between
multiple software intermediaries”. In the Bayesian case the most narrow
definition is the interactions between the user and the method to
compute the posterior. At its most broad it can include multiple steps
in the Bayesian workflow, such as specifying a random variable with a
distribution, linking different random variables to create the model,
prior and posterior predictive checks, plotting, or any task. The API is
typically first part, and sometimes the only part, a PPL practitioner
interactions with and typically is where the practitioner spends the
most amount of time. API design is both a science and an art and
designers must balance multiple concerns.</p>
<p>On the science side PPLs must be able to interface with a computer and
provide the necessary elements to control the computation methods. Many
PPLs are defined with base languages and typically need to follow the
fixed computational constraints of the base language as well as the
computational backend. In
Section <a class="reference internal" href="#conjugate-case-study"><span class="std std-ref">Example: Near Real Time Inference</span></a>, only a 4 parameters and one line of code
were needed to obtain an exact result. Contrast this with MCMC examples
that had various inputs such as number of draws, acceptance rate, number
of tuning steps and more. The complexity of MCMC, while mostly hidden,
still surfaced additional complexity in the API.</p>
<div class="admonition-so-many-apis-so-many-interfaces admonition">
<p class="admonition-title">So many APIs, so many interfaces</p>
<p>In a modern Bayesian workflow there is
not just the PPL API but APIs of all the supporting packages in the
workflow. In this book we have also used the Numpy, Matplotlib, Scipy,
Pandas and ArviZ APIs across examples, not to mention the Python API
itself. In the Python ecosystem. These choices of packages, and the APIs
they bring, are also subject to personal choice. A practitioner may
choose to use Bokeh as a replacement to Matplotlib for plotting, or
xarray in addition to pandas, and in doing so the user will need to
learn those APIs as well.</p>
<p>In addition to just APIs there are many code writing interfaces to write
Bayesian models, or just code in general. Code can be written in the
text editors, notebook, Integrated Development Environments (IDEs), or
the command line directly.</p>
<p>The use of these tools, both the supporting packages and the coding
interface, is not mutually exclusive. For someone new to computational
statistics this can be a lot to take in. When starting out we suggest
using a simple text editor and a few supporting packages to allow for
your focus to be on the code and model, before moving onto more complex
interfaces such as notebooks or Integrated Development Environments. We
provide more guidance regarding this topic in Section <a class="reference internal" href="chp_11.html#dev-environment"><span class="std std-ref">Text Editor vs Integrated Development Environment vs Notebook</span></a>.</p>
</div>
<p>On the art side API is the interface for human users. This interface is
one of the most important parts of the PPL. Some users tend to have
strong, albeit subjective views, about design choices. Users want the
simplest, most flexible, readable, and easy to write API, objectives
which, for the poor PPL designer, are both ill defined and opposed with
each other. One choice a PPL designer can make is to mirror style and
functionality of the base language. For example, there is a notion of
“Pythonic” programs which are follow in a certain style <a class="footnote-reference brackets" href="#id71" id="id12">7</a>. This
notion of pythonic API is what informs the PyMC3 API, the goal is to
explicitly have users feel like they are writing their models in Python.
In contrast Stan models are written in a domain specific language
informed by other PPLs such as BUGS <span id="id13">[<a class="reference internal" href="references.html#id113">117</a>]</span>
and languages such as C++ <span id="id14">[<a class="reference internal" href="references.html#id114">118</a>]</span>. The Stan language includes
notable API primitives such as curly braces and uses a block syntax as
shown in Code Block <a class="reference internal" href="#code-stan"><span class="std std-ref">code_stan</span></a>. Writing a Stan model
distinctly <em>does not</em> feel like writing Python, but this is not a knock
against the API. It is just a different choice from a design standpoint
and a different experience for the user.</p>
<div class="section" id="example-stan-and-slicstan">
<span id="id15"></span><h3><span class="section-number">10.3.1. </span>Example: Stan and Slicstan<a class="headerlink" href="#example-stan-and-slicstan" title="Permalink to this headline">¶</a></h3>
<p>Depending on the use case, user might prefer different level of
abstraction in terms model specification, independent of any other PPL
component. Stan and Slicstan using from Gorinova etal {cite:p}`Gorinova_2019]
which is specifically dedicated to studying and proposing Stan APIs. In
Code Block <a class="reference internal" href="#code-stan"><span class="std std-ref">code_stan</span></a> we show the, original, Stan model
syntax. In the Stan syntax various pieces of a Bayesian model are
indicated by blocks declarations. These names correspond with the
various sections of the workflow, such as specifying the model and
parameters, data transformations, prior and posterior predictive
sampling, with corresponding names such as parameters, model,
transformed parameters, and generated quantities.</p>
<div class="literal-block-wrapper docutils container" id="code-stan">
<div class="code-block-caption"><span class="caption-number">Listing 10.5 </span><span class="caption-text">code_stan</span><a class="headerlink" href="#code-stan" title="Permalink to this code">¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>parameters {
    real y_std;
    real x_std;
}
transformed parameters {
    real y = 3 * y_std;
    real x = exp(y/2) * x_std;
}
model {
    y_std ~ normal(0, 1);
    x_std ~ normal(0, 1);
}
</pre></div>
</div>
</div>
<p>An alternative syntax for Stan models is Slicstan <span id="id16">[<a class="reference internal" href="references.html#id87">119</a>]</span>, the
same model of which is shown in Code Block
<a class="reference internal" href="#slicstan"><span class="std std-ref">slicstan</span></a>. Slicstan provides a compositional
interface to Stan, letting users define functions which can be named and
reused, and does away with the block syntax. These features mean
Slicstan programs can be expressed in less code than standard Stan
models. While not always the most important metric less code means less
code that the Bayesian modeler needs to write, and less code that a
model reviewer needs to read. Also, like Python, composable functions
allow the user to define an idea once and reuse it many times, such as
<code class="docutils literal notranslate"><span class="pre">my_normal</span></code> in the Slicstan snippet.</p>
<div class="literal-block-wrapper docutils container" id="slicstan">
<div class="code-block-caption"><span class="caption-number">Listing 10.6 </span><span class="caption-text">slicstan</span><a class="headerlink" href="#slicstan" title="Permalink to this code">¶</a></div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>real my_normal(real m, real s) {
real std ~ normal(0, 1);
    return s * std + m;
}
real y = my_normal(0, 3);
real x = my_normal(0, exp(y/2));
</pre></div>
</div>
</div>
<p>For the original Stan syntax, it has the benefit of familiarity (for
those who already use it) and documentation. The familiarity may come
from Stan’s choice to model itself after BUGS ensuring that users who
have prior experience with that language, will be comfortable
transitioning to the Stan syntax. It also is familiar for people who
have been using Stan for numerous years. Since it was released 2012,
there have now been multiple years for users to get familiar with the
language, publish examples, and write models. For new users the block
model forces organization so when writing a Stan program they will end
up being more consistent.</p>
<p>Note both Stan and Slicstan use the same codebase under the API layer
the difference in API is solely for the benefit of the user. In this
case which API is “better” is a choice for each user. We should note
this case study is only a shallow discussion of the Stan API. For full
details we suggest reading the full paper, which formalizes both sets of
syntax and shows the level of detail goes into API design.</p>
</div>
<div class="section" id="example-pymc3-and-pymc4">
<span id="id17"></span><h3><span class="section-number">10.3.2. </span>Example: PyMC3 and PyMC4<a class="headerlink" href="#example-pymc3-and-pymc4" title="Permalink to this headline">¶</a></h3>
<p>Our second API is a case study of an API design change that was required
because of a computational backend change, in this case from Theano in
PyMC3 to TensorFlow in PyMC4 a PPL that was initially intended to
replace PyMC3 <span id="id18">[<a class="reference internal" href="references.html#id165">120</a>]</span>. In the design of PyMC4 the designers of
the language desired to keep the syntax <em>as close as possible</em> to the
PyMC3 syntax. While the inference algorithms remained the same, the
fundamental way in which TensorFlow and Python works meant the PyMC4 API
forced into a particular design due to the change in computational
backend. Consider the Eight Schools model <span id="id19">[<a class="reference internal" href="references.html#id115">27</a>]</span> implemented in
PyMC3 syntax in Code Block <a class="reference internal" href="#pymc3-schools"><span class="std std-ref">pymc3_schools</span></a>
and the now, defunct <a class="footnote-reference brackets" href="#id72" id="id20">8</a> PyMC4 syntax in Code Block
<a class="reference internal" href="#pymc4-schools"><span class="std std-ref">pymc4_schools</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="pymc3-schools">
<div class="code-block-caption"><span class="caption-number">Listing 10.7 </span><span class="caption-text">pymc3_schools</span><a class="headerlink" href="#pymc3-schools" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">eight_schools_pymc3</span><span class="p">:</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="literal-block-wrapper docutils container" id="pymc4-schools">
<div class="code-block-caption"><span class="caption-number">Listing 10.8 </span><span class="caption-text">pymc4_schools</span><a class="headerlink" href="#pymc4-schools" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pm</span><span class="o">.</span><span class="n">model</span>
<span class="k">def</span> <span class="nf">eight_schools_pymc4</span><span class="p">():</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">batch_stack</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="n">obs</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">pm4</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">theta</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">obs</span>
</pre></div>
</div>
</div>
<p>The differences in PyMC4 is the decorator <code class="docutils literal notranslate"><span class="pre">&#64;pm.model</span></code>, the declaration
of a Python function, the use of generators indicated by <code class="docutils literal notranslate"><span class="pre">yield</span></code>, and
differing argument names. You may have noticed that the <code class="docutils literal notranslate"><span class="pre">yield</span></code> is the
same that you have seen in the TensorFlow Probability code. In both PPLs
<code class="docutils literal notranslate"><span class="pre">yield</span></code> statement was a necessary part of the API due to the choice
coroutine. These APIs changes were not desired however, as users would
have to learn a new syntax, all existing PyMC3 code would have to be
rewritten to use PyMC4, and all existing PyMC3 documentation would
become obsolete. This is an example where the API is informed not by
user preference, but by the choice computational backend used to
calculate the posterior. In the end the feedback from users to keep the
PyMC3 API unchanged was one of the reasons to terminate PyMC4
development.</p>
</div>
</div>
<div class="section" id="ppl-driven-transformations">
<span id="id21"></span><h2><span class="section-number">10.4. </span>PPL Driven Transformations<a class="headerlink" href="#ppl-driven-transformations" title="Permalink to this headline">¶</a></h2>
<p>In this book we saw many mathematical transformations that allowed us to
define a variety of models, easily and with great flexibility such as
GLMs. Or we saw transformations that allowed us to make results more
interpretable such as centering. In this section we will specifically
discuss transformations that are driven more specifically by PPL. They
are sometimes a bit implicit and we will discuss two examples in this
section.</p>
<div class="section" id="log-probabilities">
<span id="id22"></span><h3><span class="section-number">10.4.1. </span>Log Probabilities<a class="headerlink" href="#log-probabilities" title="Permalink to this headline">¶</a></h3>
<p>One of the most common transformations is the log probability transform.
To understand why let us go through an example where we calculate an
arbitrary likelihood. Assume we observe two independent outcomes <span class="math notranslate nohighlight">\(y_0\)</span>
and <span class="math notranslate nohighlight">\(y_1\)</span>, their joint probability is:</p>
<div class="math notranslate nohighlight" id="equation-eq-expanded-likelihood">
<span class="eqno">(10.2)<a class="headerlink" href="#equation-eq-expanded-likelihood" title="Permalink to this equation">¶</a></span>\[p(y_0, y_1 \mid \boldsymbol{\theta}) = p(y_0 \mid \boldsymbol{\theta})p(y_1 \mid \boldsymbol{\theta})
    \]</div>
<p>To give a specific situation let us say we observed the value 2 twice
and we decide to use a Normal distribution as a likelihood in our model.
We can specify our model by expanding Equation
<a class="reference internal" href="#equation-eq-expanded-likelihood">(10.2)</a> into:.</p>
<div class="math notranslate nohighlight" id="equation-eq-expanded-likelihood-normal">
<span class="eqno">(10.3)<a class="headerlink" href="#equation-eq-expanded-likelihood-normal" title="Permalink to this equation">¶</a></span>\[\mathcal{N}(2, 2 \mid \mu=0,\sigma=1) = \mathcal{N}(2 \mid 0,1)\mathcal{N}(2 \mid 0,1)
    \]</div>
<p>Being computational statisticians we can now calculate this value with a
little bit of code.</p>
<div class="literal-block-wrapper docutils container" id="two-observed">
<div class="code-block-caption"><span class="caption-number">Listing 10.9 </span><span class="caption-text">two_observed</span><a class="headerlink" href="#two-observed" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">pdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">observed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.0029150244650281948
</pre></div>
</div>
<p>With two observations we get 20 decimals of precision for joint
probability density in Code Block
<a class="reference internal" href="#two-observed"><span class="std std-ref">two_observed</span></a> with no issues, but now let
us assume we see a total of 1000 observations, all of the same value of
2. We can repeat our calculation in Code Block
<a class="reference internal" href="#thousand-observed"><span class="std std-ref">thousand_observed</span></a>. This time however,
we have an issue, Python reports a joint probability density of 0.0,
which cannot be true.</p>
<div class="literal-block-wrapper docutils container" id="thousand-observed">
<div class="code-block-caption"><span class="caption-number">Listing 10.10 </span><span class="caption-text">thousand_observed</span><a class="headerlink" href="#thousand-observed" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">observed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">pdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">observed</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
<p>What we are seeing is an example of <em>floating point precision</em> error in
computers. Due to the fundamental way computers store numbers in memory
and evaluate calculations, only a limited amount of precision is
possible. In Python this error in precision is often hidden from the
user <a class="footnote-reference brackets" href="#id73" id="id23">9</a>, although in certain cases the user is exposed to the lack of
precision, as shown in Code Block
<a class="reference internal" href="#imperfect-subtract"><span class="std std-ref">imperfect_subtract</span></a></p>
<div class="literal-block-wrapper docutils container" id="imperfect-subtract">
<div class="code-block-caption"><span class="caption-number">Listing 10.11 </span><span class="caption-text">imperfect_subtract</span><a class="headerlink" href="#imperfect-subtract" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mf">1.2</span> <span class="o">-</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>0.19999999999999996
</pre></div>
</div>
<p>With relatively “large” numbers the small error in the far decimal
place matter little. However, in Bayesian modeling we often are working
with very small float point numbers, and even worse we multiply them
together many times making them smaller yet. To mitigate this problem
PPLs perform a log transformation of probability often abbreviated to
<em>logp</em>. Thus expression <a class="reference internal" href="#equation-eq-expanded-likelihood">(10.2)</a> turns into:</p>
<div class="math notranslate nohighlight" id="equation-eq-expanded-loglikelihood">
<span class="eqno">(10.4)<a class="headerlink" href="#equation-eq-expanded-loglikelihood" title="Permalink to this equation">¶</a></span>\[\log (p(y_0, y_1 \mid \boldsymbol{\theta})) = \log (p(y_0 \mid \boldsymbol{\theta}))+ \log (p(y_1 \mid \boldsymbol{\theta}))
    \]</div>
<p>This has two effects, it makes small numbers relatively large, and due
to the product rule of logarithms, changes the multiplication into a
sum. Using the same example but performing the calculation in log space,
we see a more numerically stable result in Code Block
<a class="reference internal" href="#log-transform"><span class="std std-ref">log_transform</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="log-transform">
<div class="code-block-caption"><span class="caption-number">Listing 10.12 </span><span class="caption-text">log_transform</span><a class="headerlink" href="#log-transform" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logpdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">observed</span><span class="p">)</span>

<span class="c1"># Compute individual logpdf two ways for one observation, as well as total</span>
<span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pdf</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">logpdf</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">logpdf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(-2.9189385332046727, -2.9189385332046727, -2918.9385332046736)
</pre></div>
</div>
</div>
<div class="section" id="random-variables-and-distributions-transformations">
<span id="id24"></span><h3><span class="section-number">10.4.2. </span>Random Variables and Distributions Transformations<a class="headerlink" href="#random-variables-and-distributions-transformations" title="Permalink to this headline">¶</a></h3>
<p>Random variables that distributed as a bounded distributions, like the
Uniform distribution that is specified with a fixed interval <span class="math notranslate nohighlight">\([a, b]\)</span>,
present a challenge for gradient evaluation and samplers based on them.
Sudden changes in geometry make it difficult to sample the distribution
at the neighborhood of those sudden changes. Imagine rolling a ball down
a set of stairs or a cliff, rather than a smooth surface. It is easier
to estimate the trajectory of the ball over a smooth surface rather than
the discontinuous surface. Thus, another useful set of transformations
in PPLs <a class="footnote-reference brackets" href="#id74" id="id25">10</a> are transformations that turn bounded random variables,
such as those distributed as Uniform, Beta, Halfnormal, etc, into
unbounded random variables that span entire real line from (<span class="math notranslate nohighlight">\(-\infty\)</span>,
<span class="math notranslate nohighlight">\(\infty\)</span>). These transformations however, need to be done with care as
we must now correct for the volume changes in our transformed
distributions. To do so we need to compute the Jacobians of the
transformation and accumulate the calculated log probabilities,
explained in further detail in Section <a class="reference internal" href="chp_11.html#transformations"><span class="std std-ref">Transformations</span></a>.</p>
<p>PPLs usually transform bounded random variables into unbounded random
variables and perform inference in the unbounded space, and then
transform back the values to the original bounded space, all can happen
without user input. Thus users do not need to interact with these
transformations if they do not want to. For a concrete example both the
forward and backward transform for the Uniform random variable are shown
in Equation <a class="reference internal" href="#equation-eq-interval-transform">(10.5)</a> and computed in Code Block
<a class="reference internal" href="#interval-transform"><span class="std std-ref">interval_transform</span></a>. In this
transformation the lower and upper bound <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are mapped to
<span class="math notranslate nohighlight">\(-\infty\)</span> and <span class="math notranslate nohighlight">\(\infty\)</span> respectively, and the values in between are
“stretched” to values in between accordingly.</p>
<div class="math notranslate nohighlight" id="equation-eq-interval-transform">
<span class="eqno">(10.5)<a class="headerlink" href="#equation-eq-interval-transform" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    x_t =&amp; \log (x - a) - \log(b-x)\\
    x =&amp; a + \frac{1}{1 + e^{-x_t}} (b-a)
    
\end{split}\end{split}\]</div>
<div class="literal-block-wrapper docutils container" id="interval-transform">
<div class="code-block-caption"><span class="caption-number">Listing 10.13 </span><span class="caption-text">interval_transform</span><a class="headerlink" href="#interval-transform" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">domain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">domain</span> <span class="o">-</span> <span class="n">lower</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">upper</span> <span class="o">-</span> <span class="n">domain</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original domain: </span><span class="si">{</span><span class="n">domain</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformed domain: </span><span class="si">{</span><span class="n">transform</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Original domain:[-1.   -0.25  0.5   1.25  2.  ]
Transformed domain: [-inf -1.09861229, 0., 1.09861229, inf]
</pre></div>
</div>
<p>The automatic transform can be seen by adding a Uniform random variable
to a PyMC3 model and inspecting the variable and the underlying
distribution in the model object, as shown in Code Block
<a class="reference internal" href="#uniform-transform"><span class="std std-ref">uniform_transform</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="uniform-transform">
<div class="code-block-caption"><span class="caption-number">Listing 10.14 </span><span class="caption-text">uniform_transform</span><a class="headerlink" href="#uniform-transform" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">vars</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>([x_interval__ ~ TransformedDistribution])
</pre></div>
</div>
<p>Seeing this transform we can also then query the model to check the
transformed logp values in Code Block
<a class="reference internal" href="#uniform-transform-logp"><span class="std std-ref">uniform_transform_logp</span></a>. Note how
with the transformed distribution we can sample outside of the interval
<span class="math notranslate nohighlight">\((-1, 2)\)</span> (the boundaries of the un-transformed Uniform) and still
obtain a finite logp value. Also note the logp returned from the
<code class="docutils literal notranslate"><span class="pre">logp_nojac</span></code> method, and how the value is the same for the values of -2
and 1, and how when we call <code class="docutils literal notranslate"><span class="pre">logp</span></code> the Jacobian adjustment is made
automatically.</p>
<div class="literal-block-wrapper docutils container" id="uniform-transform-logp">
<div class="code-block-caption"><span class="caption-number">Listing 10.15 </span><span class="caption-text">uniform_transform_logp</span><a class="headerlink" href="#uniform-transform-logp" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">({</span><span class="s2">&quot;x_interval__&quot;</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">}),</span>
      <span class="n">model</span><span class="o">.</span><span class="n">logp_nojac</span><span class="p">({</span><span class="s2">&quot;x_interval__&quot;</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">}))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">logp</span><span class="p">({</span><span class="s2">&quot;x_interval__&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">}),</span>
      <span class="n">model</span><span class="o">.</span><span class="n">logp_nojac</span><span class="p">({</span><span class="s2">&quot;x_interval__&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">}))</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>-2.2538560220859454 -1.0986122886681098
-1.6265233750364456 -1.0986122886681098
</pre></div>
</div>
<p>Log transformation of the probabilities and unbounding of random
variables are transformations that PPLs usually apply without most users
knowing, but they both have a practical effect on the performance and
usability of the PPL across a wide variety of models.</p>
<p>There are other more explicit transformations users can perform directly
on the distribution itself to construct new distribution. User can then
create random variables distributed as these new distributions in a
model. For example, the bijectors module <span id="id26">[<a class="reference internal" href="references.html#id141">2</a>]</span> in TFP
can be used to transform a base distribution into more complicated
distributions. Code Block
<a class="reference internal" href="#bijector-lognormal"><span class="std std-ref">bijector_lognormal</span></a> demonstrates how
to construct a <span class="math notranslate nohighlight">\(LogNormal(0, 1)\)</span> distribution by transforming the base
distribution <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span> <a class="footnote-reference brackets" href="#id75" id="id27">11</a>. This expressive API design even
allows users to define complicated transformation using trainable
bijectors (e.g., a neural network <span id="id28">[<a class="reference internal" href="references.html#id142">121</a>]</span>) like
<code class="docutils literal notranslate"><span class="pre">tfb.MaskedAutoregressiveFlow</span></code>.</p>
<div class="literal-block-wrapper docutils container" id="bijector-lognormal">
<div class="code-block-caption"><span class="caption-number">Listing 10.16 </span><span class="caption-text">bijector_lognormal</span><a class="headerlink" href="#bijector-lognormal" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tfb</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span>

<span class="n">lognormal0</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
<span class="n">lognormal1</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">TransformedDistribution</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">tfb</span><span class="o">.</span><span class="n">Exp</span><span class="p">())</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">lognormal0</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_equal</span><span class="p">(</span><span class="n">lognormal0</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lognormal1</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>Whether explicitly used, or implicitly applied, we note that these
transformations of random variables and distributions are not a strictly
required components of PPLs, but certainly are included in nearly every
modern PPL in some fashion. For example, they can be incredibly helpful
in getting good inference results efficiently, as shown in the next
example.</p>
</div>
<div class="section" id="example-sampling-comparison-between-bounded-and-unbounded-random-variables">
<span id="id29"></span><h3><span class="section-number">10.4.3. </span>Example: Sampling Comparison between Bounded and Unbounded Random Variables<a class="headerlink" href="#example-sampling-comparison-between-bounded-and-unbounded-random-variables" title="Permalink to this headline">¶</a></h3>
<p>Here we create a small example to demonstrate the differences between
sampling from transformed and un-transformed random variable. Data is
simulated from a Normal distribution with a very small standard
deviation and model is specified in Code Block
<a class="reference internal" href="#case-study-transform"><span class="std std-ref">case_study_transform</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="case-study-transform">
<div class="code-block-caption"><span class="caption-number">Listing 10.17 </span><span class="caption-text">case_study_transform</span><a class="headerlink" href="#case-study-transform" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_observed</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.01</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_transform</span><span class="p">:</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;sd&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_observed</span><span class="p">)</span>
    <span class="n">trace_transform</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model_transform</span><span class="o">.</span><span class="n">vars</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Diverging: </span><span class="si">{</span><span class="n">trace_transform</span><span class="o">.</span><span class="n">get_sampler_stats</span><span class="p">(</span><span class="s1">&#39;diverging&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[sd_log__ ~ TransformedDistribution()]
Diverging: 0
</pre></div>
</div>
<p>We can inspect free variables and after sampling we can count the number
of divergences. From the code output from Code Block
<a class="reference internal" href="#case-study-transform"><span class="std std-ref">case_study_transform</span></a> we can verify
the bounded HalfNormal <code class="docutils literal notranslate"><span class="pre">sd</span></code> variable has been transformed, and in
subsequent sampling there are no divergences.</p>
<p>For a counterexample, let us specify the same model in Code Block
<a class="reference internal" href="#case-study-no-transform"><span class="std std-ref">case_study_no_transform</span></a> but in
this case the HalfNormal prior distribution explicitly was not
transformed. This is reflected both in the model API, as well as when
inspecting the models free variables. Subsequent sampling sampling
reports 423 divergences.</p>
<div class="literal-block-wrapper docutils container" id="case-study-no-transform">
<div class="code-block-caption"><span class="caption-number">Listing 10.18 </span><span class="caption-text">case_study_no_transform</span><a class="headerlink" href="#case-study-no-transform" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_no_transform</span><span class="p">:</span>
    <span class="n">sd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;sd&quot;</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sd</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_observed</span><span class="p">)</span>
    <span class="n">trace_no_transform</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">chains</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model_no_transform</span><span class="o">.</span><span class="n">vars</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Diverging: </span><span class="si">{</span><span class="n">trace_no_transform</span><span class="o">.</span><span class="n">get_sampler_stats</span><span class="p">(</span><span class="s1">&#39;diverging&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[sd ~ HalfNormal(sigma=10.0)]
Diverging: 423
</pre></div>
</div>
<p>In the absence of automatic transforms the user would need to spend some
time assessing why the divergences are occurring, and either know that a
transformation is needed from prior experience or come to this
conclusion through debugging and research, all efforts that take time
away from building the model and performing inference.</p>
</div>
</div>
<div class="section" id="operation-graphs-and-automatic-reparameterization">
<span id="operation-graphs-ppl"></span><h2><span class="section-number">10.5. </span>Operation Graphs and Automatic Reparameterization<a class="headerlink" href="#operation-graphs-and-automatic-reparameterization" title="Permalink to this headline">¶</a></h2>
<p>One manipulation that some PPLs perform is reparameterizing models, by
first creating an <em>operation graph</em> and then subsequently optimizing
that graph. To illustrate what this means let us define a computation:</p>
<div class="math notranslate nohighlight" id="equation-eq-basic-arithmetic">
<span class="eqno">(10.6)<a class="headerlink" href="#equation-eq-basic-arithmetic" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    x=3 \\
    y=1 \\
    x*(y/x) + 0
    
\end{split}\end{split}\]</div>
<p>Humans, with basic algebra knowledge, will quickly see that the <span class="math notranslate nohighlight">\(x\)</span>
terms cancel leaving the addition <span class="math notranslate nohighlight">\(y+0\)</span>, which has no effect, leading to
an answer of 1. We can also perform this calculation in pure Python and
get the same answer which is great, but what is not great is the wasted
computation. Pure Python, and libraries like numpy, just see these
operations as <em>computational steps</em> and will faithfully perform each
step of the stated equation, first dividing <span class="math notranslate nohighlight">\(y\)</span> by <span class="math notranslate nohighlight">\(x\)</span>, then multiplying
that result by <span class="math notranslate nohighlight">\(x\)</span>, then adding 0.</p>
<p>In contrast libraries like Theano work differently. They first construct
a <em>symbolic</em> representation of the computation as shown in Code Block
<a class="reference internal" href="#unoptimized-symbolic-algebra"><span class="std std-ref">unoptimized_symbolic_algebra</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="unoptimized-symbolic-algebra">
<div class="code-block-caption"><span class="caption-number">Listing 10.19 </span><span class="caption-text">unoptimized_symbolic_algebra</span><a class="headerlink" href="#unoptimized-symbolic-algebra" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">/</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">0</span>
<span class="n">theano</span><span class="o">.</span><span class="n">printing</span><span class="o">.</span><span class="n">debugprint</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Elemwise{add,no_inplace} [id A] &#39;&#39;   
 |Elemwise{mul,no_inplace} [id B] &#39;&#39;   
 | |x [id C]
 | |Elemwise{true_div,no_inplace} [id D] &#39;&#39;   
 |   |y [id E]
 |   |x [id C]
 |InplaceDimShuffle{x} [id F] &#39;&#39;   
   |TensorConstant{0} [id G]
</pre></div>
</div>
<div class="admonition-what-is-aesara admonition">
<p class="admonition-title">What is Aesara?</p>
<p>As you have seen Theano is the workhorse of PyMC3 models
in terms of graph representation, gradient calculation, and much more.
However, Theano was deprecated in 2017 by the original authors. Since
then PyMC developers have been maintaining Theano to support of PyMC3.
In 2020 the PyMC developers decided to move from maintaining Theano to
improving it. In doing so the PyMC developers forked Theano and named
the fork Aesara <a class="footnote-reference brackets" href="#id76" id="id30">12</a>. With a focused effort led by Brandon Willard the
legacy portions of the code base have been drastically modernized.
Additionally Aesara includes expanded functionality particularly for
Bayesian use cases. These include adding new backends (JAX and Numba)
for accelerated numerical computation and better support modern compute
hardware such GPU and TPU. With greater control and coordination over
more of the PPL components between the PyMC3 and Aesara the PyMC
developers are looking to continuously foster a better PPL experience
for developers, statisticians, and users.</p>
</div>
<p>In the output of Code Block
<a class="reference internal" href="#unoptimized-symbolic-algebra"><span class="std std-ref">unoptimized_symbolic_algebra</span></a>,
working inside out, we see on Line 4 the first operation is the division
of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, then the multiplication <span class="math notranslate nohighlight">\(x\)</span>, then finally the addition
of 0 represented in a computation graph. This same graph is shown
visually in <a class="reference internal" href="#fig-unoptimized-symbolic-algebra-graph"><span class="std std-numref">Fig. 10.1</span></a>. At this
point no actual numeric calculations have taken place, but a sequence of
operations, albeit an unoptimized one, has been generated.</p>
<div class="figure align-default" id="fig-unoptimized-symbolic-algebra-graph">
<a class="reference internal image-reference" href="../_images/symbolic_graph_unopt.png"><img alt="../_images/symbolic_graph_unopt.png" src="../_images/symbolic_graph_unopt.png" style="width: 8.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 10.1 </span><span class="caption-text">Unoptimized Theano operation graph of Equation <a class="reference internal" href="#equation-eq-basic-arithmetic">(10.6)</a>
as declared in Code Block
<a class="reference internal" href="#unoptimized-symbolic-algebra"><span class="std std-ref">unoptimized_symbolic_algebra</span></a>.</span><a class="headerlink" href="#fig-unoptimized-symbolic-algebra-graph" title="Permalink to this image">¶</a></p>
</div>
<p>We can now optimize this graph using Theano, by passing this computation
graph to <code class="docutils literal notranslate"><span class="pre">theano.function</span></code> in Code Block
<a class="reference internal" href="#optimized-symbolic-algebra"><span class="std std-ref">optimized_symbolic_algebra</span></a>.
In the output nearly all the operations have disappeared, as Theano has
recognized that both multiplication and division of <span class="math notranslate nohighlight">\(x\)</span> cancels out, and
that the addition of <span class="math notranslate nohighlight">\(0\)</span> has no effect on the final outcome. The
optimized operation graph is shown in
<a class="reference internal" href="#fig-optimized-symbolic-algebra"><span class="std std-numref">Fig. 10.2</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="optimized-symbolic-algebra">
<div class="code-block-caption"><span class="caption-number">Listing 10.20 </span><span class="caption-text">optimized_symbolic_algebra</span><a class="headerlink" href="#optimized-symbolic-algebra" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fgraph</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">],</span> <span class="p">[</span><span class="n">out</span><span class="p">])</span>
<span class="n">theano</span><span class="o">.</span><span class="n">printing</span><span class="o">.</span><span class="n">debugprint</span><span class="p">(</span><span class="n">fgraph</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>DeepCopyOp [id A] &#39;y&#39;   0
 |y [id B]
</pre></div>
</div>
<div class="figure align-default" id="fig-optimized-symbolic-algebra">
<a class="reference internal image-reference" href="../_images/symbolic_graph_opt.png"><img alt="../_images/symbolic_graph_opt.png" src="../_images/symbolic_graph_opt.png" style="width: 4.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 10.2 </span><span class="caption-text">Optimized Theano operation graph of Equation <a class="reference internal" href="#equation-eq-basic-arithmetic">(10.6)</a>
after optimization in
<a class="reference internal" href="#optimized-symbolic-algebra"><span class="std std-ref">optimized_symbolic_algebra</span></a>.</span><a class="headerlink" href="#fig-optimized-symbolic-algebra" title="Permalink to this image">¶</a></p>
</div>
<p>Theano can then calculate the answer when the optimized function is
called with the numerical inputs as shown in Code Block
<a class="reference internal" href="#optimized-symbolic-algebra-calc"><span class="std std-ref">optimized_symbolic_algebra_calc</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="optimized-symbolic-algebra-calc">
<div class="code-block-caption"><span class="caption-number">Listing 10.21 </span><span class="caption-text">optimized_symbolic_algebra_calc</span><a class="headerlink" href="#optimized-symbolic-algebra-calc" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fgraph</span><span class="p">([</span><span class="mi">1</span><span class="p">],[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[array([3.])]
</pre></div>
</div>
<p>To perform the algebraic simplication, your computer did not become
sentient and rederive the rules of algebra from scratch. Theano is able
to perform these optimizations thanks to a code optimizer <a class="footnote-reference brackets" href="#id77" id="id31">13</a> that
inspects the operation graph stated through the Theano API by a user,
scans the graph for algebraic patterns, and simplifies computation and
give users the desired result.</p>
<p>Bayesian models are just a special case of both mathematics and
computation. In Bayesian computation typically desired output is the
logp of the model. Before optimization the first step is a symbolic
representation of the operations graph, an example of which is shown in
Code Block <a class="reference internal" href="#aesara-debug"><span class="std std-ref">aesara_debug</span></a> where a one line
PyMC3 model is turned into multi line computation graph at the operation
level.</p>
<div class="literal-block-wrapper docutils container" id="aesara-debug">
<div class="code-block-caption"><span class="caption-number">Listing 10.22 </span><span class="caption-text">aesara_debug</span><a class="headerlink" href="#aesara-debug" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_normal</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

<span class="n">theano</span><span class="o">.</span><span class="n">printing</span><span class="o">.</span><span class="n">debugprint</span><span class="p">(</span><span class="n">aesara_normal</span><span class="o">.</span><span class="n">logpt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Sum{acc_dtype=float64} [id A] &#39;__logp&#39;   
 |MakeVector{dtype=&#39;float64&#39;} [id B] &#39;&#39;   
   |Sum{acc_dtype=float64} [id C] &#39;&#39;   
     |Sum{acc_dtype=float64} [id D] &#39;__logp_x&#39;   
       |Elemwise{switch,no_inplace} [id E] &#39;&#39;   
         |Elemwise{mul,no_inplace} [id F] &#39;&#39;   
         | |TensorConstant{1} [id G]
         | |Elemwise{mul,no_inplace} [id H] &#39;&#39;   
         |   |TensorConstant{1} [id I]
         |   |Elemwise{gt,no_inplace} [id J] &#39;&#39;   
         |     |TensorConstant{1.0} [id K]
         |     |TensorConstant{0} [id L]
         |Elemwise{true_div,no_inplace} [id M] &#39;&#39;   
         | |Elemwise{add,no_inplace} [id N] &#39;&#39;   
         | | |Elemwise{mul,no_inplace} [id O] &#39;&#39;   
         | | | |Elemwise{neg,no_inplace} [id P] &#39;&#39;   
         | | | | |TensorConstant{1.0} [id Q]
         | | | |Elemwise{pow,no_inplace} [id R] &#39;&#39;   
         | | |   |Elemwise{sub,no_inplace} [id S] &#39;&#39;   
         | | |   | |x ~ Normal(mu=0.0, sigma=1.0) [id T]
         | | |   | |TensorConstant{0.0} [id U]
         | | |   |TensorConstant{2} [id V]
         | | |Elemwise{log,no_inplace} [id W] &#39;&#39;   
         | |   |Elemwise{true_div,no_inplace} [id X] &#39;&#39;   
         | |     |Elemwise{true_div,no_inplace} [id Y] &#39;&#39;   
         | |     | |TensorConstant{1.0} [id Q]
         | |     | |TensorConstant{3.141592653589793} [id Z]
         | |     |TensorConstant{2.0} [id BA]
         | |TensorConstant{2.0} [id BB]
         |TensorConstant{-inf} [id BC]
</pre></div>
</div>
<p>Just like algebraic optimization this graph can then be optimized in
ways that benefit the Bayesian user <span id="id32">[<a class="reference internal" href="references.html#id89">122</a>]</span>. Recall the
discussion in Section <a class="reference internal" href="chp_04.html#model-geometry"><span class="std std-ref">Posterior Geometry Matters</span></a>, certain
models benefit from non-centered parameterizations, as this helps
eliminate challenging geometry such as Neal’s funnel. Without automatic
optimization the user must be aware of the geometrical challenge to the
sampler and make the adjust themselves. In the future, libraries such as
symbolic-pymc <a class="footnote-reference brackets" href="#id78" id="id33">14</a> will be able to make this reparameterization
automatic, just as we say the automatic transformation of log
probability and bounded distributions above. With this upcoming tool PPL
users can further focus on the model and let the PPL “worry” about the
computational optimizations.</p>
</div>
<div class="section" id="effect-handling">
<span id="id34"></span><h2><span class="section-number">10.6. </span>Effect handling<a class="headerlink" href="#effect-handling" title="Permalink to this headline">¶</a></h2>
<p>Effect handlers <span id="id35">[<a class="reference internal" href="references.html#id136">123</a>]</span> are an abstraction in programming
languages that gives different interpretations, or side effects, to the
standard behavior of statements in a program. A common example is
exception handling in Python with <code class="docutils literal notranslate"><span class="pre">try</span></code> and <code class="docutils literal notranslate"><span class="pre">except</span></code>. When some specific
error is raised in the code block under <code class="docutils literal notranslate"><span class="pre">try</span></code> statement, we can perform
different processing in the <code class="docutils literal notranslate"><span class="pre">except</span></code> block and resume computation. For
Bayesian models there are two primary effects we want the random
variable to have, draw a value (sample) from its distribution, or
condition the value to some user input. Other use cases of effect
handlers are transforming bounded random variables and automatic
reparameterization as we mentioned above.</p>
<p>Effect handlers are not a required component of PPLs but rather a design
choice that strongly influences the API and the “feel” of using the
PPL. Harkening back to our car analogy this is similar to a power
steering system in a car. It is not required, it is usually hidden from
the driver under the hood but it definitely changes the driving
experience. As effect handlers are typically “hidden” they are more
easily explained through example rather than theory.</p>
<div class="section" id="example-effect-handling-in-tfp-and-numpyro">
<span id="id36"></span><h3><span class="section-number">10.6.1. </span>Example: Effect Handling in TFP and Numpyro<a class="headerlink" href="#example-effect-handling-in-tfp-and-numpyro" title="Permalink to this headline">¶</a></h3>
<p>In the rest of this section we will see how effect handling works in
TensorFlow Probability and NumPyro. Briefly NumPyro is another PPL based
on Jax. Specifically, we will compare the high level API between
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> and model written with NumPyro
primitives, which both represent Bayesian model with a Python function
in a similar way. Also, we will be using the JAX substrate of TFP, so
that both API share the same base language and numerical computation
backend. Again consider the model in Equation
<a class="reference internal" href="#equation-eq-simple-normal-model">(10.7)</a>, in Code Block
<a class="reference internal" href="#tfp-vs-numpyro"><span class="std std-ref">tfp_vs_numpyro</span></a> we import the libraries
and write the model:</p>
<div class="literal-block-wrapper docutils container" id="tfp-vs-numpyro">
<div class="code-block-caption"><span class="caption-number">Listing 10.23 </span><span class="caption-text">tfp_vs_numpyro</span><a class="headerlink" href="#tfp-vs-numpyro" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">numpyro</span>
<span class="kn">from</span> <span class="nn">tensorflow_probability.substrates</span> <span class="kn">import</span> <span class="n">jax</span> <span class="k">as</span> <span class="n">tfp_jax</span>

<span class="n">tfp_dist</span> <span class="o">=</span> <span class="n">tfp_jax</span><span class="o">.</span><span class="n">distributions</span>
<span class="n">numpyro_dist</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">distributions</span>

<span class="n">root</span> <span class="o">=</span> <span class="n">tfp_dist</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span><span class="o">.</span><span class="n">Root</span>
<span class="k">def</span> <span class="nf">tfp_model</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfp_dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfp_dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfp_dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">numpyro_model</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">numpyro_dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span> <span class="n">numpyro_dist</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">numpyro_dist</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>From a glance, <code class="docutils literal notranslate"><span class="pre">tfp_model</span></code> and <code class="docutils literal notranslate"><span class="pre">numpyro_model</span></code> looks similar, both are
Python functions with no input argument and return statement (note
NumPyro model can have inputs and return statements), both need to
indicate which statement should be considered as random variable (TFP
with <code class="docutils literal notranslate"><span class="pre">yield</span></code>, NumPyro with <code class="docutils literal notranslate"><span class="pre">numpyro.sample</span></code> primitives). Moreover, the
default behavior of both <code class="docutils literal notranslate"><span class="pre">tfp_model</span></code> and <code class="docutils literal notranslate"><span class="pre">numpyro_model</span></code> is ambiguous,
they do not really do anything <a class="footnote-reference brackets" href="#id79" id="id37">15</a> until you give it specific
instruction. For example, in Code Block
<a class="reference internal" href="#tfp-vs-numpyro-prior-sample"><span class="std std-ref">tfp_vs_numpyro_prior_sample</span></a>
we draw prior samples from both models, and evaluate the log probability
on the same prior samples (that returned by the TFP model).</p>
<div class="literal-block-wrapper docutils container" id="tfp-vs-numpyro-prior-sample">
<div class="code-block-caption"><span class="caption-number">Listing 10.24 </span><span class="caption-text">tfp_vs_numpyro_prior_sample</span><a class="headerlink" href="#tfp-vs-numpyro-prior-sample" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">52346</span><span class="p">)</span>

<span class="c1"># Draw samples</span>
<span class="n">jd</span> <span class="o">=</span> <span class="n">tfp_dist</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span><span class="p">(</span><span class="n">tfp_model</span><span class="p">)</span>
<span class="n">tfp_sample</span> <span class="o">=</span> <span class="n">jd</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">sample_key</span><span class="p">)</span>

<span class="n">predictive</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">infer</span><span class="o">.</span><span class="n">Predictive</span><span class="p">(</span><span class="n">numpyro_model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">numpyro_sample</span> <span class="o">=</span> <span class="n">predictive</span><span class="p">(</span><span class="n">sample_key</span><span class="p">)</span>

<span class="c1"># Evaluate log prob</span>
<span class="n">log_likelihood_tfp</span> <span class="o">=</span> <span class="n">jd</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">tfp_sample</span><span class="p">)</span>
<span class="n">log_likelihood_numpyro</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">infer</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">log_density</span><span class="p">(</span>
    <span class="n">numpyro_model</span><span class="p">,</span> <span class="p">[],</span> <span class="p">{},</span>
    <span class="c1"># Samples returning from JointDistributionCoroutine is a</span>
    <span class="c1"># Namedtuple like Python object, we convert it to a dictionary</span>
    <span class="c1"># so that numpyro can recognize it.</span>
    <span class="n">params</span><span class="o">=</span><span class="n">tfp_sample</span><span class="o">.</span><span class="n">_asdict</span><span class="p">())</span>

<span class="c1"># Validate that we get the same log prob</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">log_likelihood_tfp</span><span class="p">,</span> <span class="n">log_likelihood_numpyro</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<p>We can also condition some random variable to user input value in our
model, for example, in Code Block
<a class="reference internal" href="#tfp-vs-numpyro-condition"><span class="std std-ref">tfp_vs_numpyro_condition</span></a> we
condition <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">.01</span></code> and then sample from the model.</p>
<div class="literal-block-wrapper docutils container" id="tfp-vs-numpyro-condition">
<div class="code-block-caption"><span class="caption-number">Listing 10.25 </span><span class="caption-text">tfp_vs_numpyro_condition</span><a class="headerlink" href="#tfp-vs-numpyro-condition" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Condition z to .01 in TFP and sample</span>
<span class="n">jd</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">sample_key</span><span class="p">)</span>

<span class="c1"># Condition z to .01 in NumPyro and sample</span>
<span class="n">predictive</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">infer</span><span class="o">.</span><span class="n">Predictive</span><span class="p">(</span>
    <span class="n">numpyro_model</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;z&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">.01</span><span class="p">)})</span>
<span class="n">predictive</span><span class="p">(</span><span class="n">sample_key</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>From user perspective effect handling mostly happens behind the scenes
when high level APIs are used. In TFP a <code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code>
encapsulate the effect handlers inside of a single object, and change
the behavior of a function within that object when the input arguments
are different. For NumPyro the effect handling is a bit more explicit
and flexible. A set of effect handlers are implemented in
<code class="docutils literal notranslate"><span class="pre">numpyro.handlers</span></code>, which powers the high level APIs we just used to
generate prior samples and compute model log probability. This is shown
again in Code Block
<a class="reference internal" href="#tfp-vs-numpyro-condition-distribution"><span class="std std-ref">tfp_vs_numpyro_condition_distribution</span></a>,
where we conditioned random variable <span class="math notranslate nohighlight">\(z = .01\)</span>, draw a sample from <span class="math notranslate nohighlight">\(x\)</span>,
and construct conditional distribution <span class="math notranslate nohighlight">\(p(y \mid x, z)\)</span> and sample from
it.</p>
<div class="literal-block-wrapper docutils container" id="tfp-vs-numpyro-condition-distribution">
<div class="code-block-caption"><span class="caption-number">Listing 10.26 </span><span class="caption-text">tfp_vs_numpyro_condition_distribution</span><a class="headerlink" href="#tfp-vs-numpyro-condition-distribution" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Conditioned z to .01 in TFP and construct conditional distributions</span>
<span class="n">dist</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="n">jd</span><span class="o">.</span><span class="n">sample_distributions</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">sample_key</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">loc</span> <span class="o">==</span> <span class="n">value</span><span class="o">.</span><span class="n">x</span>
<span class="k">assert</span> <span class="n">dist</span><span class="o">.</span><span class="n">y</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="n">value</span><span class="o">.</span><span class="n">z</span>

<span class="c1"># Conditioned z to .01 in NumPyro and construct conditional distributions</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">handlers</span><span class="o">.</span><span class="n">substitute</span><span class="p">(</span><span class="n">numpyro_model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;z&quot;</span><span class="p">:</span> <span class="mf">.01</span><span class="p">})</span>
<span class="k">with</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">handlers</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">rng_seed</span><span class="o">=</span><span class="n">sample_key</span><span class="p">):</span>
    <span class="c1"># Under the seed context, the default behavior of a NumPyro model is the</span>
    <span class="c1"># same as in Pyro: drawing prior sample.</span>
    <span class="n">model_trace</span> <span class="o">=</span> <span class="n">numpyro</span><span class="o">.</span><span class="n">handlers</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">numpyro_model</span><span class="p">)</span><span class="o">.</span><span class="n">get_trace</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">model_trace</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">][</span><span class="s2">&quot;fn&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span> <span class="o">==</span> <span class="n">model_trace</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">][</span><span class="s2">&quot;value&quot;</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">model_trace</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">][</span><span class="s2">&quot;fn&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">scale</span> <span class="o">==</span> <span class="n">model_trace</span><span class="p">[</span><span class="s2">&quot;z&quot;</span><span class="p">][</span><span class="s2">&quot;value&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>The Python assertion in Code Block
<a class="reference internal" href="#tfp-vs-numpyro-condition-distribution"><span class="std std-ref">tfp_vs_numpyro_condition_distribution</span></a>
is to validate that the conditional distribution is indeed correct.
Compare to the <code class="docutils literal notranslate"><span class="pre">jd.sample_distributions(.)</span></code> call, You could see the
explicit effect handling in NumPyro with <code class="docutils literal notranslate"><span class="pre">numpyro.handlers.substitute</span></code>
that returns a conditioned model, <code class="docutils literal notranslate"><span class="pre">numpyro.handlers.seed</span></code> to set the
random seed (a JAX requirement for drawing random samples), and
<code class="docutils literal notranslate"><span class="pre">numpyro.handlers.trace</span></code> to trace the function execution. More
information of the effect handling in NumPyro and Pyro could be found in
their official documentation <a class="footnote-reference brackets" href="#id80" id="id38">16</a>.</p>
</div>
</div>
<div class="section" id="base-language-code-ecosystem-modularity-and-everything-else">
<span id="id39"></span><h2><span class="section-number">10.7. </span>Base Language, Code Ecosystem, Modularity and Everything Else<a class="headerlink" href="#base-language-code-ecosystem-modularity-and-everything-else" title="Permalink to this headline">¶</a></h2>
<p>When serious car enthusiasts pick a car, the availability of different
components that can be mixed and match can be an informative factor in
which car is ultimately purchased. These owners may choose to make
aesthetic changes to fit their preference such as a new hood for a
different look, or they may choose to perform an engine swap, which
substantially changes the performance of the vehicle. Regardless most
car owners would prefer to have more choices and flexibility in how they
can modify their vehicle then less, even if they do not choose to modify
their vehicle at all.</p>
<p>In this same way PPL users are not only concerned about the PPL itself,
but also what related code bases and packages exist in that particular
ecosystem, as well as the modularity of the PPL itself. In this book we
have used Python as the base language, and PyMC3 and TensorFlow
Probability as our PPLs. With them however, we have also used Matplotlib
for plotting, NumPy for numerical operations, Pandas and xarray for data
manipulation, and ArviZ for exploratory analysis of Bayesian models.
Colloquially these are all part of the PyData stack. However, there are
other base languages such as R with their own ecosystem of packages.
This ecosystem has similar set of tools under the tidyverse moniker, as
well as specific Bayesian packages aptly named loo, posterior, bayesplot
among others. Luckily Stan users are able to change base languages
relatively easily, as the model is defined in the Stan language and
there is a choice of interfaces available such as pystan, rstan, cmdstan
and others. PyMC3 users are relegated to Python. However, with Theano
there is modularity in the computational backend that can be used, from
the Theano native backend, to the newer JAX backend. Along with all the
above there is a laundry list of other points that matter,
non-uniformly, to PPL users including.</p>
<ul class="simple">
<li><p>Ease of development in production environments</p></li>
<li><p>Ease of installation in development environment</p></li>
<li><p>Developer speed</p></li>
<li><p>Computational speed</p></li>
<li><p>Availability of papers, blog posts, lectures</p></li>
<li><p>Documentation</p></li>
<li><p>Useful error messages</p></li>
<li><p>The community</p></li>
<li><p>What colleagues recommend</p></li>
<li><p>Upcoming features</p></li>
</ul>
<p>Just having choices is not enough however, to use a PPL a user must be
able to install and understand how to use them. The availability of work
that references the PPL tends to indicate how widely accepted it is and
provide confidence that it is indeed useful. Users are not keen to
invest time into a PPL that will no longer be maintained. And ultimately
as humans, even data informed Bayesian users, the recommendation of a
respected colleagues and other presence of a large user base, are all
influential factors in evaluating PPL, as much as the technical
capabilities in many circumstances.</p>
</div>
<div class="section" id="designing-a-ppl">
<span id="id40"></span><h2><span class="section-number">10.8. </span>Designing a PPL<a class="headerlink" href="#designing-a-ppl" title="Permalink to this headline">¶</a></h2>
<p>In this section we will switch our perspective from a PPL overview as a
user and to one of a PPL designer. Now that we identified the big
components let us design a hypothetical PPL to see how components fit
together and also how they sometimes do not fit as easily as you would
hope! The choices we will make are for illustrative purposes but frame
how the system comes together, and also how a PPL designer thinks when
putting together a PPL.</p>
<p>First we choose a base language with a numerical computing backend.
Since this book focuses on Python let us use NumPy. Ideally, we also
have a set of commonly used mathematical functions implemented for us
already. For example, the central piece for implementing a PPL is a set
of (log)probability mass or density function, and some pseudo random
number generators. Luckily, those are readily available via
<code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>. Let us put these together in Code Block
<a class="reference internal" href="#scipy-stats"><span class="std std-ref">scipy_stats</span></a> with a simple demonstration of
drawing some samples from a <span class="math notranslate nohighlight">\(\mathcal{N}(1, 2)\)</span> distribution and
evaluate their log probability:</p>
<div class="literal-block-wrapper docutils container" id="scipy-stats">
<div class="code-block-caption"><span class="caption-number">Listing 10.27 </span><span class="caption-text">scipy_stats</span><a class="headerlink" href="#scipy-stats" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Draw 2 samples from a Normal(1., 2.) distribution</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="c1"># Evaluate the log probability of the samples </span>
<span class="n">logp</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">stats.norm</span></code> is a Python class in the <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> module <a class="footnote-reference brackets" href="#id81" id="id41">17</a>
which contains methods and statistical functions associated with <em>the
family of</em> Normal distributions. Alternatively, we can initialize a
Normal distribution with fixed parameters as shown in Code Block
<a class="reference internal" href="#scipy-stats2"><span class="std std-ref">scipy_stats2</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="scipy-stats2">
<div class="code-block-caption"><span class="caption-number">Listing 10.28 </span><span class="caption-text">scipy_stats2</span><a class="headerlink" href="#scipy-stats2" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">random_variable_x</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">random_variable_x</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
<span class="n">logp</span> <span class="o">=</span> <span class="n">random_variable_x</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Both Code Blocks <a class="reference internal" href="#scipy-stats"><span class="std std-ref">scipy_stats</span></a> and
<a class="reference internal" href="#scipy-stats2"><span class="std std-ref">scipy_stats2</span></a> return exactly the same
output <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">logp</span></code> as we also supplied the same <code class="docutils literal notranslate"><span class="pre">random_state</span></code>. The
differences here is that Code Block
<a class="reference internal" href="#scipy-stats2"><span class="std std-ref">scipy_stats2</span></a> we have a “frozen” random
variable <a class="footnote-reference brackets" href="#id82" id="id42">18</a> <code class="docutils literal notranslate"><span class="pre">random_variable_x</span></code> that could be considered as the SciPy
representation of <span class="math notranslate nohighlight">\(x \sim \mathcal{N}(1, 2)\)</span>. Unfortunately, this object
does work well when we try to use it naively when writing a full
Bayesian models. Consider the model
<span class="math notranslate nohighlight">\(x \sim \mathcal{N}(1, 2), y \sim \mathcal{N}(x, 0.1)\)</span>. Writing it in
Code Block
<a class="reference internal" href="#simple-model-not-working-scipy"><span class="std std-ref">simple_model_not_working_scipy</span></a>
raises an exception because <code class="docutils literal notranslate"><span class="pre">scipy.stats.norm</span></code> is expecting the input to
be a NumPy array <a class="footnote-reference brackets" href="#id83" id="id43">19</a>.</p>
<div class="literal-block-wrapper docutils container" id="simple-model-not-working-scipy">
<div class="code-block-caption"><span class="caption-number">Listing 10.29 </span><span class="caption-text">simple_model_not_working_scipy</span><a class="headerlink" href="#simple-model-not-working-scipy" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>...
TypeError: unsupported operand type(s) for +: &#39;float&#39; and &#39;rv_frozen&#39;
</pre></div>
</div>
<p>From this it becomes evident how tricky it is to design an API, what
seems intuitive for the user may not be possible with the underlying
packages, In our case to write a PPL in Python we need to make a series
of API design choices and other decision to make Code Block
<a class="reference internal" href="#simple-model-not-working-scipy"><span class="std std-ref">simple_model_not_working_scipy</span></a>
work. Specifically we want:</p>
<ol class="simple">
<li><p>A representation of random variables that could be used to
initialize another random variable;</p></li>
<li><p>To be able to condition the random variable on some specific values
(e.g., the observed data);</p></li>
<li><p>The graphical model, generated by a collection of random variables,
to behave in a consistent and predictable way.</p></li>
</ol>
<p>Getting Item 1 to work is actually pretty straightforward with a Python
class that could be recognized by NumPy as an array. We do this in Code
Block <a class="reference internal" href="#scipy-rv0"><span class="std std-ref">scipy_rv0</span></a> and use the implementation to
specific the model in Equation <a class="reference internal" href="#equation-eq-simple-normal-model">(10.7)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-simple-normal-model">
<span class="eqno">(10.7)<a class="headerlink" href="#equation-eq-simple-normal-model" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    x \sim&amp; \mathcal{N}(1, 2) \\
    z \sim&amp; \mathcal{HN}(1) \\
    y \sim&amp; \mathcal{N}(x, z)
\end{split}\end{split}\]</div>
<div class="literal-block-wrapper docutils container" id="scipy-rv0">
<div class="code-block-caption"><span class="caption-number">Listing 10.30 </span><span class="caption-text">scipy_rv0</span><a class="headerlink" href="#scipy-rv0" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomVariable</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distribution</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>

    <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">rvs</span><span class="p">())</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">RandomVariable</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">RandomVariable</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">RandomVariable</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">z</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>3.7362186279475353
0.5877468494932253
4.916129854385227
1.7421638350544257
2.074813968631388
</pre></div>
</div>
<p>A more precise description for the Python class we wrote in Code Block
<a class="reference internal" href="#scipy-rv0"><span class="std std-ref">scipy_rv0</span></a> is a stochastic array. As you see
from the Code Block output, instantiation of this object, like
<code class="docutils literal notranslate"><span class="pre">np.asarray(y)</span></code>, always gives us a different array. Adding a method to
conditioned the random variable to some value, with a <code class="docutils literal notranslate"><span class="pre">log_prob</span></code> method,
we have in Code Block <a class="reference internal" href="#scipy-rv1"><span class="std std-ref">scipy_rv1</span></a> a toy
implementation of a more functional <code class="docutils literal notranslate"><span class="pre">RandomVariable</span></code>:</p>
<div class="literal-block-wrapper docutils container" id="scipy-rv1">
<div class="code-block-caption"><span class="caption-number">Listing 10.31 </span><span class="caption-text">scipy_rv1</span><a class="headerlink" href="#scipy-rv1" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RandomVariable</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distribution</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(value=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">__array__</span><span class="p">()</span><span class="si">}</span><span class="s2">)&quot;</span>

    <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">rvs</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">value</span>

    <span class="k">def</span> <span class="nf">set_value</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">value</span> <span class="o">=</span> <span class="n">value</span>
    
    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">RandomVariable</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">RandomVariable</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">RandomVariable</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</div>
<p>We can look at the value of <code class="docutils literal notranslate"><span class="pre">y</span></code> with or without conditioning the value
of its dependencies in Code Block
<a class="reference internal" href="#scipy-rv1-value"><span class="std std-ref">scipy_rv1_value</span></a>, and the output seems
to match the expected behavior. In Code Block below note how <code class="docutils literal notranslate"><span class="pre">y</span></code> is much
closer to <code class="docutils literal notranslate"><span class="pre">x</span></code> if we set <code class="docutils literal notranslate"><span class="pre">z</span></code> to a small value.</p>
<div class="literal-block-wrapper docutils container" id="scipy-rv1-value">
<div class="code-block-caption"><span class="caption-number">Listing 10.32 </span><span class="caption-text">scipy_rv1_value</span><a class="headerlink" href="#scipy-rv1-value" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Set x=5 and z=0.1&quot;</span><span class="p">)</span>
<span class="n">x</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">z</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Reset z&quot;</span><span class="p">)</span>
<span class="n">z</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RandomVariable(value=5.044294197842362)
RandomVariable(value=4.907595148778454)
RandomVariable(value=6.374656988711546)
  Set x=5 and z=0.1
RandomVariable(value=4.973898547458924)
RandomVariable(value=4.959593974224869)
RandomVariable(value=5.003811456458226)
  Reset z
RandomVariable(value=6.421473681641824)
RandomVariable(value=4.942894375257069)
RandomVariable(value=4.996621204780431)
</pre></div>
</div>
<p>Moreover, we can evaluate the unnormalized log probability density of
the random variable. For example, in Code Block
<a class="reference internal" href="#scipy-rv1-posterior"><span class="std std-ref">scipy_rv1_posterior</span></a> we generate the
posterior distribution for <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">z</span></code> when we observe <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">5.0</span></code>.</p>
<div class="literal-block-wrapper docutils container" id="scipy-rv1-posterior">
<div class="code-block-caption"><span class="caption-number">Listing 10.33 </span><span class="caption-text">scipy_rv1_posterior</span><a class="headerlink" href="#scipy-rv1-posterior" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Observed y = 5.</span>
<span class="n">y</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">5.</span><span class="p">))</span>

<span class="n">posterior_density</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">xval</span><span class="p">,</span> <span class="n">zval</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">xval</span><span class="p">)</span> <span class="o">+</span> <span class="n">z</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">zval</span><span class="p">)</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">log_prob</span><span class="p">()</span>
<span class="n">posterior_density</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>-15.881815599614018
</pre></div>
</div>
<p>We can validate it with an explicit implementation of the posterior
density function, as shown in Code Block
<a class="reference internal" href="#scipy-posterior"><span class="std std-ref">scipy_posterior</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="scipy-posterior">
<div class="code-block-caption"><span class="caption-number">Listing 10.34 </span><span class="caption-text">scipy_posterior</span><a class="headerlink" href="#scipy-posterior" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="n">xval</span><span class="p">,</span> <span class="n">zval</span><span class="p">,</span> <span class="n">yval</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">x_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="n">z_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">y_dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">xval</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">zval</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">xval</span><span class="p">)</span> <span class="o">+</span> <span class="n">z_dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">zval</span><span class="p">)</span> <span class="o">+</span> <span class="n">y_dist</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">yval</span><span class="p">)</span>

<span class="n">log_prob</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>-15.881815599614018
</pre></div>
</div>
<p>At this point, it seems we have fulfilled the requirements of Item 1 and
Item 2 , but Item 3 is the most challenging <a class="footnote-reference brackets" href="#id84" id="id44">20</a>. For example, in a
Bayesian workflow we want to draw prior and prior predictive sample from
a model. While our <code class="docutils literal notranslate"><span class="pre">RandomVariable</span></code> draws a random sample according to
its prior, when it is not conditioned on some value, it does not record
the values of its parents (in a graphical model sense). We need
additional graph utilities assign to <code class="docutils literal notranslate"><span class="pre">RandomVariable</span></code> so that the Python
object aware of its parents and children (i.e., its Markov blanket), and
propagates the change accordingly if we draw a new sample or conditioned
on some specific value <a class="footnote-reference brackets" href="#id85" id="id45">21</a>. For example, PyMC3 uses Theano to
represent the graphical model and keep track of the dependencies (see Section
<a class="reference internal" href="#operation-graphs-ppl"><span class="std std-ref">Operation Graphs and Automatic Reparameterization</span></a> above) and Edward <a class="footnote-reference brackets" href="#id86" id="id46">22</a> uses TensorFlow v1
<a class="footnote-reference brackets" href="#id87" id="id47">23</a> to achieve that.</p>
<div class="admonition-spectrum-of-probabilistic-modelling-libraries admonition">
<p class="admonition-title">Spectrum of Probabilistic Modelling Libraries</p>
<p>One aspect of PPLs that is worth mentioning is universality. A universal PPL is a PPL that is
<strong>Turing-complete</strong>. Since the PPLs used in this book are an extension
of a general-purpose base language, they could all be considered
Turing-complete. However, research and implementation dedicated to
universal PPLs usually focus on areas slightly different from what we
discussed here. For example, an area of focus in a universal PPL is to
express dynamic models, where the model contains complex control flow
that dependent on random variable <span id="id48">[<a class="reference internal" href="references.html#id144">124</a>]</span>. As a result, the
number of random variables or the shape of a random variable could
change during the execution of a dynamic probabilistic model. A good
example of universal PPLs is Anglican <span id="id49">[<a class="reference internal" href="references.html#id146">125</a>]</span>. Dynamic
models might be valid or possible to write down, but there might not be
an efficient and robust method to inference them. In this book, we
discuss mainly PPLs focusing on static models (and their inference),
with a slight sacrifice and neglect of universality. On the other end of
the spectrum of universality, there are great software libraries that
focus on some specific probabilistic models and their specialized
inference <a class="footnote-reference brackets" href="#id88" id="id50">24</a>, which could be better suited for user’s applications
and use cases.</p>
</div>
<p>Another approach is to treat model in a more encapsulated way and write
the model as a Python function. Code Block
<a class="reference internal" href="#scipy-posterior"><span class="std std-ref">scipy_posterior</span></a> gave an example
implementation of the joint log probability density function of the
model in Equation <a class="reference internal" href="#equation-eq-simple-normal-model">(10.7)</a>, but for prior samples we
need to again rewrite it a bit, shown in Code Block
<a class="reference internal" href="#scipy-prior"><span class="std std-ref">scipy_prior</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="scipy-prior">
<div class="code-block-caption"><span class="caption-number">Listing 10.35 </span><span class="caption-text">scipy_prior</span><a class="headerlink" href="#scipy-prior" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prior_sample</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
<p>With effect handling and function tracing <a class="footnote-reference brackets" href="#id89" id="id51">25</a> in Python, we can
actually combine <code class="docutils literal notranslate"><span class="pre">log_prob</span></code> from Code Block
<a class="reference internal" href="#scipy-posterior"><span class="std std-ref">scipy_posterior</span></a> and <code class="docutils literal notranslate"><span class="pre">sample</span></code> from Code
Block <a class="reference internal" href="#scipy-prior"><span class="std std-ref">scipy_prior</span></a> into a single Python
function the user just need to write once. The PPL will then change the
behavior of how the function is executed depending on the context
(whether we are trying to obtain prior samples or evaluate the log
probability). This approach of writing a Bayesian model as function and
apply effect handler has gained significant popularity in recent years
with Pyro <span id="id52">[<a class="reference internal" href="references.html#id137">126</a>]</span> (and NumPyro <span id="id53">[<a class="reference internal" href="references.html#id138">127</a>]</span>),
Edward2 <span id="id54">[<a class="reference internal" href="references.html#id139">128</a>, <a class="reference internal" href="references.html#id140">129</a>]</span>, and JointDistribution in
TensorFlow Probability <span id="id55">[<a class="reference internal" href="references.html#id128">29</a>]</span> <a class="footnote-reference brackets" href="#id90" id="id56">26</a> <a class="footnote-reference brackets" href="#id91" id="id57">27</a>.</p>
<div class="section" id="shape-handling-in-ppls">
<span id="shape-ppl"></span><h3><span class="section-number">10.8.1. </span>Shape Handling in PPLs<a class="headerlink" href="#shape-handling-in-ppls" title="Permalink to this headline">¶</a></h3>
<p>Something that all PPLs must deal with, and subsquently PPL designers
must think about, is shapes. One of the common requests for help and
frustrations that PPL designers hear from Bayesian modeler and
practitioner are about <em>shape errors</em>. They are misspecification of the
intended flow of array computation, which can cause issues like
broadcasting errors. In this section we will give some examples to
highly some subtleties of shape handling in PPLs.</p>
<p>Consider the prior predictive sample function defined in Code Block
<a class="reference internal" href="#scipy-prior"><span class="std std-ref">scipy_prior</span></a> for the model in Equation
<a class="reference internal" href="#equation-eq-simple-normal-model">(10.7)</a>, executing the function draws a single
sample from the prior and prior predictive distribution, it is certainly
quite inefficient if we want to draw a large among of iid samples from
it. Distribution in <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> has a <code class="docutils literal notranslate"><span class="pre">size</span></code> keyword argument to allow
us to draw iid samples easily, with a small modification in Code Block
<a class="reference internal" href="#prior-batch"><span class="std std-ref">prior_batch</span></a> we have:</p>
<div class="literal-block-wrapper docutils container" id="prior-batch">
<div class="code-block-caption"><span class="caption-number">Listing 10.36 </span><span class="caption-text">prior_batch</span><a class="headerlink" href="#prior-batch" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prior_sample</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">y</span>

<span class="nb">print</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">prior_sample</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">))])</span>
<span class="nb">print</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">prior_sample</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">))])</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[(2,), (2,), (2,)]
[(2, 3, 5), (2, 3, 5), (2, 3, 5)]
</pre></div>
</div>
<p>As you can see, the function can handle arbitrary sample shape by adding
<code class="docutils literal notranslate"><span class="pre">size</span></code> keyword argument when calling the random method <code class="docutils literal notranslate"><span class="pre">rvs</span></code>. Note
however, for random variable <code class="docutils literal notranslate"><span class="pre">y</span></code>, we do not supply the <code class="docutils literal notranslate"><span class="pre">size</span></code> keyword
argument as the sample shape is already implied from its parents.</p>
<p>Consider another example in Code Block
<a class="reference internal" href="#prior-lm-batch"><span class="std std-ref">prior_lm_batch</span></a> for a linear regression
model, we implemented <code class="docutils literal notranslate"><span class="pre">lm_prior_sample0</span></code> to draw one set of prior
samples, and <code class="docutils literal notranslate"><span class="pre">lm_prior_sample</span></code> to draw a batch of prior samples.</p>
<div class="literal-block-wrapper docutils container" id="prior-lm-batch">
<div class="code-block-caption"><span class="caption-number">Listing 10.37 </span><span class="caption-text">prior_lm_batch</span><a class="headerlink" href="#prior-lm-batch" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_row</span><span class="p">,</span> <span class="n">n_feature</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_row</span><span class="p">,</span> <span class="n">n_feature</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">lm_prior_sample0</span><span class="p">():</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_feature</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">beta</span> <span class="o">+</span> <span class="n">intercept</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y</span>

<span class="k">def</span> <span class="nf">lm_prior_sample</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">size</span><span class="p">,)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_feature</span><span class="p">),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span>
        <span class="n">size</span><span class="o">=</span><span class="n">size</span> <span class="o">+</span> <span class="p">(</span><span class="n">n_feature</span><span class="p">,))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">halfnorm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">beta</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
<p>Comparing the two functions above, we see that to make the prior sample
function to handle arbitrary sample shape, we need to make a few changes
in <code class="docutils literal notranslate"><span class="pre">lm_prior_sample</span></code>:</p>
<ul class="simple">
<li><p>Supply <code class="docutils literal notranslate"><span class="pre">size</span></code> keyword argument to the sample call of root random
variables only;</p></li>
<li><p>Supply <code class="docutils literal notranslate"><span class="pre">size</span> <span class="pre">+</span> <span class="pre">(n_feature,)</span></code> keyword argument to the sample call of
<code class="docutils literal notranslate"><span class="pre">beta</span></code> due to API limitations, which is a length <code class="docutils literal notranslate"><span class="pre">n_feature</span></code> vector
of regression coefficient. We need to additionally make sure <code class="docutils literal notranslate"><span class="pre">size</span></code>
is a tuple in the function so that it could be combined with the
original shape of <code class="docutils literal notranslate"><span class="pre">beta</span></code>;</p></li>
<li><p>Shape handling by appending a dimension to <code class="docutils literal notranslate"><span class="pre">beta</span></code>, <code class="docutils literal notranslate"><span class="pre">intercept</span></code>, and
<code class="docutils literal notranslate"><span class="pre">sigma</span></code>, and squeezing of the matrix multiplication result so that
they are broadcast-able.</p></li>
</ul>
<p>As you can see, there is a lot of room for error and flexibility of how
you might go about to implementing a “shape-safe” prior sample
function. The complexity does not stop here, shape issues also pop up
when computing model log probability and during inference (e.g., how
non-scalar sampler MCMC kernel parameters broadcast to model
parameters). There are convenience function transformations that
vectorize your Python function such as <code class="docutils literal notranslate"><span class="pre">numpy.vectorize</span></code> or <code class="docutils literal notranslate"><span class="pre">jax.vmap</span></code>
in JAX, but they are often not a silver bullet solution to fixing the
all issues. For example, it requires additional user input if the
vectorization is across multiple axes.</p>
<p>An example of a well defined shape handling logic is the shape semantic
in TensorFlow Probability <span id="id58">[<a class="reference internal" href="references.html#id141">2</a>]</span> <a class="footnote-reference brackets" href="#id92" id="id59">28</a>, which
conceptually partitions a Tensor’s shape into three groups:</p>
<ul class="simple">
<li><p><em>Sample shape</em> that describes iid draws from the distribution.</p></li>
<li><p><em>Batch shape</em> that describes independent, not identically
distributed draws. Usually it is a set of (different)
parameterizations to the same distribution.</p></li>
<li><p><em>Event shape</em> that describes the shape of a single draw (event
space) from the distribution. For example, samples from multivariate
distributions have non-scalar event shape.</p></li>
</ul>
<p>Explicit batch shape is a powerful concept in TFP, which can be
considered roughly along the line of <em>independent copy of the same thing
that I would like to “parallelly” evaluate over</em>. For example,
different chains from a MCMC trace, a batch of observation in mini-batch
training, etc. For example, applying the shape semantic to the prior
sample function in Code Block
<a class="reference internal" href="#prior-lm-batch"><span class="std std-ref">prior_lm_batch</span></a>, we have a <code class="docutils literal notranslate"><span class="pre">beta</span></code>
distributed as a <code class="docutils literal notranslate"><span class="pre">n_feature</span></code> batch of <span class="math notranslate nohighlight">\(\mathcal{N}(0, 10)\)</span> distribution.
Note that while it is fine for the purpose of prior sampling, to be more
precise we actually want the Event shape being <code class="docutils literal notranslate"><span class="pre">n_feature</span></code> instead of
the batch shape. In that case the shape is correct for both forward
random sampling and inverse log-probability computation. In NumPy it
could be done by defining and sampling from a
<code class="docutils literal notranslate"><span class="pre">stats.multivariate_normal</span></code> instead.</p>
<p>When a user defines a TFP distribution, they can inspect the batch shape
and the event shape to make sure it is working as intended. It is
especially useful when writing a Bayesian model using
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code>. For example, we rewrite the regression model in
Code Block <a class="reference internal" href="#prior-lm-batch"><span class="std std-ref">prior_lm_batch</span></a> into Code
Block <a class="reference internal" href="#jd-lm-batch"><span class="std std-ref">jd_lm_batch</span></a> using
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionSequential</span></code>:</p>
<div class="literal-block-wrapper docutils container" id="jd-lm-batch">
<div class="code-block-caption"><span class="caption-number">Listing 10.38 </span><span class="caption-text">jd_lm_batch</span><a class="headerlink" href="#jd-lm-batch" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">jd</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">JointDistributionSequential</span><span class="p">([</span>
<span class="linenos"> 2</span>    <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="linenos"> 3</span>    <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">n_feature</span><span class="p">),</span>
<span class="linenos"> 4</span>    <span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
<span class="linenos"> 5</span>    <span class="k">lambda</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">intercept</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
<span class="linenos"> 6</span>        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
<span class="linenos"> 7</span>            <span class="n">loc</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;ij,...j-&gt;...i&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
<span class="linenos"> 8</span>            <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
<span class="linenos"> 9</span>        <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="linenos">10</span>        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="linenos">11</span><span class="p">])</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="nb">print</span><span class="p">(</span><span class="n">jd</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="n">n_sample</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="linenos">16</span><span class="k">for</span> <span class="n">log_prob_part</span> <span class="ow">in</span> <span class="n">jd</span><span class="o">.</span><span class="n">log_prob_parts</span><span class="p">(</span><span class="n">jd</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_sample</span><span class="p">)):</span>
<span class="linenos">17</span>    <span class="k">assert</span> <span class="n">log_prob_part</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">n_sample</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tfp.distributions.JointDistributionSequential &#39;JointDistributionSequential&#39; 
batch_shape=[[], [], [], []] 
event_shape=[[], [5], [], [1000]] 
dtype=[float32, float32, float32, float32]
</pre></div>
</div>
<p>A key thing to look for when ensuring the model is specified correctly
is that <code class="docutils literal notranslate"><span class="pre">batch_shape</span></code> are consistent across arrays. In our example they
are since they are all empty. Another helpful way to check that output
is a structure of Tensor with the same shape <code class="docutils literal notranslate"><span class="pre">k</span></code> when calling
<code class="docutils literal notranslate"><span class="pre">jd.log_prob_parts(jd.sample(k))</span></code> (line 15-17 in Code Block
<a class="reference internal" href="#jd-lm-batch"><span class="std std-ref">jd_lm_batch</span></a>). This will make sure the
computation of the model log probability (e.g., for posterior inference)
is correct. You can find a nice summary and visual demonstration of the
shape semantic in TFP in a blog post by Eric J. Ma (<em>Reasoning about
Shapes and Probability Distributions</em>) <a class="footnote-reference brackets" href="#id93" id="id60">29</a>.</p>
</div>
</div>
<div class="section" id="takeaways-for-the-applied-bayesian-practitioner">
<span id="id61"></span><h2><span class="section-number">10.9. </span>Takeaways for the Applied Bayesian Practitioner<a class="headerlink" href="#takeaways-for-the-applied-bayesian-practitioner" title="Permalink to this headline">¶</a></h2>
<p>We want to stress to the reader that the goal of this chapter is <em>not</em>
to make you a proficient PPL designer but more so an informed PPL user.
As a user, particularly if you are just starting out, it can be
difficult to understand which PPL to choose and why. When you first
learn about a PPL, it is good to keep in mind the basic components we
listed in this chapter. For example, what primitives parameterize a
distribution, how to evaluate the log-probability of some value, or what
primitives define a random variables and how to link different random
variables to construct a graphical model (the effect handling) etc.</p>
<p>There are many considerations when picking PPL aside from the PPL
itself. Given everything we have discussed in this chapter so far its
very easy to get lost trying to optimize over each component to “pick
the best one”. Its also very easy for experienced practitioners to
argue about why one PPL is better than another ad nauseum. Our advice is
to pick the PPL that you feel most comfortable starting with and learn
what is needed in your situation from applied experience.</p>
<p>However, over time you will get a sense of what you need from a PPL and
more importantly what you do not. We suggest trying out a couple of
PPLs, in addition to the ones presented in this book, to get a sense of
what will work for you. As a user you have the most to gain from
actually <em>using</em> the PPL.</p>
<p>Like Bayesian modeling when you explore the distribution of
possibilities the collection of data becomes more informative than any
single point. With the knowledge of how PPLs are constructed from this
chapter, and personal experience through “taking some for a spin” we
hope you will be finding the one that works best for you.</p>
</div>
<div class="section" id="exercises">
<span id="exercises10"></span><h2><span class="section-number">10.10. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>10E1.</strong> Find a PPLs that utilizes another base language
other than Python. Determine what differences are between PyMC3 or TFP.
Specifically note a difference between the API and the computational
backend.</p>
<p><strong>10E2.</strong> In this book we primarily use the PyData ecosystem.
R is another popular programming language with a similar ecosystem. Find
the R equivalents for</p>
<ul class="simple">
<li><p>Matplotlib</p></li>
<li><p>The ArviZ LOO function</p></li>
<li><p>Bayesian visualization</p></li>
</ul>
<p><strong>10E3.</strong> What are other transformations that we have used on
data and models throughout this book? What effect did they have? Hint:
Refer to Chapter [3]](chap2)</p>
<p><strong>10E4.</strong> Draw a block diagram of a PPL<a class="footnote-reference brackets" href="#id94" id="id62">30</a>. Label each
component and explain what it does in your own words. There is no one
right answer for this question.</p>
<p><strong>10E5.</strong> Explain what batch shape, event shape, and sample
shape are in your own words. In particular be sure to detail why its
helpful to have each concept in a PPL.</p>
<p><strong>10E6.</strong> Find the Eight Schools NumPyro example online.
Compare this to the TFP example, in particular noting the difference in
primitives and syntax. What is similar? What is different?</p>
<p><strong>10E7.</strong> Specify the following computation in Theano</p>
<div class="math notranslate nohighlight">
\[\sin(\frac{1}{2}\pi x) + \exp(\log(x)) + \frac{(x-2)^2}{(x^2-4x+4)}\]</div>
<p>Generate the unoptimized computational graph. How many lines are
printed. Use the <code class="docutils literal notranslate"><span class="pre">theano.function</span></code> method to run the optimizer. What is
different about the optimized graph? Run a calculation using the
optimized Theano function where <span class="math notranslate nohighlight">\(x=1\)</span>. What is the output value?</p>
<p><strong>10M8.</strong> Create a model with following distributions using
PyMC3.</p>
<ul class="simple">
<li><p>Gamma(alpha=1, beta=1)</p></li>
<li><p>Binomial(p=5,12)</p></li>
<li><p>TruncatedNormal(mu=2, sd=1, lower=1)</p></li>
</ul>
<p>Verify which distributions are automatically transformed from bounded to
unbounded distributions. Plot samples from the priors for both the
bounded priors and their paired transforms if one exists. What
differences can you note?</p>
<p><strong>10H9.</strong> BlackJAX is a library of samplers for JAX. Generate
a random sample of from <span class="math notranslate nohighlight">\(\mathcal(N)(0,10)\)</span> of size 20. Use the HMC
sampler in JAX to recover the parameters of the data generating
distribution. The BlackJAX documentation and Section <a class="reference internal" href="chp_11.html#hmc"><span class="std std-ref">Hamiltonian Monte Carlo</span></a>
will be helpful.</p>
<p><strong>10H10.</strong> Implement the linear penguins model defined in
Code Block
<a class="reference internal" href="chp_03.html#non-centered-regression"><span class="std std-ref">non_centered_regression</span></a> in
NumPyro. After verifying the result are roughly the same as TFP and
PyMC3, what differences do you see from the TFP and PyMC3 syntax? What
similarities do you see? Be sure not just compare models, but compare
the entire workflow.</p>
<p><strong>10H11.</strong> We have explained reparameterization in previous
chapter, for example, center and non-center parameterization for linear
model in Section <a class="reference internal" href="chp_04.html#model-geometry"><span class="std std-ref">Posterior Geometry Matters</span></a>. One of the use case
for effect handling is to perform automatic reparameterization
<span id="id63">[<a class="reference internal" href="references.html#id176">130</a>]</span>. Try to write a effect handler in NumPyro to
perform automatic non-centering of a random variable in a model. Hint:
NumPyro already provides this functionality with
<code class="docutils literal notranslate"><span class="pre">numpyro.handlers.reparam</span></code>.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>With the prerequisite that basic ingredients like APIs to specify
probability distribution and random variables, basic numerical
transformations are already implemented.</p>
</dd>
<dt class="label" id="id65"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Even Wikipedia only contains a partial list
<a class="reference external" href="https://en.wikipedia.org/wiki/Probabilistic_programming#List_of_probabilistic_programming_languages">https://en.wikipedia.org/wiki/Probabilistic_programming#List_of_probabilistic_programming_languages</a>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p><em>An Introduction to Probabilistic Programming</em> by van de Meent et
al <span id="id67">[<a class="reference internal" href="references.html#id145">131</a>]</span> is a good starting point if you are
interested in both PPL development and usage.</p>
</dd>
<dt class="label" id="id68"><span class="brackets"><a class="fn-backref" href="#id6">4</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/stripe/rainier">https://github.com/stripe/rainier</a>. A more in-depth reflection of
the development of Rainer is described in a podcast
<a class="reference external" href="https://www.learnbayesstats.com/episode/22-eliciting-priors-and-doing-bayesian-inference-at-scale-with-avi-bryant">https://www.learnbayesstats.com/episode/22-eliciting-priors-and-doing-bayesian-inference-at-scale-with-avi-bryant</a></p>
</dd>
<dt class="label" id="id69"><span class="brackets"><a class="fn-backref" href="#id8">5</a></span></dt>
<dd><p>See Section <a class="reference internal" href="chp_11.html#inference-methods"><span class="std std-ref">Inference Methods</span></a> for a discussion of
some of more prevalent posterior computation methods.</p>
</dd>
<dt class="label" id="id70"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>In terms of effective samples per second.</p>
</dd>
<dt class="label" id="id71"><span class="brackets"><a class="fn-backref" href="#id12">7</a></span></dt>
<dd><p>The Zen of Python detai the philosophy behind this idea of
pythonic design<a class="reference external" href="https://www.python.org/dev/peps/pep-0020/">https://www.python.org/dev/peps/pep-0020/</a></p>
</dd>
<dt class="label" id="id72"><span class="brackets"><a class="fn-backref" href="#id20">8</a></span></dt>
<dd><p><a class="reference external" href="https://pymc-devs.medium.com/the-future-of-pymc3-or-theano-is-dead-long-live-theano-d8005f8a0e9b.">https://pymc-devs.medium.com/the-future-of-pymc3-or-theano-is-dead-long-live-theano-d8005f8a0e9b.</a>
contains more details about the decision and future road map of
PyMC3</p>
</dd>
<dt class="label" id="id73"><span class="brackets"><a class="fn-backref" href="#id23">9</a></span></dt>
<dd><p><a class="reference external" href="https://docs.Python.org/3/tutorial/floatingpoint.html">https://docs.Python.org/3/tutorial/floatingpoint.html</a></p>
</dd>
<dt class="label" id="id74"><span class="brackets"><a class="fn-backref" href="#id25">10</a></span></dt>
<dd><p><a class="reference external" href="https://mc-stan.org/docs/2_25/reference-manual/variable-transforms-chapter.html">https://mc-stan.org/docs/2_25/reference-manual/variable-transforms-chapter.html</a></p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id27">11</a></span></dt>
<dd><p>In fact, this is how <code class="docutils literal notranslate"><span class="pre">tfd.LogNormal</span></code> is implemented in TFP, with
some additional overwrite of class method to make computation more
stable.</p>
</dd>
<dt class="label" id="id76"><span class="brackets"><a class="fn-backref" href="#id30">12</a></span></dt>
<dd><p>In Greek mythology Aesara is the daughter of Theano, hence the
fitting name.</p>
</dd>
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id31">13</a></span></dt>
<dd><p><a class="reference external" href="https://theano-pymc.readthedocs.io/en/latest/optimizations.html?highlight=o1#optimizations">https://theano-pymc.readthedocs.io/en/latest/optimizations.html?highlight=o1#optimizations</a></p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id33">14</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/pymc-devs/symbolic-pymc">https://github.com/pymc-devs/symbolic-pymc</a></p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id37">15</a></span></dt>
<dd><p>The default behavior of a Pyro model is to sample from the
distribution, but not in NumPyro.</p>
</dd>
<dt class="label" id="id80"><span class="brackets"><a class="fn-backref" href="#id38">16</a></span></dt>
<dd><p><a class="reference external" href="https://pyro.ai/examples/effect_handlers.html">https://pyro.ai/examples/effect_handlers.html</a></p>
</dd>
<dt class="label" id="id81"><span class="brackets"><a class="fn-backref" href="#id41">17</a></span></dt>
<dd><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/stats.html">https://docs.scipy.org/doc/scipy/reference/stats.html</a></p>
</dd>
<dt class="label" id="id82"><span class="brackets"><a class="fn-backref" href="#id42">18</a></span></dt>
<dd><p>We will go into more details about random variable in Chapter
<a class="reference internal" href="chp_11.html#app"><span class="std std-ref">11</span></a>.</p>
</dd>
<dt class="label" id="id83"><span class="brackets"><a class="fn-backref" href="#id43">19</a></span></dt>
<dd><p>To be more precise, a Python object with a <code class="docutils literal notranslate"><span class="pre">__array__</span></code> method.</p>
</dd>
<dt class="label" id="id84"><span class="brackets"><a class="fn-backref" href="#id44">20</a></span></dt>
<dd><p>Getting the shape right, minimizing unwanted side effects, to
name a few.</p>
</dd>
<dt class="label" id="id85"><span class="brackets"><a class="fn-backref" href="#id45">21</a></span></dt>
<dd><p>The graphical representation of a Bayesian Model is a central
concept in PPL, but in many cases they are implicit.</p>
</dd>
<dt class="label" id="id86"><span class="brackets"><a class="fn-backref" href="#id46">22</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/blei-lab/edward">https://github.com/blei-lab/edward</a></p>
</dd>
<dt class="label" id="id87"><span class="brackets"><a class="fn-backref" href="#id47">23</a></span></dt>
<dd><p>The API of TensorFlow changed significantly between v1 and the
current version (v2).</p>
</dd>
<dt class="label" id="id88"><span class="brackets"><a class="fn-backref" href="#id50">24</a></span></dt>
<dd><p>For example, <a class="reference external" href="https://github.com/jmschrei/pomegranate">https://github.com/jmschrei/pomegranate</a> for
Bayesian Network.</p>
</dd>
<dt class="label" id="id89"><span class="brackets"><a class="fn-backref" href="#id51">25</a></span></dt>
<dd><p>See the Python documentation for a complete explanation
<a class="reference external" href="https://docs.python.org/3/library/trace.html">https://docs.python.org/3/library/trace.html</a></p>
</dd>
<dt class="label" id="id90"><span class="brackets"><a class="fn-backref" href="#id56">26</a></span></dt>
<dd><p>Also see mcx <a class="reference external" href="https://github.com/rlouf/mcx">https://github.com/rlouf/mcx</a> that use Python AST
to do function rewrite; and oryx
<a class="reference external" href="https://www.tensorflow.org/probability/oryx">https://www.tensorflow.org/probability/oryx</a> that make use of the
JAX tracing for function transformation</p>
</dd>
<dt class="label" id="id91"><span class="brackets"><a class="fn-backref" href="#id57">27</a></span></dt>
<dd><p>If you are interested in more details about PPL development in
Python, take a look at this PyData Talk:
<a class="reference external" href="https://www.youtube.com/watch?v=WHoS1ETYFrw">https://www.youtube.com/watch?v=WHoS1ETYFrw</a></p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id59">28</a></span></dt>
<dd><p>See also
<a class="reference external" href="https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial">https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial</a></p>
</dd>
<dt class="label" id="id93"><span class="brackets"><a class="fn-backref" href="#id60">29</a></span></dt>
<dd><p>See
<a class="reference external" href="https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/">https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/</a>.
Luciano Paz also wrote an excellent introduction on shape handling
in PPLs in <em>PyMC3 shape handling</em>
<a class="reference external" href="https://lucianopaz.github.io/2019/08/19/pymc3-shape-handling/">https://lucianopaz.github.io/2019/08/19/pymc3-shape-handling/</a></p>
</dd>
<dt class="label" id="id94"><span class="brackets"><a class="fn-backref" href="#id62">30</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Block_diagram">https://en.wikipedia.org/wiki/Block_diagram</a></p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_09.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">9. </span>End to End Bayesian Workflows</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="chp_11.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Appendiceal Topics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Martin, Kumar, Lao<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>