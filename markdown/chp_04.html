
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Extending Linear Models &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Splines" href="chp_05.html" />
    <link rel="prev" title="3. Linear Models and Probabilistic Programming Languages" href="chp_03.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_04.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transforming-covariates">
   4.1. Transforming Covariates
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#varying-uncertainty">
   4.2. Varying Uncertainty
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interaction-effects">
   4.3. Interaction Effects
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#robust-regression">
   4.4. Robust Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pooling-multilevel-models-and-mixed-effects">
   4.5. Pooling, Multilevel Models, and Mixed Effects
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unpooled-parameters">
     4.5.1. Unpooled Parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pooled-parameters">
     4.5.2. Pooled Parameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mixing-group-and-common-parameters">
     4.5.3. Mixing Group and Common Parameters
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hierarchical-models">
   4.6. Hierarchical Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-geometry-matters">
     4.6.1. Posterior Geometry Matters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predictions-at-multiple-levels">
     4.6.2. Predictions at Multiple Levels
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#priors-for-multilevel-models">
     4.6.3. Priors for Multilevel Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   4.7. Exercises
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="extending-linear-models">
<span id="chap3"></span><h1><span class="section-number">4. </span>Extending Linear Models<a class="headerlink" href="#extending-linear-models" title="Permalink to this headline">¶</a></h1>
<p>A common trope in a sales pitch is the phrase “But wait! There is
more!” In the lead up an audience is shown a product that incredibly
seems to do it all, but somehow the salesperson shows off another use
case for the already incredibly versatile tool. This is what we say to
you about linear regression. In Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a> we show a
variety of ways to use and extend linear regression. But there is still
a lot more we can do with linear models. From covariate transformation,
varying variance, to multilevel models: each of these ideas provide
extra flexibility to use linear regressions in an even wider set of
circumstances.</p>
<div class="section" id="transforming-covariates">
<span id="id1"></span><h2><span class="section-number">4.1. </span>Transforming Covariates<a class="headerlink" href="#transforming-covariates" title="Permalink to this headline">¶</a></h2>
<p>In Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a> we saw that with a linear model and an
identity link function, a unit change in <span class="math notranslate nohighlight">\(x_i\)</span> led to a <span class="math notranslate nohighlight">\(\beta_i\)</span> change
in the expected response variable <span class="math notranslate nohighlight">\(Y\)</span>, at any value of <span class="math notranslate nohighlight">\(X_i\)</span>. Then we
saw how Generalized Linear Models can be created by changing the
likelihood function (e.g. from a Gaussian to Bernoulli), which in
general requires a change in the link function.</p>
<p>Another useful modification, to the vanilla linear model, is to
transform the covariates <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, in order to make the relationship
between <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> nonlinear. For example, we may assume that
a square-root-unit change or log-unit change, etc. in <span class="math notranslate nohighlight">\(x_i\)</span> led to a
<span class="math notranslate nohighlight">\(\beta_i\)</span> change in the expected response variable <span class="math notranslate nohighlight">\(Y\)</span>. We can note
express mathematically by extending Equation
<a class="reference internal" href="chp_03.html#equation-eq-expanded-regression">(3.2)</a> with an additional
term, <span class="math notranslate nohighlight">\(f(.)\)</span>, which indicates an arbitrary transformation applied to
each covariate <span class="math notranslate nohighlight">\((X_i)\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-covariate-transformation-regression">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-eq-covariate-transformation-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \mu =&amp; \beta_0 + \beta_1 f_1(X_1) + \dots + \beta_m f_m(X_m) \\
Y \sim&amp; \mathcal{N}(\mu, \sigma)
\end{split}\end{split}\]</div>
<p>In most of our previous examples <span class="math notranslate nohighlight">\(f(.)\)</span> was present but was the identity
transformation. In a couple of the previous examples we centered the
covariates to make the coefficient easier to interpret and the centering
operation is one type of covariate transformation. However, <span class="math notranslate nohighlight">\(f(.)\)</span> can
be any arbitrary transformation. To illustrate, let us borrow an example
from Bayesian Analysis with Python <span id="id2">[<a class="reference internal" href="references.html#id68">41</a>]</span> and create a model for
the length of babies. First we will load the data and plot in Code Block
<a class="reference internal" href="#babies-data"><span class="std std-ref">babies_data</span></a> and plot the age and month in
<a class="reference internal" href="#fig-baby-length-scatter"><span class="std std-numref">Fig. 4.1</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="babies-data">
<div class="code-block-caption"><span class="caption-number">Listing 4.1 </span><span class="caption-text">babies_data</span><a class="headerlink" href="#babies-data" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">babies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/babies.csv&quot;</span><span class="p">)</span>
<span class="c1"># Add a constant term so we can use the dot product to express the intercept</span>
<span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-baby-length-scatter">
<a class="reference internal image-reference" href="../_images/Baby_Length_Scatter.png"><img alt="../_images/Baby_Length_Scatter.png" src="../_images/Baby_Length_Scatter.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Scatter plot of the nonlinear correlation between a baby’s age in months
and observed, or measured, length.</span><a class="headerlink" href="#fig-baby-length-scatter" title="Permalink to this image">¶</a></p>
</div>
<p>Let us formulate a model in Code Block
<a class="reference internal" href="#babies-linear"><span class="std std-ref">babies_linear</span></a> which we can use to predict
the length of the baby at each month of their childhood, as well as
determine how quickly a child is growing per month. Note that this model
formulation contains no transformations, and nothing we have not seen
already in Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="babies-linear">
<div class="code-block-caption"><span class="caption-number">Listing 4.2 </span><span class="caption-text">babies_linear</span><a class="headerlink" href="#babies-linear" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_baby_linear</span><span class="p">:</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">babies</span><span class="p">[[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">,</span> <span class="s2">&quot;Month&quot;</span><span class="p">]],</span> <span class="n">β</span><span class="p">))</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;ϵ&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">length</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Length&quot;</span><span class="p">])</span>

    <span class="n">trace_linear</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span>
    <span class="n">pcc_linear</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_linear</span><span class="p">)</span>
    <span class="n">inf_data_linear</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">trace_linear</span><span class="p">,</span>
                                    <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">pcc_linear</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">model_linear</span></code> faithfully gives us a linear growth rate as shown
<a class="reference internal" href="#fig-baby-length-linear"><span class="std std-numref">Fig. 4.2</span></a>, estimating that babies will grow at
the same rate of around 1.4 cm in each month of their observed
childhood. However, it likely does not come as a surprise to you that
humans do not grow at the same rate their entire lives and that they
tend to grow more rapidly in the earlier stages of life. In other words
the relationship between age and length is nonlinear. Looking more
closely at <a class="reference internal" href="#fig-baby-length-linear"><span class="std std-numref">Fig. 4.2</span></a> we can see some issues with
the linear trend and the underlying data. The model tends to
overestimate the length of babies close to 0 months of age, and over
estimate length at 10 months of age, and then once again underestimate
at 25 months of age. We asked for a straight line and we got a straight
line even if the fit is not all that great.</p>
<div class="figure align-default" id="fig-baby-length-linear">
<a class="reference internal image-reference" href="../_images/Baby_Length_Linear_Fit.png"><img alt="../_images/Baby_Length_Linear_Fit.png" src="../_images/Baby_Length_Linear_Fit.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.2 </span><span class="caption-text">A linear prediction of baby length, where the mean is the blue line, the
dark gray is the 50% highest density interval of the posterior
predictive and the light gray is the 94% highest density interval of the
posterior predictive. The highest density interval around the mean line
of fit covers most of the data points despite the predictions tend to be
either biased high in the early months, 0 to 3, as well as late months,
22 to 25, and biased low in the middle at months 10 to 15.</span><a class="headerlink" href="#fig-baby-length-linear" title="Permalink to this image">¶</a></p>
</div>
<p>Thinking back to our model choices, we still believe that at any age, or
vertical slice of the observed data, the distribution of baby lengths
being Gaussian-like, but the relationship between the month and mean
length is nonlinear. Specifically, we decide that the nonlinearity
generally follows the shape of a square root transformation on the month
covariate which we write in <code class="docutils literal notranslate"><span class="pre">model_sqrt</span></code> in Code Block
<a class="reference internal" href="#babies-transformed"><span class="std std-ref">babies_transformed</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="babies-transformed">
<div class="code-block-caption"><span class="caption-number">Listing 4.3 </span><span class="caption-text">babies_transformed</span><a class="headerlink" href="#babies-transformed" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_baby_sqrt</span><span class="p">:</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Month&quot;</span><span class="p">]))</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">length</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Length&quot;</span><span class="p">])</span>
    <span class="n">inf_data_sqrt</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">4000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-baby-length-non-linear">
<a class="reference internal image-reference" href="../_images/Baby_Length_Sqrt_Fit.png"><img alt="../_images/Baby_Length_Sqrt_Fit.png" src="../_images/Baby_Length_Sqrt_Fit.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Linear prediction with transformed covariate. On the left the x-axes is
untransformed and on the right transformed. The linearization of the
nonlinear growth rate is visible on the transformed axes on the right.</span><a class="headerlink" href="#fig-baby-length-non-linear" title="Permalink to this image">¶</a></p>
</div>
<p>Plotting the fit of the means, along with bands representing the highest
density interval of the expected length, yields
<a class="reference internal" href="#fig-baby-length-non-linear"><span class="std std-numref">Fig. 4.3</span></a>, in which the means tends to fit
the curve of the observed relationship. In addition to this visual check
we can also use <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> to verify the ELPD value for the nonlinear
model. In your own analysis you can use any transformation function you
would like. As with any model the important bit is to be able to justify
your choice whatever it may be, and verify your results are reasonable
using visual and numerical checks.</p>
</div>
<div class="section" id="varying-uncertainty">
<span id="id3"></span><h2><span class="section-number">4.2. </span>Varying Uncertainty<a class="headerlink" href="#varying-uncertainty" title="Permalink to this headline">¶</a></h2>
<p>Thus far we have used linear models to model the mean of <span class="math notranslate nohighlight">\(Y\)</span> while
assuming the variance of the residuals <a class="footnote-reference brackets" href="#id27" id="id4">1</a> is constant along the range
of the response. However, this assumption of fixed variance is a
modeling choice that may not be adequate. To account for changing
uncertainty we can extend Equation
<a class="reference internal" href="#equation-eq-covariate-transformation-regression">(4.1)</a> into:</p>
<div class="math notranslate nohighlight" id="equation-eq-varying-variance">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-eq-varying-variance" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \mu =&amp; \beta_0 + \beta_1 f_1(X_1) + \dots + \beta_m f_m(X_m) \\
    \sigma =&amp; \delta_0 + \delta_1 g_1(X_1) + \dots + \delta_m g_m(X_m) \\
Y \sim&amp; \mathcal{N}(\mu, \sigma)
\end{split}\end{split}\]</div>
<p>This second line estimating <span class="math notranslate nohighlight">\(\sigma\)</span> is very similar to our linear term
which models the mean. We can use linear models to model parameters
other than the mean/location parameter. For a concrete example let us
expand <code class="docutils literal notranslate"><span class="pre">model_sqrt</span></code> defined in Code Block
<a class="reference internal" href="#babies-transformed"><span class="std std-ref">babies_transformed</span></a>. We now assume
that when children are young their lengths tend to cluster closely
together, but as they age their lengths tend to become more dispersed.</p>
<div class="literal-block-wrapper docutils container" id="babies-varying-variance">
<div class="code-block-caption"><span class="caption-number">Listing 4.4 </span><span class="caption-text">babies_varying_variance</span><a class="headerlink" href="#babies-varying-variance" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_baby_vv</span><span class="p">:</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    
    <span class="c1"># Additional variance terms</span>
    <span class="n">δ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;δ&quot;</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Month&quot;</span><span class="p">]))</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="n">δ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">δ</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Month&quot;</span><span class="p">])</span>

    <span class="n">length</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;length&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">babies</span><span class="p">[</span><span class="s2">&quot;Length&quot;</span><span class="p">])</span>
    
    <span class="n">trace_baby_vv</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">target_accept</span><span class="o">=</span><span class="mf">.95</span><span class="p">)</span>
    <span class="n">ppc_baby_vv</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_baby_vv</span><span class="p">,</span>
                                                 <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;length&quot;</span><span class="p">,</span> <span class="s2">&quot;σ&quot;</span><span class="p">])</span>
    <span class="n">inf_data_baby_vv</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">trace_baby_vv</span><span class="p">,</span>
                                     <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">ppc_baby_vv</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>To model increasing dispersion of the length of as the observed children
get older we changed our definition of <span class="math notranslate nohighlight">\(\sigma\)</span> from a fixed value to a
value that varies as a function of age. In other words we change the
model assumption from <strong>homoscedastic</strong>, that is having constant
variance, to <strong>heteroscedastic</strong>, that is having varying variance. In
our model, defined in Code Block
<a class="reference internal" href="#babies-varying-variance"><span class="std std-ref">babies_varying_variance</span></a> all we
need to do is change the expression defining <span class="math notranslate nohighlight">\(\sigma\)</span> of our model and
the PPL handle the estimation for us . The results of this model are
plotted in <a class="reference internal" href="#fig-baby-length-sqrt-vv-fit-include-error"><span class="std std-numref">Fig. 4.4</span></a>.</p>
<div class="figure align-default" id="fig-baby-length-sqrt-vv-fit-include-error">
<a class="reference internal image-reference" href="../_images/Baby_Length_Sqrt_VV_Fit_Include_Error.png"><img alt="../_images/Baby_Length_Sqrt_VV_Fit_Include_Error.png" src="../_images/Baby_Length_Sqrt_VV_Fit_Include_Error.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Two plots showing parameter fits of baby month versus length. In the top
plot the expected mean prediction, represented with a blue line, is
identical to <a class="reference internal" href="#fig-baby-length-non-linear"><span class="std std-numref">Fig. 4.3</span></a>, however, the HDI
intervals of the posterior are non-constant. The bottom graph plots the
expected error estimate <span class="math notranslate nohighlight">\(\sigma\)</span> as a function of age in months. Note
how the expected estimate of error increases as months increase.</span><a class="headerlink" href="#fig-baby-length-sqrt-vv-fit-include-error" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="interaction-effects">
<span id="id5"></span><h2><span class="section-number">4.3. </span>Interaction Effects<a class="headerlink" href="#interaction-effects" title="Permalink to this headline">¶</a></h2>
<p>In all our models thus far, we have assumed the effect of one covariate
to the response variable is independent of any other covariates. This is
not always the case. Consider a situation where we want to model ice
cream sales for a particular town. We might say if there are many ice
cream shops, more ice cream is available so we expect a large volume of
ice cream purchases. But if this town in a cold climate with an average
daily temperature of -5 degrees Celsius, we doubt there would be many
sales of ice cream. However, in the converse scenario if the town was in
a hot desert with average temperature of 30 degrees Celsius, but there
are no ice cream stores, sales of ice cream would also be low. It is
only when there is both hot weather <em>and</em> there are many places to buy
ice cream that we expect an increased volume of sales. Modeling this
kind of joint phenomena requires that we introduce an <em>interaction
effect</em>, where the effect of one covariate on the output variable
depends on the value of other covariates. Thus, if we assume covariates
to contribute independently (as in a standard linear regression model),
we will not be able to fully explain the phenomena. We can express an
interaction effect as:</p>
<div class="math notranslate nohighlight" id="equation-eq-interaction-effect">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-eq-interaction-effect" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \mu =&amp; \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1X_2\\
    Y \sim&amp; \mathcal{N}(\mu, \sigma)
\end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_3\)</span> is the coefficient for the interaction term <span class="math notranslate nohighlight">\(X_1X_2\)</span>.
There are other ways to introduce interactions but computing the
products of original covariates is a very widely used option. Now that
we have defined what an interaction effect is we can by contrast define
a main effect, as the effect of one covariate on the dependent variable
while ignoring all other covariates.</p>
<p>To illustrate let us use another example where we model the amount of
tip a diner leaves as a function of the total bill in Code Block
<a class="reference internal" href="#tips-no-interaction"><span class="std std-ref">tips_no_interaction</span></a>. This sounds
reasonable as the amount of the tip is generally calculated as a
percentage of the total bill with the exact percentage varying by
different factors like the kind of place you are eating, the quality of
the service, the country you are living, etc. In this example we are
going to focus on the difference in tip amount from smokers versus
non-smokers. In particular, we will study if there is an interaction
effect between smoking and the total bill amount <a class="footnote-reference brackets" href="#id28" id="id6">2</a>. Just like Model
<a class="reference internal" href="chp_03.html#penguin-mass-multi"><span class="std std-ref">penguin_mass_multi</span></a> we can include
smokers as an independent categorical variable in our regression.</p>
<div class="literal-block-wrapper docutils container" id="tips-no-interaction">
<div class="code-block-caption"><span class="caption-number">Listing 4.5 </span><span class="caption-text">tips_no_interaction</span><a class="headerlink" href="#tips-no-interaction" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tips_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/tips.csv&quot;</span><span class="p">)</span>
<span class="n">tips</span> <span class="o">=</span> <span class="n">tips_df</span><span class="p">[</span><span class="s2">&quot;tip&quot;</span><span class="p">]</span>
<span class="n">total_bill_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">tips_df</span><span class="p">[</span><span class="s2">&quot;total_bill&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">tips_df</span><span class="p">[</span><span class="s2">&quot;total_bill&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>  
<span class="n">smoker</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">tips_df</span><span class="p">[</span><span class="s2">&quot;smoker&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">codes</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_no_interaction</span><span class="p">:</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="p">(</span><span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span>
         <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">total_bill_c</span> <span class="o">+</span> 
         <span class="n">β</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">smoker</span><span class="p">)</span>

    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">tips</span><span class="p">)</span>
    <span class="n">trace_no_interaction</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let us also create a model where we include an interaction term in Code
Block <a class="reference internal" href="#tips-interaction"><span class="std std-ref">tips_interaction</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="tips-interaction">
<div class="code-block-caption"><span class="caption-number">Listing 4.6 </span><span class="caption-text">tips_interaction</span><a class="headerlink" href="#tips-interaction" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_interaction</span><span class="p">:</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="p">(</span><span class="n">β</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
       <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">total_bill_c</span>
       <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">smoker</span>
       <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">smoker</span> <span class="o">*</span> <span class="n">total_bill_c</span>
        <span class="p">)</span>

    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> <span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">tips</span><span class="p">)</span>
    <span class="n">trace_interaction</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-smoker-tip-interaction">
<a class="reference internal image-reference" href="../_images/Smoker_Tip_Interaction.png"><img alt="../_images/Smoker_Tip_Interaction.png" src="../_images/Smoker_Tip_Interaction.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.5 </span><span class="caption-text">Plots of linear estimates from our two tips models. On the right we show
the non-interaction estimate from Code Block
<a class="reference internal" href="#tips-no-interaction"><span class="std std-ref">tips_no_interaction</span></a>, where the
estimated lines are parallel. On the left we show our model from Code
Block <a class="reference internal" href="#tips-no-interaction"><span class="std std-ref">tips_no_interaction</span></a> that
includes an interaction term between smoker or non-smoker and bill
amount. In the interaction model the slopes between the groups are
allowed to vary due to the added interaction term.</span><a class="headerlink" href="#fig-smoker-tip-interaction" title="Permalink to this image">¶</a></p>
</div>
<p>The difference is visible in <a class="reference internal" href="#fig-smoker-tip-interaction"><span class="std std-numref">Fig. 4.5</span></a>.
Comparing the non-interaction model on the left and the interaction on
the right, the mean fitted lines are no longer parallel, the slopes for
smokers and non-smokers are different! By introducing an interaction we
are building a model that is effectively splitting the data, in this
example into two categories, smokers and non-smokers. You may be
thinking that it is a better idea to split the data manually and fit two
separate models, one for the smokers and one for the non-smokers. Well,
not so fast. One of the benefits of using interactions is that we are
using all the available data to fit a single model, increasing the
accuracy of the estimated parameters. For example, notice that by using
a single model we are assuming that <span class="math notranslate nohighlight">\(\sigma\)</span> is not affected by the
variable <code class="docutils literal notranslate"><span class="pre">smoker</span></code> and thus <span class="math notranslate nohighlight">\(\sigma\)</span> is estimated from both smokers and
non-smokers, helping us to get a better estimation of this parameter.
Another benefit is that we get an estimate of the size effect of the
interaction. If we just split the data we are implicitly assuming the
interaction is exactly 0, by modeling the interaction we get an estimate
about how strong the interaction is. Finally, building a model with and
without interactions for the same data to make easier to compare models
using LOO. If we split the data we end-up with different models
evaluated on different data, instead of different models evaluated on
the same data, which is a requisite for using LOO. So in summary, while
the primary difference in interaction effect models is flexibility in
modeling different slopes per group, there are many additional benefits
that arise from modeling all the data together.</p>
</div>
<div class="section" id="robust-regression">
<span id="id7"></span><h2><span class="section-number">4.4. </span>Robust Regression<a class="headerlink" href="#robust-regression" title="Permalink to this headline">¶</a></h2>
<p>Outliers, as the name suggests, are observations that lie outside of the
range “reasonable expectation”. Outliers are undesirable, as one, or
few, of these data points could change the parameter estimation of a
model significantly. There are a variety of suggested formal methods
<span id="id8">[<a class="reference internal" href="references.html#id119">42</a>]</span> of handling outliers, but in practice how outliers are
handled is a choice a statistician has to make (as even the choice of a
formal method is subjective). In general though there are at least two
ways to address outliers. One is removing data using some predefined
criteria, like 3 standard deviations or 1.5 times the interquartile
range. Another strategy is choosing a model that can handle outliers and
still provide useful results. In regression the latter are typically
referred to as robust regression models, specifically to note these
models are less sensitive to observations away from the bulk of the
data. Technically speaking, robust regression are methods designed to be
less affected by violations of assumptions by the underlying
data-generating process. In Bayesian regression one example is changing
the likelihood from a Gaussian distribution to a Student’s
t-distribution.</p>
<p>Recall that Gaussian distributions are defined by two parameters
typically known as location <span class="math notranslate nohighlight">\(\mu\)</span> and scale <span class="math notranslate nohighlight">\(\sigma\)</span>. These parameters
control the mean and standard deviation of the Gaussian distribution.
Student’s t-distributions also have one parameter for the location and
scale respectively <a class="footnote-reference brackets" href="#id29" id="id9">3</a>. However, there is an additional parameter,
typically known as degrees of freedom <span class="math notranslate nohighlight">\(\nu\)</span>. This parameter controls the
weight of the tails of the Student’s t-distribution, as shown in
<a class="reference internal" href="#fig-studentt-normal-comparison"><span class="std std-numref">Fig. 4.6</span></a>. Comparing the 3 Student’s
t-distributions against each other and the Normal distribution, the key
difference is the proportion of the density in the tails versus the
proportion at the bulk of the distributions. When <span class="math notranslate nohighlight">\(\nu\)</span> is small there
is more mass distributed in the tails, as the value of <span class="math notranslate nohighlight">\(\nu\)</span> increases
the proportion of density concentrated towards the bulk also increases
and the Student’s t-distribution becomes closer and closer to the
Gaussian. Practically speaking what this means is that values farther
from the mean are more likely to occur when <span class="math notranslate nohighlight">\(\nu\)</span> is small. Which
provides robustness to outliers when substituting a Gaussian likelihood
with a Student’s t-distribution.</p>
<div class="figure align-default" id="fig-studentt-normal-comparison">
<a class="reference internal image-reference" href="../_images/StudentT_Normal_Comparison.png"><img alt="../_images/StudentT_Normal_Comparison.png" src="../_images/StudentT_Normal_Comparison.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.6 </span><span class="caption-text">Normal distribution, in blue, compared to 3 Student’s t-distributions
with varying <span class="math notranslate nohighlight">\(\nu\)</span> parameters. The location and scale parameters are all
identical, which isolates the effect <span class="math notranslate nohighlight">\(\nu\)</span> has on the tails of the
distribution. Smaller values of <span class="math notranslate nohighlight">\(\nu\)</span> put more density into the tails of
the distribution.</span><a class="headerlink" href="#fig-studentt-normal-comparison" title="Permalink to this image">¶</a></p>
</div>
<p>This can be shown in an example. Say you own a restaurant in Argentina
and you sell empanadas <a class="footnote-reference brackets" href="#id30" id="id10">4</a>. Over time you have collected data on the
number of customers per day and the total amount of Argentine pesos your
restaurant has earned, as shown in <a class="reference internal" href="#fig-empanada-scatter-plot"><span class="std std-numref">Fig. 4.7</span></a>.
Most of the data points fall along a line, except during a couple of
days where the number of empanadas sold per customer is much higher than
the surrounding data points. These may be days of big celebration such
as the 25th of May or 9th of July <a class="footnote-reference brackets" href="#id31" id="id11">5</a>, where people are consuming more
empanadas than usual.</p>
<div class="figure align-default" id="fig-empanada-scatter-plot">
<a class="reference internal image-reference" href="../_images/Empanada_Scatter_Plot.png"><img alt="../_images/Empanada_Scatter_Plot.png" src="../_images/Empanada_Scatter_Plot.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.7 </span><span class="caption-text">Simulated data of number of customers plotted against the pesos
returned. The 5 dots at the top of the chart are considered outliers.</span><a class="headerlink" href="#fig-empanada-scatter-plot" title="Permalink to this image">¶</a></p>
</div>
<p>Regardless of the outliers, we want to estimate the relationship between
our customers and revenue. When plotting the data a linear regression
seems appropriate, such as the one written in Code Block
<a class="reference internal" href="#non-robust-regression"><span class="std std-ref">non_robust_regression</span></a> which uses
a Gaussian likelihood. After estimating the parameters we plot the mean
regression in <a class="reference internal" href="#fig-empanada-scatter-non-robust"><span class="std std-numref">Fig. 4.8</span></a> at two different
scales. In the lower plot note how the fitted regression line lies above
all visible data points. In <a class="reference internal" href="#tab-non-robust-regression"><span class="std std-numref">Table 4.1</span></a> we
also can see the individual parameter estimates, noting in particular
<span class="math notranslate nohighlight">\(\sigma\)</span> which at a mean value of 574 seems high when compared to the
plot of the nominal data. With a Normal likelihood the posterior
distribution has to “stretch” itself over the nominal observations and
the 5 outliers, which affects the estimates. Additionally, note how the
estimate of <span class="math notranslate nohighlight">\(\sigma\)</span> is quite wide compared to the plotted data in
<a class="reference internal" href="#fig-empanada-scatter-non-robust"><span class="std std-numref">Fig. 4.8</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="non-robust-regression">
<div class="code-block-caption"><span class="caption-number">Listing 4.7 </span><span class="caption-text">non_robust_regression</span><a class="headerlink" href="#non-robust-regression" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_non_robust</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span> <span class="o">*</span> <span class="n">empanadas</span><span class="p">[</span><span class="s2">&quot;customers&quot;</span><span class="p">])</span>

    <span class="n">sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;sales&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">empanadas</span><span class="p">[</span><span class="s2">&quot;sales&quot;</span><span class="p">])</span>
    
    <span class="n">inf_data_non_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-empanada-scatter-non-robust">
<a class="reference internal image-reference" href="../_images/Empanada_Scatter_Non_Robust.png"><img alt="../_images/Empanada_Scatter_Non_Robust.png" src="../_images/Empanada_Scatter_Non_Robust.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.8 </span><span class="caption-text">A plot of the data, fitted regression line, and 94% HDI of
<code class="docutils literal notranslate"><span class="pre">model_non_robust</span></code> from Code Block
<a class="reference internal" href="#non-robust-regression"><span class="std std-ref">non_robust_regression</span></a> at two
scales, the top including the outliers, and the bottom focused on the
regression itself. The systemic bias is more evident in bottom plot as
the mean regression line is estimated to be above the nominal data
points.</span><a class="headerlink" href="#fig-empanada-scatter-non-robust" title="Permalink to this image">¶</a></p>
</div>
<table class="table" id="tab-non-robust-regression">
<caption><span class="caption-number">Table 4.1 </span><span class="caption-text">Estimate of parameters for model non_robust_regression.</span><a class="headerlink" href="#tab-non-robust-regression" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>207.1</p></td>
<td><p>2.9</p></td>
<td><p>201.7</p></td>
<td><p>212.5</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td><p>2951.1</p></td>
<td><p>25.0</p></td>
<td><p>2904.5</p></td>
<td><p>2997.7</p></td>
</tr>
</tbody>
</table>
<p>We can run the same regression again but this time using the Student’s
t-distribution as likelihood, shown in Code Block
<a class="reference internal" href="#code-robust-regression"><span class="std std-ref">code_robust_regression</span></a>. Note that the
dataset has not changed and the outliers are still included. When
inspecting the fitted regression line in
<a class="reference internal" href="#fig-empanada-scatter-robust"><span class="std std-numref">Fig. 4.9</span></a> we can see that the fit falls
between the nominal observed data points, closer to where we would
expect. Inspecting the mean parameter estimates in <a class="reference internal" href="#tab-robust-regression"><span class="std std-numref">Table 4.2</span></a>
note the addition of the extra parameter
<span class="math notranslate nohighlight">\(\nu\)</span>. Furthermore we can see that the estimate of <span class="math notranslate nohighlight">\(\sigma\)</span> has fallen
substantially from <span class="math notranslate nohighlight">\(\approx\)</span> 2951 pesos in the non-robust regression to
<span class="math notranslate nohighlight">\(\approx\)</span> 152 pesos in the robust regression, note how  this new estiamte is logically reasonable
compared to the plotted data. The change in likelihood
distribution shows that there is enough flexibility in the Student’s
t-distribution to reasonably model the nominal data, despite the
presence of outliers.</p>
<div class="literal-block-wrapper docutils container" id="code-robust-regression">
<div class="code-block-caption"><span class="caption-number">Listing 4.8 </span><span class="caption-text">code_robust_regression</span><a class="headerlink" href="#code-robust-regression" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_robust</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ν</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;ν&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span> <span class="o">*</span> <span class="n">empanadas</span><span class="p">[</span><span class="s2">&quot;customers&quot;</span><span class="p">])</span>
    
    <span class="n">sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s2">&quot;sales&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">ν</span><span class="p">,</span>
                        <span class="n">observed</span><span class="o">=</span><span class="n">empanadas</span><span class="p">[</span><span class="s2">&quot;sales&quot;</span><span class="p">])</span>

    <span class="n">inf_data_robust</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<table class="table" id="tab-robust-regression">
<caption><span class="caption-number">Table 4.2 </span><span class="caption-text">Estimate of parameters for model robust_regression.</span><a class="headerlink" href="#tab-robust-regression" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\beta\)</span></p></td>
<td><p>179.6</p></td>
<td><p>0.3</p></td>
<td><p>179.1</p></td>
<td><p>180.1</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td><p>152.3</p></td>
<td><p>13.9</p></td>
<td><p>127.1</p></td>
<td><p>179.5</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\nu\)</span></p></td>
<td><p>1.3</p></td>
<td><p>0.2</p></td>
<td><p>1.0</p></td>
<td><p>1.6</p></td>
</tr>
</tbody>
</table>
<div class="figure align-default" id="fig-empanada-scatter-robust">
<a class="reference internal image-reference" href="../_images/Empanada_Scatter_Robust.png"><img alt="../_images/Empanada_Scatter_Robust.png" src="../_images/Empanada_Scatter_Robust.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.9 </span><span class="caption-text">A plot of the data, fitted regression line of <code class="docutils literal notranslate"><span class="pre">model_robust</span></code> and 94% HDI
from Code Block <a class="reference internal" href="#code-robust-regression"><span class="std std-ref">code_robust_regression</span></a>.
The outliers are not plotted but are present in the data. The fitted
line falls within the range of the nominal data points, particularly if
compared to <a class="reference internal" href="#fig-empanada-scatter-non-robust"><span class="std std-numref">Fig. 4.8</span></a>.</span><a class="headerlink" href="#fig-empanada-scatter-robust" title="Permalink to this image">¶</a></p>
</div>
<p>In this example the “outliers” are actually part of the problem we want
to model, in the sense that they are not measurement error, data entry
errors etc, but observations that can actually happened under certain
conditions. Hence, it is ok to treat them as outliers if we want to
model the average number of empanadas on a “regular” day, but it will
lead to a disaster if we use this average to make plans for the next
25th of May or 9th of July. Therefore in this example the robust linear
regression model is a trick to avoid explicitly modeling the high sales
day which, if needed, will be probably better modeled using mixture
model or a multilevel model.</p>
<div class="admonition-model-adaptions-for-data-considerations admonition">
<p class="admonition-title">Model adaptions for data considerations</p>
<p>Changing the likelihood to accommodate for robustness is just one example of a modification we can
make to the model to better suit the observed data. For example, in
detecting radioactive particle emission a zero count can arise because
of a faulty sensor<span id="id12">[<a class="reference internal" href="references.html#id135">43</a>]</span> (or some other measuring
problem), or because there was actually no event to register. This
unknown source of variation has the effect of <em>inflating</em> the count of
zeros. A useful aid for this kind of problem is the class of models
aptly named zero-inflated models which estimate the combined data
generating process. For example, a Poisson likelihood, which will
generally be a starting point for modeling counts, can be expanded into
a zero-inflated Poisson likelihood. With such a likelihood we can better
separate the counts generated from a Poisson process from those
generated from the <em>excess zero generating process</em>.</p>
<p>Zero-inflated models are an example of handling a mixture of data, in
which observations come from two or more groups, without knowledge of
which observation belongs to which group. Actually, we can express
another type of robust regression using a mixture likelihood, which
assigns a latent label (outlier or not) to each data point.</p>
<p>In all of these situations and many more the bespoke nature of Bayesian
models allows the modeler the flexibility to create a model that fits
the situation, rather than having to fit a situation to a predefined
model.</p>
</div>
</div>
<div class="section" id="pooling-multilevel-models-and-mixed-effects">
<span id="multilevel-models"></span><h2><span class="section-number">4.5. </span>Pooling, Multilevel Models, and Mixed Effects<a class="headerlink" href="#pooling-multilevel-models-and-mixed-effects" title="Permalink to this headline">¶</a></h2>
<p>Often we have dataset that contain additional nested structures among
the predictors, which gives some hierarchical way to group the data. We
can also think of it as different data generation processes. We are
going to use an example to illustrate this. Let us say you work at a
restaurant company which sells salads. This company has a
long-established business in some geographic markets and, due to
customer demand, has just opened a location in a new market as well. You
need to predict how many US dollars the restaurant location in this new
market will earn each day for financial planning purposes. You have two
datasets, 3 days of data for the sales of salads, as well as roughly a
year’s worth of data on pizza and sandwich sales in the same market. The
(simulated) data is shown in <a class="reference internal" href="#fig-restaurant-order-scatter"><span class="std std-numref">Fig. 4.10</span></a>.</p>
<div class="figure align-default" id="fig-restaurant-order-scatter">
<a class="reference internal image-reference" href="../_images/Restaurant_Order_Scatter.png"><img alt="../_images/Restaurant_Order_Scatter.png" src="../_images/Restaurant_Order_Scatter.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.10 </span><span class="caption-text">A simulated dataset for a real world scenario. In this case an
organization has 3 data points for the daily sales of salads, but has
lots of data on the sales of pizza and sandwiches.</span><a class="headerlink" href="#fig-restaurant-order-scatter" title="Permalink to this image">¶</a></p>
</div>
<p>From both expert knowledge and data, there is agreement that there are
similarities between the sales of these 3 food categories. They all
appeal to the same type of customer, represent the same “food category”
of <em>quick to go</em> food but they are not exactly the same either. In the
following sections we will discuss how to model this
<em>similarity-yet-disimilarity</em> but let us start with the simpler case,
all groups are unrelated to each other.</p>
<div class="section" id="unpooled-parameters">
<span id="id13"></span><h3><span class="section-number">4.5.1. </span>Unpooled Parameters<a class="headerlink" href="#unpooled-parameters" title="Permalink to this headline">¶</a></h3>
<p>We can create a regression model where we treat each group, in this case
food category, as completely separated from the others. This is
identical to running a separate regression for each category, and that
is why we call it unpooled regression. The only difference to run
separated regression is that we are writing a single model and
estimating all coefficients at the same time. The relationship between
parameters and groups is visually represented in
<a class="reference internal" href="#fig-unpooled-model"><span class="std std-numref">Fig. 4.11</span></a> and in mathematical notation in Equation
<a class="reference internal" href="#equation-eq-unpooled-regression">(4.4)</a>, where <span class="math notranslate nohighlight">\(j\)</span> is an index identifying each
separated group.</p>
<div class="figure align-default" id="fig-unpooled-model">
<a class="reference internal image-reference" href="../_images/unpooled_model.png"><img alt="../_images/unpooled_model.png" src="../_images/unpooled_model.png" style="width: 5.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.11 </span><span class="caption-text">An unpooled model where each group of observations, <span class="math notranslate nohighlight">\(y_1, y_2, ..., y_j\)</span>
has its own set of parameters, independent from any other group.</span><a class="headerlink" href="#fig-unpooled-model" title="Permalink to this image">¶</a></p>
</div>
<div class="math notranslate nohighlight" id="equation-eq-unpooled-regression">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-eq-unpooled-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\beta_{mj} \sim&amp; \overbrace{\mathcal{N}(\mu_{\beta m}, \sigma_{\beta m})}^{\text{Group-specific}}\\
\sigma_{j} \sim&amp; \overbrace{\mathcal{HN}(\sigma_{\sigma})}^{\text{Group-specific}}\\
\mu_{j} =&amp; \beta_{1j} X_1 + \dots + \beta_{mj} X_m \\
Y \sim&amp; \mathcal{N}(\mu_{j}, \sigma_{j})
\end{split}\end{split}\]</div>
<p>The parameters are labeled as <em>group-specific</em> parameters to denote
there is one dedicated to each group. The unpooled PyMC3 model, and some
data cleaning, is shown in Code Block
<a class="reference internal" href="#model-sales-unpooled"><span class="std std-ref">model_sales_unpooled</span></a> and the block
representation is shown in
<a class="reference internal" href="#fig-salad-sales-basic-regression-model-unpooled"><span class="std std-numref">Fig. 4.12</span></a>. We do not
include an intercept parameter for the simple reason that if a
restaurant has zero customers, total sales will also be zero, so there
is neither any interest nor any need for the extra parameter.</p>
<div class="literal-block-wrapper docutils container" id="model-sales-unpooled">
<div class="code-block-caption"><span class="caption-number">Listing 4.9 </span><span class="caption-text">model_sales_unpooled</span><a class="headerlink" href="#model-sales-unpooled" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">customers</span> <span class="o">=</span> <span class="n">sales_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;customers&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">sales_observed</span> <span class="o">=</span> <span class="n">sales_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;sales&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">food_category</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">sales_df</span><span class="p">[</span><span class="s2">&quot;Food_Category&quot;</span><span class="p">])</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_sales_unpooled</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="n">food_category</span><span class="o">.</span><span class="n">codes</span><span class="p">]</span> <span class="o">*</span><span class="n">customers</span><span class="p">)</span>
    
    <span class="n">sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;sales&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">[</span><span class="n">food_category</span><span class="o">.</span><span class="n">codes</span><span class="p">],</span>
                      <span class="n">observed</span><span class="o">=</span><span class="n">sales_observed</span><span class="p">)</span>
    
    <span class="n">trace_sales_unpooled</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">target_accept</span><span class="o">=</span><span class="mf">.9</span><span class="p">)</span>
    <span class="n">inf_data_sales_unpooled</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_sales_unpooled</span><span class="p">,</span> 
        <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;β_dim_0&quot;</span><span class="p">:</span><span class="n">food_category</span><span class="o">.</span><span class="n">categories</span><span class="p">,</span>
                <span class="s2">&quot;σ_dim_0&quot;</span><span class="p">:</span><span class="n">food_category</span><span class="o">.</span><span class="n">categories</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-salad-sales-basic-regression-model-unpooled">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_Model_Unpooled.png"><img alt="../_images/Salad_Sales_Basic_Regression_Model_Unpooled.png" src="../_images/Salad_Sales_Basic_Regression_Model_Unpooled.png" style="width: 3.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.12 </span><span class="caption-text">A diagram of <code class="docutils literal notranslate"><span class="pre">model_sales_unpooled</span></code>. Note how the box around parameters
<span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> has a three in the lower right, indicating that the
model estimated 3 parameters each for <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</span><a class="headerlink" href="#fig-salad-sales-basic-regression-model-unpooled" title="Permalink to this image">¶</a></p>
</div>
<p>After sampling from <code class="docutils literal notranslate"><span class="pre">model_sales_unpooled</span></code> we can create forest plots of
the parameter estimates as shown in Figures
<a class="reference internal" href="#fig-salad-sales-basic-regression-forestplot-beta"><span class="std std-numref">Fig. 4.13</span></a> and
<a class="reference internal" href="#fig-salad-sales-basic-regression-forestplot-sigma"><span class="std std-numref">Fig. 4.14</span></a>. Note how
the estimate of <span class="math notranslate nohighlight">\(\sigma\)</span> for the salad food category is quite wide
compared to the sandwich and pizza groups. This is what we would expect
from our unpooled model when we have large amounts of data for some of
the categories, but much less for others.</p>
<div class="figure align-default" id="fig-salad-sales-basic-regression-forestplot-beta">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_ForestPlot_beta.png"><img alt="../_images/Salad_Sales_Basic_Regression_ForestPlot_beta.png" src="../_images/Salad_Sales_Basic_Regression_ForestPlot_beta.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.13 </span><span class="caption-text">Forest plot of the <span class="math notranslate nohighlight">\(\beta\)</span> parameter estimates <code class="docutils literal notranslate"><span class="pre">model_sales_unpooled</span></code>.
As expected the estimate of the <span class="math notranslate nohighlight">\(\beta\)</span> coefficient for the salads group
is the widest as this group has the least amount of data.</span><a class="headerlink" href="#fig-salad-sales-basic-regression-forestplot-beta" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-salad-sales-basic-regression-forestplot-sigma">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_ForestPlot_sigma.png"><img alt="../_images/Salad_Sales_Basic_Regression_ForestPlot_sigma.png" src="../_images/Salad_Sales_Basic_Regression_ForestPlot_sigma.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.14 </span><span class="caption-text">Forest plot of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter estimates <code class="docutils literal notranslate"><span class="pre">model_sales_unpooled</span></code>.
Like <a class="reference internal" href="#fig-salad-sales-basic-regression-forestplot-beta"><span class="std std-numref">Fig. 4.13</span></a> the
estimate of the variation of sales, <span class="math notranslate nohighlight">\(\sigma\)</span>, is largest for the salads
group as there are not as many data points relative to the pizza and
sandwich groups.</span><a class="headerlink" href="#fig-salad-sales-basic-regression-forestplot-sigma" title="Permalink to this image">¶</a></p>
</div>
<p>The unpooled model is no different than if we have created three
separated models with subsets of the data, exactly as we did in
Section <a class="reference internal" href="chp_03.html#comparing-distributions"><span class="std std-ref">Comparing Two (or More) Groups</span></a>, where the
parameters of each group were estimated separately so we can consider
the unpooled model architecture syntactic sugar for modeling independent
linear regressions of each group. More importantly now we can use the
unpooled model and the estimated parameters from it as a baseline to
compare other models in the following sections, particularly to
understand if the extra complexity is justified.</p>
</div>
<div class="section" id="pooled-parameters">
<span id="id14"></span><h3><span class="section-number">4.5.2. </span>Pooled Parameters<a class="headerlink" href="#pooled-parameters" title="Permalink to this headline">¶</a></h3>
<p>If there are unpooled parameters, you might guess there are pooled
parameters and you would be correct. As the name suggests pooled
parameters are ones where the group distinction is ignored.
Conceptually, this type of model is shown in <a class="reference internal" href="#fig-pooled-model"><span class="std std-numref">Fig. 4.15</span></a>
were each group shares the same parameters and thus we also refer to
them as common parameters.</p>
<div class="figure align-default" id="fig-pooled-model">
<a class="reference internal image-reference" href="../_images/pooled_model.png"><img alt="../_images/pooled_model.png" src="../_images/pooled_model.png" style="width: 5.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.15 </span><span class="caption-text">A pooled model where each group of observations, <span class="math notranslate nohighlight">\(y_1, y_2, ..., y_j\)</span>
shares parameters.</span><a class="headerlink" href="#fig-pooled-model" title="Permalink to this image">¶</a></p>
</div>
<p>For our restaurant example, the model is written in Equation
<a class="reference internal" href="#equation-eq-pooled-regression">(4.5)</a> and Code Block
<a class="reference internal" href="#model-sales-pooled"><span class="std std-ref">model_sales_pooled</span></a>. The GraphViz
representation is also shown in
<a class="reference internal" href="#fig-salad-sales-basic-regression-model-unpooled"><span class="std std-numref">Fig. 4.12</span></a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-pooled-regression">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-eq-pooled-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\beta \sim&amp; \overbrace{\mathcal{N}(\mu_{\beta}, \sigma_{\beta})}^{\text{Common}}\\
\sigma \sim&amp; \overbrace{\mathcal{HN}(\sigma_{\sigma})}^{\text{Common}}\\
\mu =&amp; \beta_{1} X_{1} + \dots + \beta_{m} X_{m} \\
Y \sim&amp; \mathcal{N}(\mu, \sigma)
\end{split}\end{split}\]</div>
<div class="literal-block-wrapper docutils container" id="model-sales-pooled">
<div class="code-block-caption"><span class="caption-number">Listing 4.10 </span><span class="caption-text">model_sales_pooled</span><a class="headerlink" href="#model-sales-pooled" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_sales_pooled</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span> <span class="o">*</span> <span class="n">customers</span><span class="p">)</span>
    
    <span class="n">sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;sales&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span>
                      <span class="n">observed</span><span class="o">=</span><span class="n">sales_observed</span><span class="p">)</span>
                        
    <span class="n">inf_data_sales_pooled</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-salad-sales-basic-regression-model-pooled">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_Model_Pooled.png"><img alt="../_images/Salad_Sales_Basic_Regression_Model_Pooled.png" src="../_images/Salad_Sales_Basic_Regression_Model_Pooled.png" style="width: 3.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.16 </span><span class="caption-text">Diagram of <code class="docutils literal notranslate"><span class="pre">model_sales_pooled</span></code>. Unlike
<a class="reference internal" href="#fig-salad-sales-basic-regression-model-unpooled"><span class="std std-numref">Fig. 4.12</span></a> there is only
one instance of <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</span><a class="headerlink" href="#fig-salad-sales-basic-regression-model-pooled" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-salad-sales-basic-regression-forestplot-sigma-comparison">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_ForestPlot_Sigma_Comparison.png"><img alt="../_images/Salad_Sales_Basic_Regression_ForestPlot_Sigma_Comparison.png" src="../_images/Salad_Sales_Basic_Regression_ForestPlot_Sigma_Comparison.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.17 </span><span class="caption-text">A comparison of the estimates of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter from
<code class="docutils literal notranslate"><span class="pre">model_pooled_sales</span></code> and <code class="docutils literal notranslate"><span class="pre">model_unpooled_sales</span></code>. Note how we only get
one estimate of <span class="math notranslate nohighlight">\(\sigma\)</span> that is much higher compared to the unpooled
model as the single linear fit estimated must capture the variance in
the pooled data.</span><a class="headerlink" href="#fig-salad-sales-basic-regression-forestplot-sigma-comparison" title="Permalink to this image">¶</a></p>
</div>
<p>The benefit of the pooled approach is that more data will be used to
estimate each parameter. However, this means we cannot understand each
group individually, just all food categories as a whole. Looking at
<a class="reference internal" href="#fig-salad-sales-basic-regression-scatter-pooled"><span class="std std-numref">Fig. 4.18</span></a>, our estimates
<span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are not indicative of any particular food group as
the model is grouping together data with very different scales. Compare
the value of <span class="math notranslate nohighlight">\(\sigma\)</span> with the ones from the unpooled model in
<a class="reference internal" href="#fig-salad-sales-basic-regression-forestplot-sigma-comparison"><span class="std std-numref">Fig. 4.17</span></a>.
When plotting the regression in
<a class="reference internal" href="#fig-salad-sales-basic-regression-scatter-pooled"><span class="std std-numref">Fig. 4.18</span></a> we can see
that a single line, despite being informed by more data than any single
group, fails to fit any one group well. This result implies that the
differences in the groups are too large to ignore and thus pooling the
data it is not particularly useful for our intended purpose.</p>
<div class="figure align-default" id="fig-salad-sales-basic-regression-scatter-pooled">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_Scatter_Pooled.png"><img alt="../_images/Salad_Sales_Basic_Regression_Scatter_Pooled.png" src="../_images/Salad_Sales_Basic_Regression_Scatter_Pooled.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.18 </span><span class="caption-text">Linear regression <code class="docutils literal notranslate"><span class="pre">model_sales_pooled</span></code> where all the data is pooled
together. Each of the parameters is estimated using all the data but we
end up with poor estimates of each individual group’s behavior as a 2
parameter model cannot generalize well enough to capture the nuances of
each group.</span><a class="headerlink" href="#fig-salad-sales-basic-regression-scatter-pooled" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="mixing-group-and-common-parameters">
<span id="id15"></span><h3><span class="section-number">4.5.3. </span>Mixing Group and Common Parameters<a class="headerlink" href="#mixing-group-and-common-parameters" title="Permalink to this headline">¶</a></h3>
<p>In the unpooled approach we get the benefit of preserving the
differences in our groups, and thus getting an estimated set of
parameters for each group. In the pooled approach we get the benefit of
utilizing all the data to estimate a single set of parameters, and thus
more informed, albeit more generic, estimates. Fortunately we are not
forced to pick just one option or the other. We can mix these two
concepts in a single model shown in Equation
<a class="reference internal" href="#equation-eq-multilevel-regression">(4.6)</a>. In this formulation we have decided to
keep the estimate of <span class="math notranslate nohighlight">\(\beta\)</span> group specific, or unpooled, and to use a
common, or pooled, <span class="math notranslate nohighlight">\(\sigma\)</span>. In our current example we do not have an
intercept, but in a regression that included an intercept term we would
have a similar choice, pool all the data into a single estimate, or
leave the data separated in groups for an estimate per group.</p>
<div class="math notranslate nohighlight" id="equation-eq-multilevel-regression">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-eq-multilevel-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\beta_{mj} \sim&amp; \overbrace{\mathcal{N}(\mu_{\beta m}, \sigma_{\beta m})}^{\text{Group-specific}}\\
\sigma \sim&amp; \overbrace{\mathcal{HN}(\sigma_{\sigma})}^{\text{Common}}\\
\mu_{j} =&amp; \beta_{1j} X_{1} + \dots + \beta_{m} X_{m} \\
Y \sim&amp; \mathcal{N}(\mu_{j}, \sigma)
\end{split}\end{split}\]</div>
<div class="admonition-random-and-fixed-effects-and-why-you-should-forget-these-terms admonition">
<p class="admonition-title">Random and fixed effects and why you should forget these terms</p>
<p>The parameters that are specific to each level and those that are common
across levels get different names, including random or varying effect,
or fixed or constant effect, respectively. To add to the confusion
different people may assign different meanings to these terms especially
when talking about fixed and random effects <span id="id16">[<a class="reference internal" href="references.html#id80">44</a>]</span>. If we have to
label these terms we suggest <em>common</em> and <em>group-specific</em>
<span id="id17">[<a class="reference internal" href="references.html#id118">33</a>, <a class="reference internal" href="references.html#id83">36</a>]</span>. However, as all these different
terms are widely used we recommend that you always verify the details of
the model so to avoid confusions and misunderstandings.</p>
</div>
<p>To reiterate in our sales model we are interested in pooling the data to
estimate <span class="math notranslate nohighlight">\(\sigma\)</span> as we believe there could be identical variance of the
sales of pizza, sandwiches, and salads, but we leave our estimate of
<span class="math notranslate nohighlight">\(\beta\)</span> unpooled, or independent, as we know there are differences
between the groups. With these ideas we can write our PyMC3 model as
shown in Code Block
<a class="reference internal" href="#model-sales-mixed-effect"><span class="std std-ref">model_sales_mixed_effect</span></a>, as
well as generate a graphical diagram of the model structure shown in
<a class="reference internal" href="#fig-salad-sales-basic-regression-model-multilevel"><span class="std std-numref">Fig. 4.19</span></a>. From the
model we can plot
<a class="reference internal" href="#fig-salad-sales-basic-regression-scatter-sigma-pooled-slope-unpooled"><span class="std std-numref">Fig. 4.20</span></a>
showing the estimate of fit overlaid on the data, as well as a
comparison of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter estimates from the multilevel and
unpooled models in
<a class="reference internal" href="#fig-salad-sales-forestplot-sigma-unpooled-multilevel-comparison"><span class="std std-numref">Fig. 4.21</span></a>.
These results are encouraging, for all three categories the fits looks
reasonable and for the salad group in particular it seems this model
will be able to produce plausible inferences about salad sales in this
new market.</p>
<div class="literal-block-wrapper docutils container" id="model-sales-mixed-effect">
<div class="code-block-caption"><span class="caption-number">Listing 4.11 </span><span class="caption-text">model_sales_mixed_effect</span><a class="headerlink" href="#model-sales-mixed-effect" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_pooled_sigma_sales</span><span class="p">:</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="n">food_category</span><span class="o">.</span><span class="n">codes</span><span class="p">]</span> <span class="o">*</span> <span class="n">customers</span><span class="p">)</span>
    
    <span class="n">sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;sales&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">sales_observed</span><span class="p">)</span>
    
    <span class="n">trace_pooled_sigma_sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">ppc_pooled_sigma_sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span>
        <span class="n">trace_pooled_sigma_sales</span><span class="p">)</span>

    <span class="n">inf_data_pooled_sigma_sales</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_pooled_sigma_sales</span><span class="p">,</span>
        <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">ppc_pooled_sigma_sales</span><span class="p">,</span>
        <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;β_dim_0&quot;</span><span class="p">:</span><span class="n">food_category</span><span class="o">.</span><span class="n">categories</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-salad-sales-basic-regression-model-multilevel">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_Model_Multilevel.png"><img alt="../_images/Salad_Sales_Basic_Regression_Model_Multilevel.png" src="../_images/Salad_Sales_Basic_Regression_Model_Multilevel.png" style="width: 3.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.19 </span><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">model_pooled_sigma_sales</span></code> where <span class="math notranslate nohighlight">\(\beta\)</span> is unpooled, as indicated by
the box with 3 in the right corner, and <span class="math notranslate nohighlight">\(\sigma\)</span> is pooled, as the lack
of number indicates a single parameter estimate for all groups.
[fig:Salad_Sales_Basic_Regression_Model_Multilevel]{#fig:Salad_Sales_Basic_Regression_Model_Multilevel
label=”fig:Salad_Sales_Basic_Regression_Model_Multilevel”}</span><a class="headerlink" href="#fig-salad-sales-basic-regression-model-multilevel" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-salad-sales-basic-regression-scatter-sigma-pooled-slope-unpooled">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Basic_Regression_Scatter_Sigma_Pooled_Slope_Unpooled.png"><img alt="../_images/Salad_Sales_Basic_Regression_Scatter_Sigma_Pooled_Slope_Unpooled.png" src="../_images/Salad_Sales_Basic_Regression_Scatter_Sigma_Pooled_Slope_Unpooled.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.20 </span><span class="caption-text">Linear model with 50% HDI from <code class="docutils literal notranslate"><span class="pre">model_pooled_sigma_sales</span></code>. This model is
more useful for our purposes of estimating salad sales as the slopes are
independently estimated for each group. Note how all the data is being
used to estimate the single posterior distribution of <span class="math notranslate nohighlight">\(\sigma\)</span>.
[fig:Salad_Sales_Basic_Regression_Scatter_Sigma_Pooled_Slope_Unpooled]{#fig:Salad_Sales_Basic_Regression_Scatter_Sigma_Pooled_Slope_Unpooled
label=”fig:Salad_Sales_Basic_Regression_Scatter_Sigma_Pooled_Slope_Unpooled”}</span><a class="headerlink" href="#fig-salad-sales-basic-regression-scatter-sigma-pooled-slope-unpooled" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-salad-sales-forestplot-sigma-unpooled-multilevel-comparison">
<a class="reference internal image-reference" href="../_images/Salad_Sales_ForestPlot_Sigma_Unpooled_Multilevel_Comparison.png"><img alt="../_images/Salad_Sales_ForestPlot_Sigma_Unpooled_Multilevel_Comparison.png" src="../_images/Salad_Sales_ForestPlot_Sigma_Unpooled_Multilevel_Comparison.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.21 </span><span class="caption-text">Comparison of <span class="math notranslate nohighlight">\(\sigma\)</span> from <code class="docutils literal notranslate"><span class="pre">model_pooled_sigma_sales</span></code> and
<code class="docutils literal notranslate"><span class="pre">model_pooled_sales</span></code>. Note how the estimated of <span class="math notranslate nohighlight">\(\sigma\)</span> in the
multilevel model is within the bounds of the <span class="math notranslate nohighlight">\(\sigma\)</span> estimates from the
pooled model.
[fig:Salad_Sales_ForestPlot_Sigma_Unpooled_Multilevel_Comparison]{#fig:Salad_Sales_ForestPlot_Sigma_Unpooled_Multilevel_Comparison
label=”fig:Salad_Sales_ForestPlot_Sigma_Unpooled_Multilevel_Comparison”}</span><a class="headerlink" href="#fig-salad-sales-forestplot-sigma-unpooled-multilevel-comparison" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="hierarchical-models">
<span id="id18"></span><h2><span class="section-number">4.6. </span>Hierarchical Models<a class="headerlink" href="#hierarchical-models" title="Permalink to this headline">¶</a></h2>
<p>In our data treatment thus far we have had two options for groups,
pooled where there is no distinction between groups, and unpooled where
there a complete distinction between groups. Recall though in our
motivating restaurant example we believed the parameter <span class="math notranslate nohighlight">\(\sigma\)</span> of the
3 food categories to be similar, but not exactly the same. In Bayesian
modeling we can express this idea with <em>hierarchical models</em>. In
hierarchical models the parameters are <em>partially pooled</em>. The partial
refers to the idea that groups that do not share one fixed parameter,
but share a which describes the distribution of for the parameters of
the prior itself. Conceptually this idea is shown in
<a class="reference internal" href="#fig-partial-pooled-model"><span class="std std-numref">Fig. 4.22</span></a>. Each group gets its own parameters
which are drawn from a common hyperprior distribution.</p>
<div class="figure align-default" id="fig-partial-pooled-model">
<a class="reference internal image-reference" href="../_images/partial_pooled_model.png"><img alt="../_images/partial_pooled_model.png" src="../_images/partial_pooled_model.png" style="width: 5.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.22 </span><span class="caption-text">A partially pooled model architecture where each group of observations,
<span class="math notranslate nohighlight">\(y_1, y_2, ..., y_k\)</span> has its own set of parameters, but they are not
independent as they are drawn from a common distribution.</span><a class="headerlink" href="#fig-partial-pooled-model" title="Permalink to this image">¶</a></p>
</div>
<p>Using statistical notation we can write a hierarchical model in Equation
<a class="reference internal" href="#equation-eq-hierarchical-regression">(4.7)</a>, the computational model in Code Block
<a class="reference internal" href="#model-hierarchical-sales"><span class="std std-ref">model_hierarchical_sales</span></a>, and
a graphical representation in
<a class="reference internal" href="#fig-salad-sales-hierarchial-regression-model"><span class="std std-numref">Fig. 4.23</span></a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-hierarchical-regression">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-eq-hierarchical-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\beta_{mj} \sim&amp; \mathcal{N}(\mu_{\beta m}, \sigma_{\beta m}) \\
\sigma_{h} \sim&amp; \overbrace{\mathcal{HN}(\sigma)}^{\text{Hyperprior}} \\
\sigma_{j} \sim&amp; \overbrace{\mathcal{HN}(\sigma_{h})}^{\substack{\text{Group-specific} \\ \text{pooled}}} \\
\mu_{j} =&amp; \beta_{1j} X_1 + \dots + \beta_{mj} X_m \\
Y \sim&amp; \mathcal{N}(\mu_{j},\sigma_{j})
\end{split}\end{split}\]</div>
<p>Note the addition of <span class="math notranslate nohighlight">\(\sigma_{h}\)</span> when compared to the multilevel model
in <a class="reference internal" href="#fig-salad-sales-basic-regression-model-multilevel"><span class="std std-numref">Fig. 4.19</span></a>. This is
our new hyperprior distributions that defines the possible parameters of
individual groups. We can add the hyperprior in Code Block
<a class="reference internal" href="#model-hierarchical-sales"><span class="std std-ref">model_hierarchical_sales</span></a> as
well. You may ask “could we have added a hyperprior for the <span class="math notranslate nohighlight">\(\beta\)</span>
terms as well?”, and the answer is quite simply yes we could have. But
in this case we assume that only the variance is related, which
justifying the use of partial pooling and that the slopes are completely
independent. Because this is a simulated textbook example we can plainly
make this statement and “get away with it”, in a real life scenario
more domain expertise and model comparison would be advised to justify
this claim.</p>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-sales">
<div class="code-block-caption"><span class="caption-number">Listing 4.12 </span><span class="caption-text">model_hierarchical_sales</span><a class="headerlink" href="#model-hierarchical-sales" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_hierarchical_sales</span><span class="p">:</span>
    <span class="n">σ_hyperprior</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ_hyperprior&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s2">&quot;σ&quot;</span><span class="p">,</span> <span class="n">σ_hyperprior</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;β&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;μ&quot;</span><span class="p">,</span> <span class="n">β</span><span class="p">[</span><span class="n">food_category</span><span class="o">.</span><span class="n">codes</span><span class="p">]</span> <span class="o">*</span> <span class="n">customers</span><span class="p">)</span>
    
    <span class="n">sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;sales&quot;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">σ</span><span class="p">[</span><span class="n">food_category</span><span class="o">.</span><span class="n">codes</span><span class="p">],</span>
                      <span class="n">observed</span><span class="o">=</span><span class="n">sales_observed</span><span class="p">)</span>
    
    <span class="n">trace_hierarchical_sales</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">target_accept</span><span class="o">=</span><span class="mf">.9</span><span class="p">)</span>
    
    <span class="n">inf_data_hierarchical_sales</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_pymc3</span><span class="p">(</span>
        <span class="n">trace</span><span class="o">=</span><span class="n">trace_hierarchical_sales</span><span class="p">,</span> 
        <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;β_dim_0&quot;</span><span class="p">:</span><span class="n">food_category</span><span class="o">.</span><span class="n">categories</span><span class="p">,</span>
                <span class="s2">&quot;σ_dim_0&quot;</span><span class="p">:</span><span class="n">food_category</span><span class="o">.</span><span class="n">categories</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-salad-sales-hierarchial-regression-model">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Hierarchial_Regression_Model.png"><img alt="../_images/Salad_Sales_Hierarchial_Regression_Model.png" src="../_images/Salad_Sales_Hierarchial_Regression_Model.png" style="width: 3.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.23 </span><span class="caption-text"><code class="docutils literal notranslate"><span class="pre">model_hierarchical_sales</span></code> where <span class="math notranslate nohighlight">\(\sigma_{hyperprior}\)</span> is the single
hierarchical distribution for the three <span class="math notranslate nohighlight">\(\sigma\)</span> distributions.</span><a class="headerlink" href="#fig-salad-sales-hierarchial-regression-model" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-default" id="fig-salad-sales-forestplot-sigma-hierarchical">
<a class="reference internal image-reference" href="../_images/Salad_Sales_ForestPlot_Sigma_Hierarchical.png"><img alt="../_images/Salad_Sales_ForestPlot_Sigma_Hierarchical.png" src="../_images/Salad_Sales_ForestPlot_Sigma_Hierarchical.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.24 </span><span class="caption-text">Forest plot of the <span class="math notranslate nohighlight">\(\sigma\)</span> parameter estimates for
<code class="docutils literal notranslate"><span class="pre">model_hierarchical_sales</span></code>. Note how the hyperprior tends to represent
fall within the range of the three group priors.</span><a class="headerlink" href="#fig-salad-sales-forestplot-sigma-hierarchical" title="Permalink to this image">¶</a></p>
</div>
<p>After fitting the hierarchical model we can inspect the <span class="math notranslate nohighlight">\(\sigma\)</span>
parameter estimates in
<a class="reference internal" href="#fig-salad-sales-forestplot-sigma-hierarchical"><span class="std std-numref">Fig. 4.24</span></a>. Again note the
addition of <span class="math notranslate nohighlight">\(\sigma_{hyperprior}\)</span> which is a distribution that estimates
the distribution of the parameters for each of the three food
categories. We can also see the effect of a hierarchical model if we
compare the summary tables of the unpooled model and hierarchical models
in <a class="reference internal" href="#tab-unpooled-sales"><span class="std std-numref">Table 4.3</span></a>. In the unpooled estimate the mean of
the <span class="math notranslate nohighlight">\(\sigma\)</span> estimate for salads is 21.3, whereas in the hierarchical
estimate the mean of the same parameter estimate is now 25.5, and has
been “pulled” up by the means of the pizza and sandwiches category.
Moreover, the estimates of the pizza and salad categories in the
hierarchical category, while regressed towards the mean slightly, remain
largely the same as the unpooled estimates. Note how the estimates of each
sigma are distinctly different from each other. Given that our observed data
and the model which does not share information between groups this consistent
with our expectations.</p>
<table class="table" id="tab-unpooled-sales">
<caption><span class="caption-number">Table 4.3 </span><span class="caption-text">Estimates of σ for each category from the unpooled sales model</span><a class="headerlink" href="#tab-unpooled-sales" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span>Pizza</p></td>
<td><p>40.1</p></td>
<td><p>1.5</p></td>
<td><p>37.4</p></td>
<td><p>42.8</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span>Salad</p></td>
<td><p>21.3</p></td>
<td><p>8.3</p></td>
<td><p>8.8</p></td>
<td><p>36.8</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span>Sandwich</p></td>
<td><p>35.9</p></td>
<td><p>2.5</p></td>
<td><p>31.6</p></td>
<td><p>40.8</p></td>
</tr>
</tbody>
</table>
<table class="table" id="tab-hierarchical-sales">
<caption><span class="caption-number">Table 4.4 </span><span class="caption-text">Estimates of σ for each category from the hierarchical sales model</span><a class="headerlink" href="#tab-hierarchical-sales" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>mean</strong></p></td>
<td><p><strong>sd</strong></p></td>
<td><p><strong>hdi_3%</strong></p></td>
<td><p><strong>hdi_97%</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span>Pizza</p></td>
<td><p>40.3</p></td>
<td><p>1.5</p></td>
<td><p>37.5</p></td>
<td><p>43.0</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span>Salad</p></td>
<td><p>25.5</p></td>
<td><p>12.4</p></td>
<td><p>8.4</p></td>
<td><p>48.7</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\sigma\)</span>Sandwich</p></td>
<td><p>36.2</p></td>
<td><p>2.6</p></td>
<td><p>31.4</p></td>
<td><p>41.0</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\sigma_{hyperprior}\)</span></p></td>
<td><p>31.2</p></td>
<td><p>8.7</p></td>
<td><p>15.8</p></td>
<td><p>46.9</p></td>
</tr>
</tbody>
</table>
<div class="admonition-i-heard-you-like-hyperpriors-so-i-put-a-hyperpriors-on-top-of-your-hyperpriors admonition">
<p class="admonition-title">I heard you like hyperpriors so I put a hyperpriors on top of your hyperpriors</p>
<p>In Code Block <a class="reference internal" href="#model-hierarchical-salad-sales-centered"><span class="std std-ref">model_hierarchical_salad_sales_centered</span></a>
we placed hyperprior on group level parameter <span class="math notranslate nohighlight">\(\sigma_j\)</span>. Similarly, we
can extend the model by also adding hyperprior to parameter
<span class="math notranslate nohighlight">\(\beta_{mj}\)</span>. Note that since <span class="math notranslate nohighlight">\(\beta_{mj}\)</span> has a Gaussian distributed
prior, we can actually choose two hyperprior - one for each
hyperparameter. A natural question you might ask is can we go even
further and adding hyperhyperprior to the parameters that are
parameterized the hyperprior? What about hyperhyperhyperprior? While it
is certainly possible to write down such models and sample from it, it
is worth to take a step back and think about what hyperpriors are doing.
Intuitively, they are a way for the model to “borrow” information from
sub-group or sub-cluster of data to inform the estimation of other
sub-group/cluster with less observation. The group with more
observations will inform the posterior of the hyperparameter, which then
in turn regulates the parameters for the group with less observations.
In this lens, putting hyperprior on parameters that are not group
specific is quite meaningless.</p>
</div>
<p>Hierarchical estimates are not just limited to two levels. For example,
the restaurant sales model could be extended into a three-level
hierarchical model where the top level represented the company level,
the next level represented the geographical market (New York, Chicago,
Los Angeles), and the lowest level represented an individual location.
By doing so we can have a hyperprior characterizing how the whole
company was doing, hyperpriors indicating how a region was doing, and
priors on how each store was doing. This allows easy comparisons in mean
and variation, and expands the application in many different ways based
on a single model.</p>
<div class="section" id="posterior-geometry-matters">
<span id="model-geometry"></span><h3><span class="section-number">4.6.1. </span>Posterior Geometry Matters<a class="headerlink" href="#posterior-geometry-matters" title="Permalink to this headline">¶</a></h3>
<p>So far we have largely focused on the structure and math behind the
model, and assumed our sampler would be able to provide us an
“accurate” estimate of the posterior. And for relatively simple models
this is largely true, the newest versions of Universal Inference Engines
mostly “just work”, but an important point is that they do not <em>always</em>
work. Certain posterior geometries are challenging for samplers, a
common example is Neal’s Funnel<span id="id19">[<a class="reference internal" href="references.html#id86">45</a>]</span> shown in
<a class="reference internal" href="#fig-neals-funnel"><span class="std std-numref">Fig. 4.25</span></a>. As the name funnel connotes, the shape at
one end is quite wide, before narrowing into a small neck. Recalling Section
<a class="reference internal" href="chp_01.html#sampling-methods-intro"><span class="std std-ref">A DIY Sampler, Do Not Try This at Home</span></a> samplers function
by taking steps from one set of parameter values to another, and a key
setting is how big of a step to take when exploring the posterior
surface. In complex geometries, such as with Neal’s funnel, a step size
that works well in one area, fails miserably in another.</p>
<div class="figure align-default" id="fig-neals-funnel">
<a class="reference internal image-reference" href="../_images/Neals_Funnel.png"><img alt="../_images/Neals_Funnel.png" src="../_images/Neals_Funnel.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.25 </span><span class="caption-text">Correlated samples in a particular shape referred to as Neal’s Funnel.
At sampling at the top of the funnel where Y is around a value 6 to 8, a
sampler can take wide steps of lets say 1 unit, and likely remain within
a dense region of the posterior. However, if sampling near the bottom of
the funnel where Y is around a value -6 to -8, a 1 unit step in almost
any direction will likely result in step into a low-density region. This
drastic difference in the posterior geometry shape is one reason poor
posterior estimation, can occur for sampling based estimates. For HMC
samplers the occurence of divergences can help diagnose these sampling
issues.</span><a class="headerlink" href="#fig-neals-funnel" title="Permalink to this image">¶</a></p>
</div>
<p>In hierarchical models the geometry is largely defined by the
correlation of hyperpriors to other parameters, which can result in
funnel geometry that are difficult to sample. Unfortunately this is not
only a theoretical problem, but a practical one, that can sneak up
relatively quickly on an unsuspecting Bayesian modeler. Luckily, there
is a relatively easy tweak to models, referred to as a non-centered
parameterization, that helps alleviate the issue.</p>
<p>Continuing with our salad example, let us say we open 6 salad
restaurants and like before are interested in predicting the sales as a
function of the number of customers. The synthetic dataset has been
generated in Python and is shown in
<a class="reference internal" href="#fig-multiple-salad-sales-scatter"><span class="std std-numref">Fig. 4.26</span></a>. Since the restaurants are
selling the exact same product a hierarchical model is appropriate to
share information across groups. We write the centered model
mathematically in Equation <a class="reference internal" href="#equation-eq-centered-hierarchical-regression">(4.8)</a> and
also Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales"><span class="std std-ref">model_hierarchical_salad_sales</span></a>.
We will be using TFP and <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> in the rest of
the chapter, which more easily highlights the change in
parameterization. This model follows the standard hierarchical format,
where a hyperprior partially pools the parameters of the slope
<span class="math notranslate nohighlight">\(\beta_m\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-centered-hierarchical-regression">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-eq-centered-hierarchical-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\beta_{\mu h} \sim&amp; \mathcal{N} \\
\beta_{\sigma h} \sim&amp; \mathcal{HN} \\
\beta_m \sim&amp; \overbrace{\mathcal{N}(\beta_{\mu h},\beta_{\sigma h})}^{\text{Centered}}  \\
\sigma_{h} \sim&amp; \mathcal{HN} \\
\sigma_{m} \sim&amp; \mathcal{HN}(\sigma_{h}) \\
Y \sim&amp; \mathcal{N}(\beta_{m} * X_m,\sigma_{m})
\end{split}\end{split}\]</div>
<div class="figure align-default" id="fig-multiple-salad-sales-scatter">
<a class="reference internal image-reference" href="../_images/Multiple_Salad_Sales_Scatter.png"><img alt="../_images/Multiple_Salad_Sales_Scatter.png" src="../_images/Multiple_Salad_Sales_Scatter.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.26 </span><span class="caption-text">Observed salad sales across 6 locations. Note how some locations have
very few data points relative to others.</span><a class="headerlink" href="#fig-multiple-salad-sales-scatter" title="Permalink to this image">¶</a></p>
</div>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-salad-sales">
<div class="code-block-caption"><span class="caption-number">Listing 4.13 </span><span class="caption-text">model_hierarchical_salad_sales</span><a class="headerlink" href="#model-hierarchical-salad-sales" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span> <span class="nf">gen_hierarchical_salad_sales</span><span class="p">(</span><span class="n">input_df</span><span class="p">,</span> <span class="n">beta_prior_fn</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="n">customers</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
        <span class="n">hierarchical_salad_df</span><span class="p">[</span><span class="s2">&quot;customers&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">location_category</span> <span class="o">=</span> <span class="n">hierarchical_salad_df</span><span class="p">[</span><span class="s2">&quot;location&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">sales</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">hierarchical_salad_df</span><span class="p">[</span><span class="s2">&quot;sales&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
    <span class="k">def</span> <span class="nf">model_hierarchical_salad_sales</span><span class="p">():</span>
        <span class="n">β_μ_hyperprior</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_mu&quot;</span><span class="p">))</span>
        <span class="n">β_σ_hyperprior</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_sigma&quot;</span><span class="p">))</span>
        <span class="n">β</span> <span class="o">=</span> <span class="k">yield from</span> <span class="n">beta_prior_fn</span><span class="p">(</span><span class="n">β_μ_hyperprior</span><span class="p">,</span> <span class="n">β_σ_hyperprior</span><span class="p">)</span>

        <span class="n">σ_hyperprior</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sigma_prior&quot;</span><span class="p">))</span>
        <span class="n">σ</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="n">σ_hyperprior</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sigma&quot;</span><span class="p">)</span>

        <span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">location_category</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">customers</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">σ</span><span class="p">,</span> <span class="n">location_category</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sales</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="p">),</span>
                                      <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sales&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model_hierarchical_salad_sales</span><span class="p">,</span> <span class="n">sales</span>
</pre></div>
</div>
</div>
<p>Similar to the TFP models we used in Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a>, the
model is wrapped within a function so we can condition on an arbitrary
inputs more easily. Besides the input data,
<code class="docutils literal notranslate"><span class="pre">gen_hierarchical_salad_sales</span></code> also takes a callable <code class="docutils literal notranslate"><span class="pre">beta_prior_fn</span></code>
which defines the prior of slope <span class="math notranslate nohighlight">\(\beta_m\)</span>. Inside the Coroutine model
we use a <code class="docutils literal notranslate"><span class="pre">yield</span> <span class="pre">from</span></code> statement to invoke the <code class="docutils literal notranslate"><span class="pre">beta_prior_fn</span></code>. This
description may be too abstract in words but is easier to see action in
Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-centered"><span class="std std-ref">model_hierarchical_salad_sales_centered</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-salad-sales-centered">
<div class="code-block-caption"><span class="caption-number">Listing 4.14 </span><span class="caption-text">model_hierarchical_salad_sales_centered</span><a class="headerlink" href="#model-hierarchical-salad-sales-centered" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">centered_beta_prior_fn</span><span class="p">(</span><span class="n">hyper_mu</span><span class="p">,</span> <span class="n">hyper_sigma</span><span class="p">):</span>
    <span class="n">β</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">hyper_mu</span><span class="p">,</span> <span class="n">hyper_sigma</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">β</span>

<span class="c1"># hierarchical_salad_df is the generated dataset as pandas.DataFrame</span>
<span class="n">centered_model</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">gen_hierarchical_salad_sales</span><span class="p">(</span>
    <span class="n">hierarchical_salad_df</span><span class="p">,</span> <span class="n">centered_beta_prior_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>As shown above, Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-centered"><span class="std std-ref">model_hierarchical_salad_sales_centered</span></a>
defined a centered parameterization of the slope <span class="math notranslate nohighlight">\(\beta_m\)</span>, which
follows a Normal distribution with <code class="docutils literal notranslate"><span class="pre">hyper_mu</span></code> and <code class="docutils literal notranslate"><span class="pre">hyper_sigma</span></code>.
<code class="docutils literal notranslate"><span class="pre">centered_beta_prior_fn</span></code> is a function that yields a <code class="docutils literal notranslate"><span class="pre">tfp.distribution</span></code>,
similar to the way we write a <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> model.
Now that we have our model, we can run inference and inspect the result
in Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-centered-inference"><span class="std std-ref">model_hierarchical_salad_sales_centered_inference</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-salad-sales-centered-inference">
<div class="code-block-caption"><span class="caption-number">Listing 4.15 </span><span class="caption-text">model_hierarchical_salad_sales_centered_inference</span><a class="headerlink" href="#model-hierarchical-salad-sales-centered-inference" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mcmc_samples_centered</span><span class="p">,</span> <span class="n">sampler_stats_centered</span> <span class="o">=</span> <span class="n">run_mcmc</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">centered_model</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">sales</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>

<span class="n">divergent_per_chain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampler_stats_centered</span><span class="p">[</span><span class="s2">&quot;diverging&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;There were </span><span class="si">{</span><span class="n">divergent_per_chain</span><span class="si">}</span><span class="s2"> divergences after tuning per chain.&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>There were [37 31 17 37] divergences after tuning per chain.
</pre></div>
</div>
<p>We reuse the inference code previously shown in Code Block
<a class="reference internal" href="chp_03.html#tfp-posterior-inference"><span class="std std-ref">tfp_posterior_inference</span></a> to run
our model. After running our model the first indication of issues is the
divergences, the details of which we covered in Section
<a class="reference internal" href="chp_02.html#divergences"><span class="std std-ref">Divergences</span></a>. A plot of the sample space is the next
diagnostic and is shown in <a class="reference internal" href="#fig-neals-funnel-salad-centered"><span class="std std-numref">Fig. 4.27</span></a>.
Note how as the hyperprior <span class="math notranslate nohighlight">\(\beta_{\sigma h}\)</span> approaches zero, the width
of the posterior estimate of the <span class="math notranslate nohighlight">\(\beta_m\)</span> parameters tend to shrink. In
particular note how there are no samples near zero. In other words as
the value <span class="math notranslate nohighlight">\(\beta_{\sigma h}\)</span> approaches zero, there the region in which
to sample parameter <span class="math notranslate nohighlight">\(\beta_m\)</span> collapses and the sampler is not able to
effectively characterize this space of the posterior.</p>
<div class="figure align-default" id="fig-neals-funnel-salad-centered">
<a class="reference internal image-reference" href="../_images/Neals_Funnel_Salad_Centered.png"><img alt="../_images/Neals_Funnel_Salad_Centered.png" src="../_images/Neals_Funnel_Salad_Centered.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.27 </span><span class="caption-text">Scatter plot of the hyperprior and the slope of <span class="math notranslate nohighlight">\(\beta[4]\)</span> from
<code class="docutils literal notranslate"><span class="pre">centered_model</span></code> defined in Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-centered"><span class="std std-ref">model_hierarchical_salad_sales_centered</span></a>.
As the hyperprior approaches zero the posterior space for slope
collapses results in the divergences seen in blue.</span><a class="headerlink" href="#fig-neals-funnel-salad-centered" title="Permalink to this image">¶</a></p>
</div>
<p>To alleviate this issue the centered parameterization can be converted
into a non-centered parameterization shown in Code Block in
<a class="reference internal" href="#model-hierarchical-salad-sales-non-centered"><span class="std std-ref">model_hierarchical_salad_sales_non_centered</span></a>
and Equation <a class="reference internal" href="#equation-eq-noncentered-hierarchical-regression">(4.9)</a>. The key
difference is that instead of estimating parameters of the slope
<span class="math notranslate nohighlight">\(\beta_m\)</span> directly, it is instead modeled as a common term shared
between all groups and a term for each group that captures the deviation
from the common term. This modifies the posterior geometry in a manner
that allows the sampler to more easily explore all possible values of
<span class="math notranslate nohighlight">\(\beta_{\sigma h}\)</span>. The effect of this posterior geometry change is as
shown in <a class="reference internal" href="#fig-neals-funnel-salad-noncentered"><span class="std std-numref">Fig. 4.28</span></a>, where there are
multiple samples down to the 0 value on the x-axis.</p>
<div class="math notranslate nohighlight" id="equation-eq-noncentered-hierarchical-regression">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-eq-noncentered-hierarchical-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
\beta_{\mu h} \sim&amp; \mathcal{N} \\
\beta_{\sigma h} \sim&amp; \mathcal{HN} \\
\beta_\text{m\_offset} \sim&amp; \mathcal{N}(0,1) \\
\beta_m =&amp; \overbrace{\beta_{\mu h} + \beta_\text{m\_offset}*\beta_{\sigma h}}^{\text{Non-centered}}  \\
\sigma_{h} \sim&amp; \mathcal{HN} \\
\sigma_{m} \sim&amp; \mathcal{HN}(\sigma_{h}) \\
Y \sim&amp; \mathcal{N}(\beta_{m} * X_m,\sigma_{m})
\end{split}\end{split}\]</div>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-salad-sales-non-centered">
<div class="code-block-caption"><span class="caption-number">Listing 4.16 </span><span class="caption-text">model_hierarchical_salad_sales_non_centered</span><a class="headerlink" href="#model-hierarchical-salad-sales-non-centered" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">non_centered_beta_prior_fn</span><span class="p">(</span><span class="n">hyper_mu</span><span class="p">,</span> <span class="n">hyper_sigma</span><span class="p">):</span>
    <span class="n">β_offset</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_offset&quot;</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">β_offset</span> <span class="o">*</span> <span class="n">hyper_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">hyper_mu</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>

<span class="c1"># hierarchical_salad_df is the generated dataset as pandas.DataFrame</span>
<span class="n">non_centered_model</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">gen_hierarchical_salad_sales</span><span class="p">(</span>
    <span class="n">hierarchical_salad_df</span><span class="p">,</span> <span class="n">non_centered_beta_prior_fn</span><span class="p">)</span>

<span class="n">mcmc_samples_noncentered</span><span class="p">,</span> <span class="n">sampler_stats_noncentered</span> <span class="o">=</span> <span class="n">run_mcmc</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">non_centered_model</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">sales</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>

<span class="n">divergent_per_chain</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sampler_stats_noncentered</span><span class="p">[</span><span class="s2">&quot;diverging&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There were </span><span class="si">{</span><span class="n">divergent_per_chain</span><span class="si">}</span><span class="s2"> divergences after tuning per chain.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>There were [1 0 2 0] divergences after tuning per chain.
</pre></div>
</div>
<div class="figure align-default" id="fig-neals-funnel-salad-noncentered">
<a class="reference internal image-reference" href="../_images/Neals_Funnel_Salad_NonCentered.png"><img alt="../_images/Neals_Funnel_Salad_NonCentered.png" src="../_images/Neals_Funnel_Salad_NonCentered.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.28 </span><span class="caption-text">Scatter plot of the hyperprior and the estimated slope <span class="math notranslate nohighlight">\(\beta[4]\)</span> of
location 4 from <code class="docutils literal notranslate"><span class="pre">non_centered_model</span></code> defined in Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-non-centered"><span class="std std-ref">model_hierarchical_salad_sales_non_centered</span></a>.
In the non-centered parameterization the sampler is able to sample
parameters close to zero. The divergences are lesser in number and are
not concentrated in one area.</span><a class="headerlink" href="#fig-neals-funnel-salad-noncentered" title="Permalink to this image">¶</a></p>
</div>
<p>The improvement in sampling has a material effect on the estimated
distribution shown in <a class="reference internal" href="#fig-salad-sales-hierarchical-comparison"><span class="std std-numref">Fig. 4.29</span></a>.
While it may be jarring to be reminded of this fact again, samplers
merely estimate the posterior distribution, and while in many cases they
do quite well, it is not guaranteed! Be sure to pay heed to the
diagnostics and investigate more deeply if warnings arise.</p>
<p>It is worth noting that there is no one size fits all solution when it
comes to centered or non-centered parameterization
<span id="id20">[<a class="reference internal" href="references.html#id173">46</a>]</span>. It is a complex interaction among the
informativeness of the individual likelihood at group level (usually the
more data you have for a specific group, the more informative the
likelihood function will be), the informativeness of the group level
prior, and the parameterization. A general heuristic is that if there
are not a lot of observations, a non-centered parameterization is
preferred. In practice however, you should try a few different
combinations of centered and non-centered parameterizations, with
different prior specifications. You might even find cases where you need
<em>both</em> centered and non-centered parameterization in a single model. We
recommend you to read Michael Betancourt’s case study Hierarchical
Modeling on this topic if you suspect model parameterization is causing
you sampling issues <span id="id21">[<a class="reference internal" href="references.html#id164">47</a>]</span>.</p>
<div class="figure align-default" id="fig-salad-sales-hierarchical-comparison">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Hierarchical_Comparison.png"><img alt="../_images/Salad_Sales_Hierarchical_Comparison.png" src="../_images/Salad_Sales_Hierarchical_Comparison.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.29 </span><span class="caption-text">KDE of the distributions of <span class="math notranslate nohighlight">\(\beta_{\sigma h}\)</span> in both centered and
non-centered parameterizations. The change is due to the sampler being
able to more adequately explore the possible parameter space.</span><a class="headerlink" href="#fig-salad-sales-hierarchical-comparison" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="predictions-at-multiple-levels">
<span id="id22"></span><h3><span class="section-number">4.6.2. </span>Predictions at Multiple Levels<a class="headerlink" href="#predictions-at-multiple-levels" title="Permalink to this headline">¶</a></h3>
<p>A subtle feature of hierarchical models is that they are able to make
estimates at multiple levels. While seemingly obvious this is very
useful, as it lets us use one model to answer many more questions than a
single level model. In Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a> we could built a model
to estimate the mass of a single species or a separate model to estimate
the mass of any penguin regardless of species. Using a hierarchical
model we could estimate the mass of all penguins, and each penguin
species, at the same time with one model. With our salad sales model we
can both make estimations about an individual location and about the
population a whole. We can do so by using our previous
<code class="docutils literal notranslate"><span class="pre">non_centered_model</span></code> from Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-non-centered"><span class="std std-ref">model_hierarchical_salad_sales_non_centered</span></a>,
and write an <code class="docutils literal notranslate"><span class="pre">out_of_sample_prediction_model</span></code> as shown in Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-predictions"><span class="std std-ref">model_hierarchical_salad_sales_predictions</span></a>.
This using the fitted parameter estimates to make an out of sample
prediction for the distribution of customers for 50 customers, at two
locations and for the company as a whole <em>simultaneously</em>. Since our
<code class="docutils literal notranslate"><span class="pre">non_centered_model</span></code> is also a TFP distribution, we can nest it into
another <code class="docutils literal notranslate"><span class="pre">tfd.JointDistribution</span></code>, doing so constructed a larger Bayesian
graphical model that extends our initial <code class="docutils literal notranslate"><span class="pre">non_centered_model</span></code> to include
nodes for out of sample prediction. The estimates are plotted in
<a class="reference internal" href="#fig-salad-sales-hierarchical-predictions"><span class="std std-numref">Fig. 4.30</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-salad-sales-predictions">
<div class="code-block-caption"><span class="caption-number">Listing 4.17 </span><span class="caption-text">model_hierarchical_salad_sales_predictions</span><a class="headerlink" href="#model-hierarchical-salad-sales-predictions" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out_of_sample_customers</span> <span class="o">=</span> <span class="mf">50.</span>

<span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">out_of_sample_prediction_model</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">non_centered_model</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">beta_offset</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">beta_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">beta_mu</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    
    <span class="n">β_group</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">beta_mu</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">beta_sigma</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;group_beta_prediction&quot;</span><span class="p">)</span>
    <span class="n">group_level_prediction</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">β_group</span> <span class="o">*</span> <span class="n">out_of_sample_customers</span><span class="p">,</span>
        <span class="n">model</span><span class="o">.</span><span class="n">sigma_prior</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;group_level_prediction&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
        <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">out_of_sample_customers</span><span class="p">,</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;location_</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s2">_prediction&quot;</span><span class="p">)</span>

<span class="n">amended_posterior</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
    <span class="n">non_centered_model</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span>
    <span class="nb">list</span><span class="p">(</span><span class="n">mcmc_samples_noncentered</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">observed</span><span class="p">])</span>

<span class="n">ppc</span> <span class="o">=</span> <span class="n">out_of_sample_prediction_model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">var0</span><span class="o">=</span><span class="n">amended_posterior</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="figure align-default" id="fig-salad-sales-hierarchical-predictions">
<a class="reference internal image-reference" href="../_images/Salad_Sales_Hierarchical_Predictions.png"><img alt="../_images/Salad_Sales_Hierarchical_Predictions.png" src="../_images/Salad_Sales_Hierarchical_Predictions.png" style="width: 7.00in;" /></a>
<p class="caption"><span class="caption-number">Fig. 4.30 </span><span class="caption-text">Posterior predictive estimates for the revenues for two of the groups
and for total population estimated by model
<code class="docutils literal notranslate"><span class="pre">model_hierarchical_salad_sales_non_centered</span></code>.</span><a class="headerlink" href="#fig-salad-sales-hierarchical-predictions" title="Permalink to this image">¶</a></p>
</div>
<p>Another feature in making predictions is using hierarchical models with
hyperpriors is that we can make prediction for never before seen groups.
In this case, imagine we are opening another salad restaurant in a new
location we can already make some predictions of how the salad sales
might looks like by first sampling from the hyper prior to get the
<span class="math notranslate nohighlight">\(\beta_{i+1}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{i+1}\)</span> of the new location, then sample from
the posterior predictive distribution to get salad sales prediction.
This is demonstrated in Code Block
<a class="reference internal" href="#model-hierarchical-salad-sales-predictions-new-location"><span class="std std-ref">model_hierarchical_salad_sales_predictions_new_location</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="model-hierarchical-salad-sales-predictions-new-location">
<div class="code-block-caption"><span class="caption-number">Listing 4.18 </span><span class="caption-text">model_hierarchical_salad_sales_predictions_new_location</span><a class="headerlink" href="#model-hierarchical-salad-sales-predictions-new-location" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out_of_sample_customers2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">90</span><span class="p">)</span>

<span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">out_of_sample_prediction_model2</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">non_centered_model</span><span class="p">)</span>
    
    <span class="n">β_new_loc</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">beta_mu</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">beta_sigma</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;beta_new_loc&quot;</span><span class="p">)</span>
    <span class="n">σ_new_loc</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sigma_prior</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;sigma_new_loc&quot;</span><span class="p">)</span>
    <span class="n">group_level_prediction</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">β_new_loc</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">out_of_sample_customers2</span><span class="p">,</span>
        <span class="n">σ_new_loc</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">&quot;new_location_prediction&quot;</span><span class="p">)</span>

<span class="n">ppc</span> <span class="o">=</span> <span class="n">out_of_sample_prediction_model2</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">var0</span><span class="o">=</span><span class="n">amended_posterior</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In addition to the mathematical benefits of hierarchical modeling, there
is a benefit from a computational perspective as we only need to
construct and fit a single model. This speeds up the modeling process
and the subsequent model maintenance process, if the model is reused
multiple times over time.</p>
<div class="admonition-on-the-validity-of-loo admonition">
<p class="admonition-title">On the validity of LOO</p>
<p>Hierarchical models allow us to make posterior
predictions even for group(s) that have never been seen before. However,
how valid would the prediction be? Could we use cross-validation to
assess the performance of the model? As usually in statistics the answer
is <em>that depends</em>. Whether cross-validation (and methods like LOO and
WAIC) is valid or not depends on the prediction task you want to
perform, and also on the data generating mechanism. If we want to use
LOO to assess how well the model is able to predict new observations ,
then LOO is fine. Now if we want to assess how well one entire group is
predicted, then you will need to perform leave one-group-out cross
validation, which is a well defined procedure. In that case however, the
LOO method will most likely not be good, as we are removing many
observations at a time and the importance sampling step at the core of
the LOO approximation relies on the distributions with and without the
point/group/etc being close to each other.</p>
</div>
</div>
<div class="section" id="priors-for-multilevel-models">
<span id="id23"></span><h3><span class="section-number">4.6.3. </span>Priors for Multilevel Models<a class="headerlink" href="#priors-for-multilevel-models" title="Permalink to this headline">¶</a></h3>
<p>Prior choice is all the more important for multilevel models, because of
how the prior interacts with the informativeness of the likelihood, as
shown above in Section <a class="reference internal" href="#model-geometry"><span class="std std-ref">Posterior Geometry Matters</span></a>. Moreover, not only does the
shape of prior distribution matter, we also have additional choices of
how to parameterize them. This does not limit us to Gaussian priors as
it applies to all distributions in the location-scale distribution
family <a class="footnote-reference brackets" href="#id32" id="id24">6</a>.</p>
<p>In multilevel models prior distributions not only characterize the
in-group variation, but the between-group variation as well. In a sense
the choice of hyperprior is defining the “variation of variation”,
which could make expressing and reasoning about prior information
difficult. Moreover, since the effect of partial pooling is the
combination of how informative the hyperprior is, the number of groups
you have, and the number of observations in each group. Due to this the
same hyperprior might not work if you are performing inference using the
same model on similar dataset but with fewer groups.</p>
<p>As is such besides empirical experience (e.g., general recommendations
published in articles) or general advice <a class="footnote-reference brackets" href="#id33" id="id25">7</a>, we can also perform
sensitivity studies to better inform our prior choice. For instance
Lemoine <span id="id26">[<a class="reference internal" href="references.html#id122">48</a>]</span> showed that when modeling ecology data with a
model structure of</p>
<div class="math notranslate nohighlight" id="equation-eq-ecology-regression">
<span class="eqno">(4.10)<a class="headerlink" href="#equation-eq-ecology-regression" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \alpha_i \sim&amp; \mathcal{N}(\mu_{\alpha},\sigma^2_{\alpha}) \\
    \mu_{i} =&amp; \alpha_i + \beta Day_i \\
    Y \sim&amp; \mathcal{N}(\mu_{j},\sigma^2)
\end{split}\end{split}\]</div>
<p>where the intercept is unpooled, Cauchy priors provide regularization at
few data points, and do not obscure the posterior when the model is
fitted on additional data. This is done through prior sensitivity
analysis across both prior parameterizations and differing amounts of
data. In your own multilevel models be sure to note multitude of ways a
prior choice affects inference, and use either your domain expertise or
tools such as prior predictive distributions to make an informed choice.</p>
</div>
</div>
<div class="section" id="exercises">
<span id="exercises4"></span><h2><span class="section-number">4.7. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<p><strong>4E1.</strong> What are examples of covariate-response
relationships that are nonlinear in everyday life?</p>
<p><strong>4E2.</strong> Assume you are studying the relationship between a
covariate and an outcome and the data can be into 2 groups. You will be
using a regression with a slope and intercept as your basic model
structure.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
    \mu =&amp; \beta_0 + \beta_1 X_1 \\
    Y \sim&amp; \mathcal{N}(\mu, \sigma)
\end{split}\end{split}\]</div>
<p>Also assume you now need to extend the model structure in each of the
ways listed below. For each item write the mathematical equations that
specify the full model.</p>
<ol class="simple">
<li><p>Pooled</p></li>
<li><p>Unpooled</p></li>
<li><p>Mixed Effect with pooled <span class="math notranslate nohighlight">\(\beta_0\)</span></p></li>
<li><p>Hierarchical <span class="math notranslate nohighlight">\(\beta_0\)</span></p></li>
<li><p>Hierarchical all parameters</p></li>
<li><p>Hierarchical all parameters with non-centered <span class="math notranslate nohighlight">\(\beta\)</span> parameters</p></li>
</ol>
<p><strong>4E3.</strong> Use statistical notation to write a robust linear
regression model for the baby dataset.</p>
<p><strong>4E4.</strong> Consider the plight of a bodybuilder who needs to
lift weights, do cardiovascular exercise, and eat to build a physique
that earns a high score at a contest. If we were to build a model where
weightlifting, cardiovascular exercise, and eating were covariates do
you think these covariates are independent or do they interact? From
your domain knowledge justify your answer?</p>
<p><strong>4E5.</strong> An interesting property of the Student’s
t-distribution is that at values of <span class="math notranslate nohighlight">\(\nu = 1\)</span> and <span class="math notranslate nohighlight">\(\nu = \infty\)</span>, the
Student’s t-distribution becomes identical two other distributions the
Cauchy distribution and the Normal distribution. Plot the Student’s
t-distribution at both parameter values of <span class="math notranslate nohighlight">\(\nu\)</span> and match each
parameterization to Cauchy or Normal.</p>
<p><strong>4E6.</strong> Assume we are trying to predict the heights of
individuals. If given a dataset of height and one of the following
covariates explain which type of regression would be appropriate between
unpooled, pooled, partially pooled, and interaction. Explain why</p>
<ol class="simple">
<li><p>A vector of random noise</p></li>
<li><p>Gender</p></li>
<li><p>Familial relationship</p></li>
<li><p>Weight</p></li>
</ol>
<p><strong>4E7.</strong> Use LOO to compare the results of
<code class="docutils literal notranslate"><span class="pre">baby_model_linear</span></code> and <code class="docutils literal notranslate"><span class="pre">baby_model_sqrt</span></code>. Using LOO justify why the
transformed covariate is justified as a modeling choice.</p>
<p><strong>4E8.</strong> Go back to the penguin dataset. Add an interaction
term to estimate penguin mass between species and flipper length. How do
the predictions differ? Is this model better? Justify your reasoning in
words and using LOO.</p>
<p><strong>4M9.</strong> Ancombe’s Quartet is a famous dataset highlighting
the challenges with evaluating regressions solely on numerical
summaries. The dataset is available at the GitHub repository. Perform a
regression on the third case of Anscombe’s quartet with both robust and
non-robust regression. Plot the results.</p>
<p><strong>4M10.</strong> Revisit the penguin mass model defined in Code Block
<a class="reference internal" href="chp_03.html#nocovariate-mass"><span class="std std-ref">nocovariate_mass</span></a>. Add a hierarchical
term for <span class="math notranslate nohighlight">\(\mu\)</span>. What is the estimated mean of the hyperprior? What is
the average mass for all penguins? Compare the empirical mean to the
estimated mean of the hyperprior. Do the values of the two estimates
make sense to you, particularly when compared to each other? Why?</p>
<p><strong>4M11.</strong> The compressive strength of concrete is dependent
on the amount of water and cement used to produce it. In the GitHub
repository we have provided a dataset of concrete compressive strength,
as well the amount of water and cement included (kilograms per cubic
meter). Create a linear model with an interaction term between water and
cement. What is different about the inputs of this interaction model
versus the smoker model we saw earlier? Plot the concrete compressive
strength as function of concrete at various fixed values of water.</p>
<p><strong>4M12.</strong> Rerun the pizza regression but this time do it with
heteroskedastic regression. What are the results?</p>
<p><strong>4H13.</strong> Radon is a radioactive gas that can cause lung
cancer and thus it is something that would be undesirable in a domicile.
Unfortunately the presence of a basement may increase the radon levels
in a household as radon may enter the household more easily through the
ground. We have provided a dataset of the radon levels at homes in
Minnesota, in the GitHub repository as well as the county of the home,
and the presence of a basement.</p>
<ol class="simple">
<li><p>Run an unpooled regression estimating the effect of basements on
radon levels.</p></li>
<li><p>Create a hierarchical model grouping by county. Justify why this
model would be useful for the given the data.</p></li>
<li><p>Create a non-centered regression. Using plots and diagnostics
justify if the non-centered parameterization was needed.</p></li>
</ol>
<p><strong>4H14.</strong> Generate a synthetic dataset for each of the models
below with your own choice of parameters. Then fit two models to each
dataset, one model matching the data generating process, and one that
does not. See how the diagnostic summaries and plots differ between the
two.</p>
<p>For example, we may generate data that follows a linear pattern
<span class="math notranslate nohighlight">\(x=[1,2,3,4], y=[2,4,6,8]\)</span>. Then fit a model of the form <span class="math notranslate nohighlight">\(y=bx\)</span> and
another of the form <span class="math notranslate nohighlight">\(y=bx**2\)</span></p>
<ol class="simple">
<li><p>Linear Model</p></li>
<li><p>Linear model with transformed covariate</p></li>
<li><p>Linear model with interaction effect</p></li>
<li><p>4 group model with pooled intercept, and unpooled slope and noise</p></li>
<li><p>A Hierarchical Model</p></li>
</ol>
<p><strong>4H15.</strong> For the hierarchical salad regression model
evaluate the posterior geometry for the slope parameter <span class="math notranslate nohighlight">\(\beta_{\mu h}\)</span>.
Then create a version of the model where <span class="math notranslate nohighlight">\(\beta_{\mu h}\)</span> is
non-centered. Plot the geometry now. Are there any differences? Evaluate
the divergences and output as well. Does non-centering help in this
case?</p>
<p><strong>4H16.</strong> A colleague of yours, who now lives on an unknown
planet, ran experiment to test the basic laws of physics. She dropped a
ball of a cliff and registers the position for 20 seconds. The data is
available in the Github repository in the file
<code class="docutils literal notranslate"><span class="pre">gravity_measurements.csv</span></code> You know that from Newton’s Laws of physics
if the acceleration is <span class="math notranslate nohighlight">\(g\)</span> and the time <span class="math notranslate nohighlight">\(t\)</span> then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
\text{velocity} &amp;= gt \\
\text{position} &amp;= \frac{1}{2}gt^2 \\
\end{split}\end{split}\]</div>
<p>Your friend asks you to estimate the following quantities</p>
<ol class="simple">
<li><p>The gravitational constant of the planet</p></li>
<li><p>A characterization of the noise of her measurement device</p></li>
<li><p>The velocity of the ball at each point during her measurements</p></li>
<li><p>The estimated position of the ball from time 20 to time 30</p></li>
</ol>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>The difference between the observed value and the estimated value
of a quantity of interest is call the residual.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id6">2</a></span></dt>
<dd><p>Remember this is just a toy dataset, so the take-home message
should be about modeling interactions and not about tips.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id9">3</a></span></dt>
<dd><p>Although the mean is defined only for <span class="math notranslate nohighlight">\(\nu &gt; 1\)</span>, and the value of
<span class="math notranslate nohighlight">\(\sigma\)</span> agrees with the standard deviation only when
<span class="math notranslate nohighlight">\(\nu \to \infty\)</span>.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id10">4</a></span></dt>
<dd><p>A thin dough filled with a salty or sweet preparation and baked or
fried. The filling can include red or white meat, fish, vegetables,
or fruit. Empanadas are common in Southern European, Latin American,
and the Filipino cultures.</p>
</dd>
<dt class="label" id="id31"><span class="brackets"><a class="fn-backref" href="#id11">5</a></span></dt>
<dd><p>The commemoration of the first Argentine government and the
Argentine independence day respectively.</p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id24">6</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Location%E2%80%93scale_family">https://en.wikipedia.org/wiki/Location–scale_family</a></p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id25">7</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations</a></p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_03.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3. </span>Linear Models and Probabilistic Programming Languages</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="chp_05.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Splines</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Martin, Kumar, Lao<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>