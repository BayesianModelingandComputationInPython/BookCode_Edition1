
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>11. Appendiceal Topics &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="12. Glossary" href="glossary.html" />
    <link rel="prev" title="10. Probabilistic Programming Languages" href="chp_10.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Buscar este libro ..." aria-label="Buscar este libro ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Navegación de palanca" aria-controls="site-navigation"
                title="Navegación de palanca" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Repositorio de origen"><i
                    class="fab fa-github"></i>repositorio</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_11.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Abrir un problema"><i class="fas fa-lightbulb"></i>Tema abierto</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Modo de pantalla completa"
        title="Modo de pantalla completa"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contenido
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-background">
   11.1. Probability Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability">
     11.1.1. Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-probability">
     11.1.2. Conditional Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-distribution">
     11.1.3. Probability Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-random-variables-and-distributions">
     11.1.4. Discrete Random Variables and Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-uniform-distribution">
       11.1.4.1. Discrete Uniform Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#binomial-distribution">
       11.1.4.2. Binomial Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#poisson-distribution">
       11.1.4.3. Poisson Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-random-variables-and-distributions">
     11.1.5. Continuous Random Variables and Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#continuous-uniform-distribution">
       11.1.5.1. Continuous Uniform Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-or-normal-distribution">
       11.1.5.2. Gaussian or Normal Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#student-s-t-distribution">
       11.1.5.3. Student’s t-distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beta-distribution">
       11.1.5.4. Beta Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-conditional-and-marginal-distributions">
     11.1.6. Joint, Conditional and Marginal Distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-integral-transform-pit">
     11.1.7. Probability Integral Transform (PIT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectations">
     11.1.8. Expectations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformations">
     11.1.9. Transformations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limits">
     11.1.10. Limits
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-law-of-large-numbers">
       11.1.10.1. The Law of Large Numbers
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-central-limit-theorem">
       11.1.10.2. The Central Limit Theorem
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chains">
     11.1.11. Markov Chains
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#entropy">
   11.2. Entropy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kullback-leibler-divergence">
   11.3. Kullback-Leibler Divergence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-criterion">
   11.4. Information Criterion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loo-in-depth">
   11.5. LOO in Depth
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#jeffreys-prior-derivation">
   11.6. Jeffreys’ Prior Derivation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffreys-prior-for-the-binomial-likelihood-in-terms-of-theta">
     11.6.1. Jeffreys’ Prior for the Binomial Likelihood in Terms of
     <span class="math notranslate nohighlight">
      \(\theta\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffreys-prior-for-the-binomial-likelihood-in-terms-of-kappa">
     11.6.2. Jeffreys’ Prior for the Binomial Likelihood in Terms of
     <span class="math notranslate nohighlight">
      \(\kappa\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffreys-posterior-for-the-binomial-likelihood">
     11.6.3. Jeffreys’ Posterior for the Binomial Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#marginal-likelihood">
   11.7. Marginal Likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-harmonic-mean-estimator">
     11.7.1. The Harmonic Mean Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marginal-likelihood-and-model-comparison">
     11.7.2. Marginal Likelihood and Model Comparison
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-factor-vs-waic-and-loo">
     11.7.3. Bayes Factor vs WAIC and LOO
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moving-out-of-flatland">
   11.8. Moving out of Flatland
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-methods">
   11.9. Inference Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grid-method">
     11.9.1. Grid Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metropolis-hastings">
     11.9.2. Metropolis-Hastings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hamiltonian-monte-carlo">
     11.9.3. Hamiltonian Monte Carlo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequential-monte-carlo">
     11.9.4. Sequential Monte Carlo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variational-inference">
     11.9.5. Variational Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#programming-references">
   11.10. Programming References
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#which-programming-language">
     11.10.1. Which Programming Language?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#version-control">
     11.10.2. Version Control
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dependency-management-and-package-repositories">
     11.10.3. Dependency Management and Package Repositories
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#environment-management">
     11.10.4. Environment Management
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-editor-vs-integrated-development-environment-vs-notebook">
     11.10.5. Text Editor vs Integrated Development Environment vs Notebook
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-specific-tools-used-for-this-book">
     11.10.6. The Specific Tools Used for this Book
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Appendiceal Topics</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contenido </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#probability-background">
   11.1. Probability Background
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability">
     11.1.1. Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conditional-probability">
     11.1.2. Conditional Probability
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-distribution">
     11.1.3. Probability Distribution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#discrete-random-variables-and-distributions">
     11.1.4. Discrete Random Variables and Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#discrete-uniform-distribution">
       11.1.4.1. Discrete Uniform Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#binomial-distribution">
       11.1.4.2. Binomial Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#poisson-distribution">
       11.1.4.3. Poisson Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-random-variables-and-distributions">
     11.1.5. Continuous Random Variables and Distributions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#continuous-uniform-distribution">
       11.1.5.1. Continuous Uniform Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-or-normal-distribution">
       11.1.5.2. Gaussian or Normal Distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#student-s-t-distribution">
       11.1.5.3. Student’s t-distribution
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#beta-distribution">
       11.1.5.4. Beta Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#joint-conditional-and-marginal-distributions">
     11.1.6. Joint, Conditional and Marginal Distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability-integral-transform-pit">
     11.1.7. Probability Integral Transform (PIT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expectations">
     11.1.8. Expectations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transformations">
     11.1.9. Transformations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#limits">
     11.1.10. Limits
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-law-of-large-numbers">
       11.1.10.1. The Law of Large Numbers
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-central-limit-theorem">
       11.1.10.2. The Central Limit Theorem
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#markov-chains">
     11.1.11. Markov Chains
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#entropy">
   11.2. Entropy
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kullback-leibler-divergence">
   11.3. Kullback-Leibler Divergence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#information-criterion">
   11.4. Information Criterion
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loo-in-depth">
   11.5. LOO in Depth
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#jeffreys-prior-derivation">
   11.6. Jeffreys’ Prior Derivation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffreys-prior-for-the-binomial-likelihood-in-terms-of-theta">
     11.6.1. Jeffreys’ Prior for the Binomial Likelihood in Terms of
     <span class="math notranslate nohighlight">
      \(\theta\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffreys-prior-for-the-binomial-likelihood-in-terms-of-kappa">
     11.6.2. Jeffreys’ Prior for the Binomial Likelihood in Terms of
     <span class="math notranslate nohighlight">
      \(\kappa\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffreys-posterior-for-the-binomial-likelihood">
     11.6.3. Jeffreys’ Posterior for the Binomial Likelihood
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#marginal-likelihood">
   11.7. Marginal Likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-harmonic-mean-estimator">
     11.7.1. The Harmonic Mean Estimator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#marginal-likelihood-and-model-comparison">
     11.7.2. Marginal Likelihood and Model Comparison
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayes-factor-vs-waic-and-loo">
     11.7.3. Bayes Factor vs WAIC and LOO
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#moving-out-of-flatland">
   11.8. Moving out of Flatland
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference-methods">
   11.9. Inference Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#grid-method">
     11.9.1. Grid Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metropolis-hastings">
     11.9.2. Metropolis-Hastings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hamiltonian-monte-carlo">
     11.9.3. Hamiltonian Monte Carlo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sequential-monte-carlo">
     11.9.4. Sequential Monte Carlo
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#variational-inference">
     11.9.5. Variational Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#programming-references">
   11.10. Programming References
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#which-programming-language">
     11.10.1. Which Programming Language?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#version-control">
     11.10.2. Version Control
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dependency-management-and-package-repositories">
     11.10.3. Dependency Management and Package Repositories
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#environment-management">
     11.10.4. Environment Management
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#text-editor-vs-integrated-development-environment-vs-notebook">
     11.10.5. Text Editor vs Integrated Development Environment vs Notebook
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-specific-tools-used-for-this-book">
     11.10.6. The Specific Tools Used for this Book
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="appendiceal-topics">
<span id="app"></span><h1><span class="section-number">11. </span>Appendiceal Topics<a class="headerlink" href="#appendiceal-topics" title="Permalink to this headline">¶</a></h1>
<p>This chapter is different from the rest because it is not about any
specific topic. Instead it is a collection of different topics that
provide support for the rest of the book by complementing topics
discussed in other chapters. These topics are here for readers who are
interested in going deeper into each of the methods and theory. In terms
of writing style it will be a little bit more theoretical and abstract
than other chapters.</p>
<section id="probability-background">
<span id="id1"></span><h2><span class="section-number">11.1. </span>Probability Background<a class="headerlink" href="#probability-background" title="Permalink to this headline">¶</a></h2>
<p>The Spanish word azahar (the flower of certain critics) and azar
(randomness) are not similar out of pure luck. They both come from the
Arabic language <a class="footnote-reference brackets" href="#id98" id="id2">1</a>. From ancient times, and even today, certain games
of chance use a bone that it has two flat sides. This bone is similar to
a coin or a two-side die. To make it easier to identify one side from
the other, at least one side has a distinctive mark, for example,
ancient Arabs commonly used a flower. With the passage of time Spanish
adopted the term azahar for certain flowers and azar to mean randomness.
One of the motivation to the development of probability theory can be
track back to understanding games of chance and probably trying to make
an small fortune in the process. So let us start this brief introduction
to some of the central concepts in probability theory<a class="footnote-reference brackets" href="#id99" id="id3">2</a> imagining a
6-sided die. Each time we roll the die it is only possible to obtain an
integer from 1 to 6 without preference of one over another. Using Python
we can program a die like this in the following way:</p>
<div class="literal-block-wrapper docutils container" id="die">
<div class="code-block-caption"><span class="caption-number">Listing 11.1 </span><span class="caption-text">die</span><a class="headerlink" href="#die" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">die</span><span class="p">():</span>
    <span class="n">outcomes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">outcomes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Suppose we suspect that the die is biased. What could we do to evaluate
this possibility? A scientific way to answer this question is to collect
data and analyze it. Using Python we can simulate the data collection as
in Code Block <a class="reference internal" href="#experiment"><span class="std std-ref">experiment</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="experiment">
<div class="code-block-caption"><span class="caption-number">Listing 11.2 </span><span class="caption-text">experiment</span><a class="headerlink" href="#experiment" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">experiment</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">die</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">sample</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="si">:</span><span class="s2">.2g</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">experiment</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1: 0
2: 0.1
3: 0.4
4: 0.1
5: 0.4
6: 0
</pre></div>
</div>
<p>The numbers in the first column are the possible outcomes. Those in the
second column correspond to the frequency with which each number
appears. Frequency is the number of times each of the possible outcomes
appears divided by <code class="docutils literal notranslate"><span class="pre">N</span></code>, the total number of times we roll the die.</p>
<p>There are at least two things to take note of in this example. First if
we execute <code class="docutils literal notranslate"><span class="pre">experiment()</span></code> several times, we will see that we obtain a
different result each time. This is precisely the reason for using dice
in games of chance, every time we roll them we get a number that we
cannot predict. Second, if we roll the same die many times, the ability
to predict each single outcome does not improve. Nevertheless, data
collection and analysis can actually help us to estimate the <em>list of
frequencies</em> for the outcomes, in fact the ability improves as the value
of <code class="docutils literal notranslate"><span class="pre">N</span></code> increases. Run the experiment for <code class="docutils literal notranslate"><span class="pre">N=10000</span></code> times you will see
that the frequencies obtained are approximately <span class="math notranslate nohighlight">\(0.17\)</span> and it turns out
that <span class="math notranslate nohighlight">\(0.17 \approx \frac{1}{6}\)</span> which is what we would expect if each
number on the die had the same chance.</p>
<p>These two observations are not restricted to dice and games of chance.
If we weigh ourselves every day we would obtain different values, since
our weight is related to the amount of food we eat, the water we drink,
how many times we went to the bathroom, the precision of the scale, the
clothes we wear and many other factors. Therefore, a single measurement
may not be <em>representative</em> of our weight. Of course it could be that
the variations are small and we do not consider them important, but we
are getting ahead of ourselves. At this point what matters is that the
data measurement and/or collection is accompanied by uncertainty.
Statistics is basically the field concerned with how to deal with
uncertainty in practical problems and Probability theory is one of the
theoretical pillars of statistics. Probability theory help us formalize
discussion like the one we just had and extend them beyond dice. This is
so we can better ask and answer questions related to expected outcomes,
such as what happens when we increase the number of experiments, what
event has higher chance than another, etc.</p>
<section id="probability">
<span id="id4"></span><h3><span class="section-number">11.1.1. </span>Probability<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h3>
<p>Probability is the mathematical device that allow us to quantify
uncertainty in a principled way. Like other mathematical objects and
theories, they can be justified entirely from a pure mathematical
perspective. Nevertheless, from a practical point of view we can justify
them as <em>naturally</em> arising from performing experiments, collecting
observational data and even when doing computational simulations. For
simplicity we will talk about experiments, knowing we are using this
term in a very broad sense. To think about probabilities we can think in
terms of mathematical sets. The <strong>sample space</strong> <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is the
set of all possible events from an <strong>experiment</strong>. An <strong>event</strong> <span class="math notranslate nohighlight">\(A\)</span> is a
subset of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>. We say <span class="math notranslate nohighlight">\(A\)</span> happens when we perform an
experiment and we get <span class="math notranslate nohighlight">\(A\)</span> as the outcome. For a typical 6 face die we
can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-sample-space-dice">
<span class="eqno">(11.1)<a class="headerlink" href="#equation-eq-sample-space-dice" title="Permalink to this equation">¶</a></span>\[\mathcal{X} = \{1, 2, 3, 4, 5, 6\}
    \]</div>
<p>We may define the event <span class="math notranslate nohighlight">\(A\)</span> as any subset of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, for example,
getting an even number, <span class="math notranslate nohighlight">\(A = \{2, 4, 6\}\)</span>. We can associate
probabilities to events. If we want to indicate the probability of the
event <span class="math notranslate nohighlight">\(A\)</span> we write <span class="math notranslate nohighlight">\(P(A=\{2, 4, 6\})\)</span> or to be more concise <span class="math notranslate nohighlight">\(P(A)\)</span>. The
probability function <span class="math notranslate nohighlight">\(P\)</span> takes an event <span class="math notranslate nohighlight">\(A\)</span> which is a subset of
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as input and returns <span class="math notranslate nohighlight">\(P(A)\)</span>. Probabilities, like <span class="math notranslate nohighlight">\(P(A)\)</span>,
can take any number in the interval 0 to 1 (including both extremes),
using interval notation we write this is as <span class="math notranslate nohighlight">\([0, 1]\)</span>, the brackets
meaning inclusive bounds. If the event never happens then the
probability of that event is 0, for example <span class="math notranslate nohighlight">\(P(A=-1)=0\)</span>, if the event
always happens then it has probability 1, for example
<span class="math notranslate nohighlight">\(P(A=\{1,2,3,4,5,6\})=1\)</span>. We say events are disjoint if they can not
happen together, for example, if the event <span class="math notranslate nohighlight">\(A_1\)</span> represent odds numbers
and <span class="math notranslate nohighlight">\(A_2\)</span> even numbers, then the probability of rolling a die an getting
both <span class="math notranslate nohighlight">\(A_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span> is 0. If the event <span class="math notranslate nohighlight">\(A_1, A_2, \cdots A_n\)</span> are
disjoint, meaning they can not happen at the same time, then
<span class="math notranslate nohighlight">\(\sum_i^n P(A_i) = 1\)</span>. Continuing with the example of <span class="math notranslate nohighlight">\(A_1\)</span> representing
odd numbers and <span class="math notranslate nohighlight">\(A_2\)</span> even numbers, then the probability of rolling a
die and getting either <span class="math notranslate nohighlight">\(A_1\)</span> or <span class="math notranslate nohighlight">\(A_2\)</span> is 1. Any function that satisfies
these properties is a valid probability function. We can think of
probability as a positive, conserved quantity that is allocated across
the possible events <a class="footnote-reference brackets" href="#id101" id="id5">3</a>.</p>
<p>As we just saw, probabilities have a clear mathematical definition. How
we interpret probabilities is a different story with different schools
of thought. As Bayesian we tend to interpret probabilities as degrees of
uncertainty. For example, for a fair die, the probability of getting an
odd number when rolling a die is <span class="math notranslate nohighlight">\(0.5\)</span>, meaning we are half-sure we will
get an odd number. Alternatively we can interpret this number as saying
if we roll a die infinite times, half the time we will get odd numbers
and half the time even numbers. This is the frequentist interpretation
which is also a useful way of thinking about probabilities. If you do
not want to roll the die infinite times you can just roll it a large
number of times and say that you will approximately get odds half of the
time. This is in fact what we did in Code Block
<a class="reference internal" href="#experiment"><span class="std std-ref">experiment</span></a>. Finally, we notice that for a
fair die we expect the probability of getting any single number to be
<span class="math notranslate nohighlight">\(\frac{1}{6}\)</span>, but for a non-fair die this probability may be different.
The equiprobability of the outcomes is just a special case.</p>
<p>If probabilities reflect uncertainty, then is <em>natural</em> to ask what is
the probability that the mass of Mars is <span class="math notranslate nohighlight">\(6.39 \times 10^{23}\)</span> kg, or
the probability of rain on May 1 in Helsinki, or the probability that
capitalism gets replaced by a different socio-economical system in the
next 3 decades. We say this definition of probability is epistemic,
since it is not about a property of the <em>real world</em> (whatever that is)
but a property about our knowledge about that world. We collect data and
analyze it, because we think that we can update our internal knowledge
state based on external information.</p>
<p>We notice that what can happen in the <em>real world</em> is determined by all
the details of the experiments, even those we do not control or are not
aware of. On the contrary the sample space is a mathematical object that
we define either implicitly or explicitly. For example, by defining the
sample space of our die as in Equation <a class="reference internal" href="#equation-eq-sample-space-dice">(11.1)</a> we are
ruling out the possibility of the die landing on an edge, which is
actually possible when rolling dice in a non-flat surface. Elements from
the sample space may be excluded on purpose, for example, we may have
designed the experiment in such a way that we roll the die until we get
one integer from <span class="math notranslate nohighlight">\(\{1, 2, 3, 4, 5, 6\}\)</span>. Or by omission, like in a
survey we may ask people about their gender, but if we only include
female or male as options in the possible response, we may force people
to choose between two non-adequate answers or completely miss their
answers as they may not feel interested in responding to the rest of the
survey. We must be aware that the platonic world of ideas which includes
all mathematical concepts is different from the <em>real world</em>, in
statistical modeling we constantly switch back and forth between these
two worlds.</p>
</section>
<section id="conditional-probability">
<span id="id6"></span><h3><span class="section-number">11.1.2. </span>Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h3>
<p>Given two events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> with <span class="math notranslate nohighlight">\(P(B) &gt; 0\)</span>, the probability of <span class="math notranslate nohighlight">\(A\)</span>
given <span class="math notranslate nohighlight">\(B\)</span>, which we write as <span class="math notranslate nohighlight">\(P(A \mid B)\)</span> is defined as :</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-probability">
<span class="eqno">(11.2)<a class="headerlink" href="#equation-eq-conditional-probability" title="Permalink to this equation">¶</a></span>\[P(A \mid B) = \frac{P(A, B)}{P(B)}\]</div>
<p><span class="math notranslate nohighlight">\(P(A, B)\)</span> is the probability that the events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> occur, it is
also usually written as <span class="math notranslate nohighlight">\(P(A \cap B)\)</span> (the symbol <span class="math notranslate nohighlight">\(\cap\)</span> indicates
intersection of sets), the probability of the intersection of the events
<span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p><span class="math notranslate nohighlight">\(P(A \mid B)\)</span> it is known as conditional probability, and it is the
probability that event <span class="math notranslate nohighlight">\(A\)</span> occurs, conditioned by the fact that we know
(or assume, imagine, hypothesize, etc) that <span class="math notranslate nohighlight">\(B\)</span> has occurred. For
example, the probability that a sidewalk is wet is different from the
probability that such a sidewalk is wet given that it is raining.</p>
<p>A conditional probability can be viewed as the reduction or restriction
of the sample space. <a class="reference internal" href="#fig-cond"><span class="std std-numref">Fig. 11.1</span></a> shows how we went from having
the events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> in the sample space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, on the left,
to having <span class="math notranslate nohighlight">\(B\)</span> as the sample space and a subset of <span class="math notranslate nohighlight">\(A\)</span>, the one being
compatible with <span class="math notranslate nohighlight">\(B\)</span>. When we say <em>that <span class="math notranslate nohighlight">\(B\)</span> has occurred</em> we are not
necessarily talking about something in the past, it is just a more
colloquial way of saying, <em>once we have conditioned on <span class="math notranslate nohighlight">\(B\)</span></em> or <em>once we
have restricted the sample space to agree with the evidence <span class="math notranslate nohighlight">\(B\)</span></em>.</p>
<figure class="align-default" id="fig-cond">
<a class="reference internal image-reference" href="../_images/cond.png"><img alt="../_images/cond.png" src="../_images/cond.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.1 </span><span class="caption-text">Conditioning is redefining the sample space. On the left we see the
sample <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, each circle represent a possible outcome. We have
two events, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. On the right we have represented <span class="math notranslate nohighlight">\(P(A \mid B)\)</span>,
once we know <span class="math notranslate nohighlight">\(B\)</span> we can rule out all events not in <span class="math notranslate nohighlight">\(B\)</span>. This figure was
adapted from Introduction to Probability <span id="id7">[<a class="reference internal" href="references.html#id10" title="J.K. Blitzstein and J. Hwang. Introduction to Probability, Second Edition. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2019. ISBN 9780429766732.">5</a>]</span>.</span><a class="headerlink" href="#fig-cond" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The concept of conditional probability is at the heart of statistics and
is central to thinking about how we should update our knowledge of an
event in the light of new data. All probabilities are conditional, with
respect to some assumption or model. Even when we do not express it
explicitly, there are no probabilities without context.</p>
</section>
<section id="probability-distribution">
<span id="id8"></span><h3><span class="section-number">11.1.3. </span>Probability Distribution<a class="headerlink" href="#probability-distribution" title="Permalink to this headline">¶</a></h3>
<p>Instead of calculating the probability of obtaining the number 5 when
rolling a die, we may be more interested in finding out the <em>list of
probabilities</em> for all numbers on a die. Once this list is computed we
can display it or use it to compute other quantities like the
probability of getting the number 5, or the probability of getting a
number equal or larger than 5. The formal name of this <em>list</em> is
<strong>probability distribution</strong>.</p>
<p>Using Code Block <a class="reference internal" href="#experiment"><span class="std std-ref">experiment</span></a> we obtained an
empirical probability distribution of a die, that is, a distribution
calculated from data. But there are also theoretical distributions,
which are central in statistics among other reasons because they allow
the construction of probabilistic models.</p>
<p>Theoretical probability distributions have precise mathematical
formulas, similar to how circles have a precise mathematical definition.
A circle is the geometric space of points on a plane that are
equidistant from another point called the center. Given the parameter
radius, a circle is perfectly defined <a class="footnote-reference brackets" href="#id103" id="id9">4</a>. We could say that there is
not a single circumference, but a family of circumferences where each
member differs from the rest only by the value of the parameter radius,
since once this parameter is also defined, the circumference is defined.</p>
<p>Similarly, probability distributions come in families, whose members are
perfectly defined by one or more parameters. It is common to write the
parameter names with letters of the Greek alphabet, although this is not
always the case. <a class="reference internal" href="#fig-dice-distribution"><span class="std std-numref">Fig. 11.2</span></a> is an example of such
families of distributions, that we may use to represent a loaded die. We
can see how this probability distribution is controlled by two
parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. If we change them the <em>shape</em> of the
distribution changes, we can make it flat or concentrated towards one
side, push most of the mass the extremes, or concentrated the mass in
the middle. As the radius of the circumference is restricted to be
positive, the parameters of distributions also have restrictions, in
fact <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> must both be positive.</p>
<figure class="align-default" id="fig-dice-distribution">
<a class="reference internal image-reference" href="../_images/dice_distribution.png"><img alt="../_images/dice_distribution.png" src="../_images/dice_distribution.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.2 </span><span class="caption-text">Four members of a family of discrete distributions with parameters
<span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. The height of the bars represents the probability
of each <span class="math notranslate nohighlight">\(x\)</span> value. The values of <span class="math notranslate nohighlight">\(x\)</span> not drawn have probability 0 as
they are out of the support of the distribution.</span><a class="headerlink" href="#fig-dice-distribution" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="discrete-random-variables-and-distributions">
<span id="id10"></span><h3><span class="section-number">11.1.4. </span>Discrete Random Variables and Distributions<a class="headerlink" href="#discrete-random-variables-and-distributions" title="Permalink to this headline">¶</a></h3>
<p>A random variable is a function that maps the sample space into the real
numbers <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Continuing with the die example if the events of
interest were the number of the die, the mapping is very simple, we
associate <span class="math notranslate nohighlight">\(\LARGE \unicode{x2680}\)</span> with the number 1, <span class="math notranslate nohighlight">\(\LARGE \unicode{x2681}\)</span> with 2, etc.
With two dice we could have an <span class="math notranslate nohighlight">\(S\)</span> random variable as the sum of the
outcomes of both dice. Thus the domain of the random variable <span class="math notranslate nohighlight">\(S\)</span> is
<span class="math notranslate nohighlight">\(\{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}\)</span>, and if both dice are fair then
their probability distribution is depicted in
<a class="reference internal" href="#fig-sum-dice-distribution"><span class="std std-numref">Fig. 11.3</span></a>.</p>
<figure class="align-default" id="fig-sum-dice-distribution">
<a class="reference internal image-reference" href="../_images/sum_dice_distribution.png"><img alt="../_images/sum_dice_distribution.png" src="../_images/sum_dice_distribution.png" style="width: 3.5in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.3 </span><span class="caption-text">If the sample space is the set of possible numbers rolled on two dice,
and the random variable of interest is the sum <span class="math notranslate nohighlight">\(S\)</span> of the numbers on the
two dice, then <span class="math notranslate nohighlight">\(S\)</span> is a discrete random variable whose distribution is
described in this figure with the probability of each outcome
represented as the height of the columns. This figure has been adapted
from
<a class="reference external" href="https://commons.wikimedia.org/wiki/File:Dice_Distribution_(bar).svg">https://commons.wikimedia.org/wiki/File:Dice_Distribution_(bar).svg</a></span><a class="headerlink" href="#fig-sum-dice-distribution" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We could also define another random variable <span class="math notranslate nohighlight">\(C\)</span> with sample space
<span class="math notranslate nohighlight">\(\{\text{red}, \text{green}, \text{blue}\}\)</span>. We could map the sample
space to <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> in the following way:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
C(\text{red})\; = 0 \\
C(\text{green}) = 1 \\
C(\text{blue})\, = 2\end{aligned}\end{split}\]</div>
<p>This encoding is useful, because performing math with numbers is easier
than with strings regardless of whether we are using analog computation
on “pen and paper” or digital computation with a computer.</p>
<p>As we said a random variable is a function, and given that the mapping
between the sample space and <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> is deterministic it is not
immediately clear where the randomness in a random variable comes from.
We say a variable is random in the sense that if we perform an
experiment, i.e. we <em>ask</em> the variable for a value like we did in Code
Block <a class="reference internal" href="#die"><span class="std std-ref">die</span></a> and
<a class="reference internal" href="#experiment"><span class="std std-ref">experiment</span></a> we will get a different number
each time without the succession of outcomes following a deterministic
pattern. For example, if we ask for the value of random variable <span class="math notranslate nohighlight">\(C\)</span> 3
times in a row we may get red, red, blue or maybe blue, green, blue,
etc.</p>
<p>A random variable <span class="math notranslate nohighlight">\(X\)</span> is said to be discrete if there is a finite list
of values <span class="math notranslate nohighlight">\(a_1, a_2, \dots, a_n\)</span> or an infinite list of values
<span class="math notranslate nohighlight">\(a_1, a_2, \dots\)</span> such that the total probability is
<span class="math notranslate nohighlight">\(\sum_j P(X=a_j) = 1\)</span>. If <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable then a
finite or countably infinite set of values <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(P(X = x) &gt; 0\)</span>
is called the <em>support</em> of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>As we said before we can think of a probability distribution as a list
associating a probability with each event. Additionally a random
variable has a probability distribution associated to it. In the
particular case of discrete random variables the probability
distribution is also called a Probability Mass Function (PMF). It is
important to note that the PMF is a function that returns probabilities.
The PMF of <span class="math notranslate nohighlight">\(X\)</span> is the function <span class="math notranslate nohighlight">\(P(X=x)\)</span> for <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span>. For a
PMF to be valid, it must be non-negative and sum to 1, i.e. all its
values should be non-negative and the sum over all its domain should be
1.</p>
<p>It is important to remark that the term <em>random</em> in random variable does
not implies that any value is allowed, only those in the sample space.
For example, we can not get the value orange from <span class="math notranslate nohighlight">\(C\)</span>, nor the value 13
from <span class="math notranslate nohighlight">\(S\)</span>. Another common source of confusion is that the term random
implies equal probability, but that is not true, the probability of each
event is given by the PMF, for example, we may have
<span class="math notranslate nohighlight">\(P(C=\text{red}) = \frac{1}{2}, P(C=\text{green}) = \frac{1}{4}, P(C=\text{blue}) = \frac{1}{4}\)</span>.
The equiprobability is just a special case.</p>
<p>We can also define a discrete random variable using a cumulative
distribution function (CDF). The CDF of a random variable <span class="math notranslate nohighlight">\(X\)</span> is the
function <span class="math notranslate nohighlight">\(F_X\)</span> given by <span class="math notranslate nohighlight">\(F_X(x) = P(X \le x)\)</span>. For a CDF to be valid, it
must be monotonically increasing <a class="footnote-reference brackets" href="#id104" id="id11">5</a>, right-continuous <a class="footnote-reference brackets" href="#id105" id="id12">6</a>, converge
to 0 as <span class="math notranslate nohighlight">\(x\)</span> approaches to <span class="math notranslate nohighlight">\(- \infty\)</span>, and converge to 1 as <span class="math notranslate nohighlight">\(x\)</span>
approaches <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
<p>In principle, nothing prevents us from defining our own probability
distribution. But there are many already defined distributions that are
so commonly used, they have their own names. It is a good idea to become
familiar with them as they appear quite often. If you check the models
defined in this book you will see that most of them use combinations of
predefined probability distributions and only a few examples used custom
defined distribution. For example, in Section <a class="reference internal" href="chp_08.html#abc-ma"><span class="std std-ref">Approximating Moving Averages</span></a> Code Block
<a class="reference internal" href="chp_08.html#ma2-abc"><span class="std std-ref">MA2_abc</span></a> we used a Uniform distribution and two
potentials to define a 2D triangular distribution.</p>
<p>Figures <a class="reference internal" href="#fig-discrete-uniform-pmf-cdf"><span class="std std-numref">Fig. 11.4</span></a>,
<a class="reference internal" href="#fig-binomial-pmf-cdf"><span class="std std-numref">Fig. 11.5</span></a>, and <a class="reference internal" href="#fig-poisson-pmf-cdf"><span class="std std-numref">Fig. 11.6</span></a>, are
example of some common discrete distribution represented with their PMF
and CDF. On the left we have the PMFs, the height of the bars represents
the probability of each <span class="math notranslate nohighlight">\(x\)</span>. On the right we have the CDF, here the
<em>jump</em> between two horizontal lines at a value of <span class="math notranslate nohighlight">\(x\)</span> represents the
probability of <span class="math notranslate nohighlight">\(x\)</span>. The figure also includes the values of the mean and
standard deviation of the distributions, is important to remark that
these values are properties of the distributions, like the length of a
circumference, and not something that we compute from a finite sample
(see Section <a class="reference internal" href="#expectations"><span class="std std-ref">Expectations</span></a> for details).</p>
<p>Another way to describe random variables is to use stories. A story for
<span class="math notranslate nohighlight">\(X\)</span> describes an experiment that could give rise to a random variable
with the same distribution as <span class="math notranslate nohighlight">\(X\)</span>. Stories are not formal devices, but
they are useful anyway. Stories have helped humans to make sense of
their surrounding for millennia and they continue to be useful today,
even in statistics. In the book Introduction to Probability
<span id="id13">[<a class="reference internal" href="references.html#id10" title="J.K. Blitzstein and J. Hwang. Introduction to Probability, Second Edition. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2019. ISBN 9780429766732.">5</a>]</span> Joseph K. Blitzstein and Jessica Hwang make extensive
use of this device. They even use story proofs extensively, these are
similar to mathematical proof but they can be more intuitive. Stories
are also very useful devices to create statistical models, you can think
about how the data may have been generated, and then try to write that
down in statistical notation and/or code. We do this, for example, in
Chapter <a class="reference internal" href="chp_09.html#chap9"><span class="std std-ref">9</span></a> with our flight delay example.</p>
<section id="discrete-uniform-distribution">
<span id="id14"></span><h4><span class="section-number">11.1.4.1. </span>Discrete Uniform Distribution<a class="headerlink" href="#discrete-uniform-distribution" title="Permalink to this headline">¶</a></h4>
<p>This distribution assigns equal probability to a finite set of
consecutive integers from interval a to b inclusive. Its PMF is:</p>
<div class="math notranslate nohighlight" id="equation-eq-pmf-uniform">
<span class="eqno">(11.3)<a class="headerlink" href="#equation-eq-pmf-uniform" title="Permalink to this equation">¶</a></span>\[P(X = x) = {\frac {1}{b - a + 1}} = \frac{1}{n}\]</div>
<p>for values of <span class="math notranslate nohighlight">\(x\)</span> in the interval <span class="math notranslate nohighlight">\([a, b]\)</span>, otherwise <span class="math notranslate nohighlight">\(P(X = x) = 0\)</span>,
where <span class="math notranslate nohighlight">\(n=b-a+1\)</span> is the total number values that <span class="math notranslate nohighlight">\(x\)</span> can take.</p>
<p>We can use this distribution to model, for example, a fair die. Code
Block <a class="reference internal" href="#scipy-unif"><span class="std std-ref">scipy_unif</span></a> shows how we can use Scipy
to define a distribution and then compute useful quantities such as the
PMF, CDF, and moments (see Section <a class="reference internal" href="#expectations"><span class="std std-ref">Expectations</span></a>).</p>
<div class="literal-block-wrapper docutils container" id="scipy-unif">
<div class="code-block-caption"><span class="caption-number">Listing 11.3 </span><span class="caption-text">scipy_unif</span><a class="headerlink" href="#scipy-unif" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">rv</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">b</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">x_pmf</span> <span class="o">=</span> <span class="n">rv</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># evaluate the pmf at the x values</span>
<span class="n">x_cdf</span> <span class="o">=</span> <span class="n">rv</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># evaluate the cdf at the x values</span>
<span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">rv</span><span class="o">.</span><span class="n">stats</span><span class="p">(</span><span class="n">moments</span><span class="o">=</span><span class="s2">&quot;mv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Using Code Block <a class="reference internal" href="#scipy-unif"><span class="std std-ref">scipy_unif</span></a> plus a few lines
of Matplotlib we generate <a class="reference internal" href="#fig-discrete-uniform-pmf-cdf"><span class="std std-numref">Fig. 11.4</span></a>. On the
left panel we have the PMF where the height of each point indicates the
probability of each event, we use points and dotted lines to highlight
that the distribution is discrete. On the right we have the CDF, the
height of the jump at each value of <span class="math notranslate nohighlight">\(x\)</span> indicates the probability of
that value.</p>
<figure class="align-default" id="fig-discrete-uniform-pmf-cdf">
<a class="reference internal image-reference" href="../_images/discrete_uniform_pmf_cdf.png"><img alt="../_images/discrete_uniform_pmf_cdf.png" src="../_images/discrete_uniform_pmf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.4 </span><span class="caption-text">Discrete Uniform with parameters (1, 6). On the left the PMF. The height
of the lines represents the probabilities for each value of <span class="math notranslate nohighlight">\(x\)</span>. On the
right the CDF. The height of the jump at each value of <span class="math notranslate nohighlight">\(x\)</span> represent its
probability. Values outside of the support of the distribution are not
represented. The filled dots represent the inclusion of the CDF value at
a particular <span class="math notranslate nohighlight">\(x\)</span> value, the open dots reflect the exclusion.</span><a class="headerlink" href="#fig-discrete-uniform-pmf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In this specific example the discrete Uniform distribution is defined on
the interval <span class="math notranslate nohighlight">\([1, 6]\)</span>. Therefore, all values less than 1 and greater
than 6 have probability 0. Being a Uniform distribution, all the points
have the same height and that height is <span class="math notranslate nohighlight">\(\frac{1}{6}\)</span>. There are two
parameters of the Uniform discrete distribution, the lower limit <span class="math notranslate nohighlight">\(a\)</span> and
upper limit <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>As we already mentioned in this chapter if we change the parameters of a
distribution the <em>particular shape</em> of the distribution will change (try
for example, replacing <code class="docutils literal notranslate"><span class="pre">stats.randint(1,</span> <span class="pre">7)</span></code> in Code Block
<a class="reference internal" href="#scipy-unif"><span class="std std-ref">scipy_unif</span></a> with <code class="docutils literal notranslate"><span class="pre">stats.randint(1,</span> <span class="pre">4)</span></code>. That
is why we usually talk about family of distributions, each member of
that family is a distribution with a particular and valid combination of
parameters. Equation <a class="reference internal" href="#equation-eq-pmf-uniform">(11.3)</a> defines the family of discrete
Uniform distributions as long as <span class="math notranslate nohighlight">\(a &lt; b\)</span> and both <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are
integers.</p>
<p>When using probability distributions to create statistical applied
models it is common to link the parameters with quantities that make
physical sense. For example, in a 6 sided die it makes sense that <span class="math notranslate nohighlight">\(a=1\)</span>
and <span class="math notranslate nohighlight">\(b=6\)</span>. In probability we generally know the values of these
parameters while in statistics we generally do not know these values and
we use data to infer them.</p>
</section>
<section id="binomial-distribution">
<span id="id15"></span><h4><span class="section-number">11.1.4.2. </span>Binomial Distribution<a class="headerlink" href="#binomial-distribution" title="Permalink to this headline">¶</a></h4>
<p>A Bernoulli trial is an experiment with only two possible outcomes
yes/no (success/failure, happy/sad, ill/healthy, etc). Suppose we
perform <span class="math notranslate nohighlight">\(n\)</span> independent <a class="footnote-reference brackets" href="#id106" id="id16">7</a> Bernoulli trials, each with the same
success probability <span class="math notranslate nohighlight">\(p\)</span> and let us call <span class="math notranslate nohighlight">\(X\)</span> the number of success. Then
the distribution of <span class="math notranslate nohighlight">\(X\)</span> is called the Binomial distribution with
parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is a positive integer and
<span class="math notranslate nohighlight">\(p \in [0, 1]\)</span>. Using statistical notation we can write
<span class="math notranslate nohighlight">\(X \sim Bin(n, p)\)</span> to mean that <span class="math notranslate nohighlight">\(X\)</span> has the Binomial distribution with
parameters <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(p\)</span>, with the PMF being:</p>
<div class="math notranslate nohighlight" id="equation-eq-binomial-pmf">
<span class="eqno">(11.4)<a class="headerlink" href="#equation-eq-binomial-pmf" title="Permalink to this equation">¶</a></span>\[P(X = x) = \frac{n!}{x!(n-x)!}p^x(1-p)^{n-x}\]</div>
<p>The term <span class="math notranslate nohighlight">\(p^x(1-p)^{n-x}\)</span> counts the number of <span class="math notranslate nohighlight">\(x\)</span> success in <span class="math notranslate nohighlight">\(n\)</span>
trials. This term only considers the total number of success but not the
precise sequence, for example, <span class="math notranslate nohighlight">\((0,1)\)</span> is the same as <span class="math notranslate nohighlight">\((1,0)\)</span>, as both
have one success in two trials. The first term is known as Binomial
Coefficient and computes all the possible combinations of <span class="math notranslate nohighlight">\(x\)</span> elements
taken from a set of <span class="math notranslate nohighlight">\(n\)</span> elements.</p>
<p>The Binomial PMFs are often written omitting the values that return 0,
that is the values outside of the support. Nevertheless it is important
to be sure what the support of a random variable is in order to avoid
mistakes. A good practice is to check that PMFs are valid, and this is
essential if we are proposing a new PMFs instead of using one off the
<em>shelf</em>.</p>
<p>When <span class="math notranslate nohighlight">\(n=1\)</span> the Binomial distribution is also known as the Bernoulli
distribution. Many distributions are special cases of other
distributions or can be obtained somehow from other distributions.</p>
<figure class="align-default" id="fig-binomial-pmf-cdf">
<a class="reference internal image-reference" href="../_images/binomial_pmf_cdf.png"><img alt="../_images/binomial_pmf_cdf.png" src="../_images/binomial_pmf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.5 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\text{Bin}(n=4, p=0.5)\)</span> On the left the PMF. The height of the lines
represents the probabilities for each value of <span class="math notranslate nohighlight">\(x\)</span>. On the right the
CDF. The height of the jump at each value of <span class="math notranslate nohighlight">\(x\)</span> represent its
probability. Values outside of the support of the distribution are not
represented.</span><a class="headerlink" href="#fig-binomial-pmf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="poisson-distribution">
<span id="id17"></span><h4><span class="section-number">11.1.4.3. </span>Poisson Distribution<a class="headerlink" href="#poisson-distribution" title="Permalink to this headline">¶</a></h4>
<p>This distribution expresses the probability that <span class="math notranslate nohighlight">\(x\)</span> events happen
during a fixed time interval (or space interval) if these events occur
with an average rate <span class="math notranslate nohighlight">\(\mu\)</span> and independently from each other. It is
generally used when there are a large number of trials, each with a
small probability of success. For example</p>
<ul class="simple">
<li><p>Radioactive decay, the number of atoms in a given material is huge,
the actual number that undergo nuclear fission is low compared to
the total number of atoms.</p></li>
<li><p>The daily number of car accidents in a city. Even when we may
consider this number to be high relative to what we would prefer, it
is low in the sense that every maneuver that the driver performs,
including turns, stopping at lights, and parking, is an independent
trial where an accident could occur.</p></li>
</ul>
<p>The PMF of a Poisson is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-poisson-pmf">
<span class="eqno">(11.5)<a class="headerlink" href="#equation-eq-poisson-pmf" title="Permalink to this equation">¶</a></span>\[P(X = x)  = \frac{\mu^{x} e^{-\mu}}{x!}, x = 0, 1, 2, \dots
    \]</div>
<p>Notice that the support of this PMF are all the natural numbers, which
is an infinite set. So we have to be careful with our <em>list</em> of
probabilities analogy, as summing an infinite series can be tricky. In
fact Equation <a class="reference internal" href="#equation-eq-poisson-pmf">(11.5)</a> is a valid PMF because of the Taylor
series <span class="math notranslate nohighlight">\(\sum_0^{\infty} \frac{\mu^{x}}{x!} = e^{\mu}\)</span></p>
<p>Both the mean and variance of the Poisson distribution are defined by
<span class="math notranslate nohighlight">\(\mu\)</span>. As <span class="math notranslate nohighlight">\(\mu\)</span> increases, the Poisson distribution approximates to a
Normal distribution, although the latter is continuous and the Poisson
is discrete. The Poisson distribution is also closely related to the
Binomial distribution. A Binomial distribution can be approximated with
a Poisson, when <span class="math notranslate nohighlight">\(n &gt;&gt; p\)</span> <a class="footnote-reference brackets" href="#id107" id="id18">8</a>, that is, when the probability of success
(<span class="math notranslate nohighlight">\(p\)</span>) is low compared with the number o trials (<span class="math notranslate nohighlight">\(n\)</span>) then
<span class="math notranslate nohighlight">\(\text{Pois}(\mu=np) \approx \text{Bin}(n, p)\)</span>. For this reason the
Poisson distribution is also known as <em>the law of small numbers</em> or the
<em>law of rare events</em>. As we previously mentioned this does not mean that
<span class="math notranslate nohighlight">\(\mu\)</span> has to be small, but instead that <span class="math notranslate nohighlight">\(p\)</span> is low with respect to <span class="math notranslate nohighlight">\(n\)</span>.</p>
<figure class="align-default" id="fig-poisson-pmf-cdf">
<a class="reference internal image-reference" href="../_images/poisson_pmf_cdf.png"><img alt="../_images/poisson_pmf_cdf.png" src="../_images/poisson_pmf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.6 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\text{Pois}(2.3)\)</span> On the left the PMF. The height of the lines
represents the probabilities for each value of <span class="math notranslate nohighlight">\(x\)</span>. On the right the
CDF. The height of the jump at each value of <span class="math notranslate nohighlight">\(x\)</span> represent its
probability. Values outside of the support of the distribution are not
represented.</span><a class="headerlink" href="#fig-poisson-pmf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="continuous-random-variables-and-distributions">
<span id="cont-rvs"></span><h3><span class="section-number">11.1.5. </span>Continuous Random Variables and Distributions<a class="headerlink" href="#continuous-random-variables-and-distributions" title="Permalink to this headline">¶</a></h3>
<p>So far we have seen discrete random variables. There is another type of
random variable that is widely used called continuous random variables,
whose support takes values in <span class="math notranslate nohighlight">\(\mathbb {R}\)</span>. The most important
difference between discrete and continuous random variables is that the
latter can take on any <span class="math notranslate nohighlight">\(x\)</span> value in an interval, although the
probability of any <span class="math notranslate nohighlight">\(x\)</span> value is exactly 0. Introduced this way you may
think that these are the most useless probability distributions ever.
But that is not the case, the actual problem is that our analogy of
treating a probability distribution as a finite list is a very limited
analogy and it fails badly with continuous random variables <a class="footnote-reference brackets" href="#id108" id="id19">9</a>.</p>
<p>In Figures <a class="reference internal" href="#fig-discrete-uniform-pmf-cdf"><span class="std std-numref">Fig. 11.4</span></a>,
<a class="reference internal" href="#fig-binomial-pmf-cdf"><span class="std std-numref">Fig. 11.5</span></a>, and <a class="reference internal" href="#fig-poisson-pmf-cdf"><span class="std std-numref">Fig. 11.6</span></a>, to
represent PMFs (discrete variables), we used the height of the lines to
represent the probability of each event. If we add the heights we always
get 1, that is, the total sum of the probabilities. In a continuous
distribution we do not have <em>lines</em> but rather we have a continuous
curve, the height of that curve is not a probability but a <strong>probability
density</strong> and instead of of a PMF we use a Probability Density Function
(PDF). One important difference is that height of <span class="math notranslate nohighlight">\(\text{PDF}(x)\)</span> can be
larger than 1, as is not the probability value but a probability
density. To obtain a probability from a PDF instead we must integrate
over some interval:</p>
<div class="math notranslate nohighlight" id="equation-eq-pdf-to-prob">
<span class="eqno">(11.6)<a class="headerlink" href="#equation-eq-pdf-to-prob" title="Permalink to this equation">¶</a></span>\[P(a &lt; X &lt; b) =  \int_a^b pdf(x) dx\]</div>
<p>Thus, we can say that the area below the curve of the PDF (and not the
height as in the PMF) gives us a probability, the total area under the
curve, i.e. evaluated over the entire support of the PDF, must integrate
to 1. Notice that if we want to find out how much more likely the value
<span class="math notranslate nohighlight">\(x_1\)</span> is compared to <span class="math notranslate nohighlight">\(x_2\)</span> we can just compute
<span class="math notranslate nohighlight">\(\frac{pdf(x_1)}{pdf(x_2)}\)</span>.</p>
<p>In many texts, including this one, it is common to use the symbol <span class="math notranslate nohighlight">\(p\)</span> to
talk about the <span class="math notranslate nohighlight">\(pmf\)</span> or <span class="math notranslate nohighlight">\(pdf\)</span>. This is done in favour of generality and
hoping to avoid being very rigorous with the notation which can be an
actual burden when the difference can be more or less clear from the
context.</p>
<p>For a discrete random variable, the CDF jumps at every point in the
support, and is flat everywhere else. Working with the CDF of a discrete
random variable is awkward because of this jumpiness. Its derivative is
almost useless since it is undefined at the jumps and 0 everywhere else.
This is a problem for gradient-based sampling methods like Hamiltonian
Monte Carlo (Section <a class="reference internal" href="#inference-methods"><span class="std std-ref">Inference Methods</span></a>). On the contrary for
continuous random variables, the CDF is often very convenient to work
with, and its derivative is precisely the probability density function
(PDF) that we have discussed before.</p>
<p><a class="reference internal" href="#fig-cmf-pdf-pmf"><span class="std std-numref">Fig. 11.7</span></a> summarize the relationship between the CDF,
PDF and PMF. The transformations between discrete CDF and PMF on one
side and continuous CDF and PMF on the other are well defined and thus
we used arrows with solid lines. Instead the transformations between
discrete and continuous variables are more about numerical approximation
than well defined mathematical operations. To approximately get from a
discrete to a continuous distribution we use a smoothing method. One
form of smoothing is to use a continuous distribution instead of a
discrete one. To go from continuous to discrete we can discretize or bin
the continuous outcomes. For example, a Poisson distribution with a
large value of <span class="math notranslate nohighlight">\(\mu\)</span> approximately Gaussian <a class="footnote-reference brackets" href="#id109" id="id20">10</a>, while still being
discrete. For those cases using a scenarios using a Poisson or a
Gaussian maybe be interchangeable from a practical point of view. Using
ArviZ you can use <code class="docutils literal notranslate"><span class="pre">az.plot_kde</span></code> with discrete data to approximate a
continuous functions, how nice the results of this operation look
depends on many factors. As we already said it may look good for a
Poisson distribution with a relatively large value of <span class="math notranslate nohighlight">\(\mu\)</span>. When
calling <code class="docutils literal notranslate"><span class="pre">az.plot_bpv(.)</span></code> for a discrete variable, ArviZ will smooth it,
using an interpolation method, because the probability integral
transform only works for continuous variables.</p>
<figure class="align-default" id="fig-cmf-pdf-pmf">
<a class="reference internal image-reference" href="../_images/cmf_pdf_pmf.png"><img alt="../_images/cmf_pdf_pmf.png" src="../_images/cmf_pdf_pmf.png" style="width: 5.5in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.7 </span><span class="caption-text">Relationship between the CDF, PDF and PMF. Adapted from the book Think
Stats <span id="id21">[<a class="reference internal" href="references.html#id92" title="Allen B. Downey. Think Stats: Exploratory Data Analysis. O'Reilly Media;, 2014.">132</a>]</span>.</span><a class="headerlink" href="#fig-cmf-pdf-pmf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As we did with the discrete random variables, now we will see a few
example of continuous random variables with their PDF and CDF.</p>
<section id="continuous-uniform-distribution">
<span id="id22"></span><h4><span class="section-number">11.1.5.1. </span>Continuous Uniform Distribution<a class="headerlink" href="#continuous-uniform-distribution" title="Permalink to this headline">¶</a></h4>
<p>A continuous random variable is said to have a Uniform distribution on
the interval <span class="math notranslate nohighlight">\((a, b)\)</span> if its PDF is:</p>
<div class="math notranslate nohighlight" id="equation-eq-uniform-pdf">
<span class="eqno">(11.7)<a class="headerlink" href="#equation-eq-uniform-pdf" title="Permalink to this equation">¶</a></span>\[\begin{split}p(x \mid a,b)=\begin{cases} \frac{1}{b-a} &amp; if a \le x \le b \\ 0 &amp;  \text{otherwise} \end{cases}\end{split}\]</div>
<figure class="align-default" id="fig-uniform-pdf-cdf">
<a class="reference internal image-reference" href="../_images/uniform_pdf_cdf.png"><img alt="../_images/uniform_pdf_cdf.png" src="../_images/uniform_pdf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.8 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span> On the left the PDF, the black line represents the
probability density, the gray shaded area represents the probability
<span class="math notranslate nohighlight">\(P(0.25 &lt; X &lt; 0.75) =0.5\)</span>. On the right the CDF, the height of the gray
continuous segment represents <span class="math notranslate nohighlight">\(P(0.25 &lt; X &lt; 0.75) =0.5\)</span>. Values outside
of the support of the distribution are not represented.</span><a class="headerlink" href="#fig-uniform-pdf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The most commonly used Uniform distribution in statistics is
<span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span> also known as the standard Uniform. The PDF and CDF
for the standard Uniform are very simple: <span class="math notranslate nohighlight">\(p(x) = 1\)</span> and <span class="math notranslate nohighlight">\(F_{(x)} = x\)</span>
respectively, <a class="reference internal" href="#fig-uniform-pdf-cdf"><span class="std std-numref">Fig. 11.8</span></a> represents both of them,
this figure also indicated how to compute probabilities from the PDF and
CDF.</p>
</section>
<section id="gaussian-or-normal-distribution">
<span id="id23"></span><h4><span class="section-number">11.1.5.2. </span>Gaussian or Normal Distribution<a class="headerlink" href="#gaussian-or-normal-distribution" title="Permalink to this headline">¶</a></h4>
<p>This is perhaps the best known distribution <a class="footnote-reference brackets" href="#id110" id="id24">11</a>. On the one hand,
because many phenomena can be described approximately using this
distribution (thanks to central limit theorem, see Subsection
<a class="reference internal" href="#appendix-clt"><span class="std std-ref">The Central Limit Theorem</span></a> below). On the other hand, because it has certain
mathematical properties that make it easier to work with it
analytically.</p>
<p>The Gaussian distribution is defined by two parameters, the mean <span class="math notranslate nohighlight">\(\mu\)</span>
and the standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> as shown in Equation
<a class="reference internal" href="#equation-eq-gaussian-pdf">(11.8)</a>. A Gaussian distribution with <span class="math notranslate nohighlight">\(\mu=0\)</span> and
<span class="math notranslate nohighlight">\(\sigma=1\)</span> is known as the <strong>standard Gaussian distribution</strong>.</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-pdf">
<span class="eqno">(11.8)<a class="headerlink" href="#equation-eq-gaussian-pdf" title="Permalink to this equation">¶</a></span>\[    p (x \mid \mu, \sigma) = \frac {1} {\sigma \sqrt {2 \pi}} e^{-\frac {(x -\mu)^2} {2 \sigma^2}}\]</div>
<p>On the left panel of <a class="reference internal" href="#fig-normal-pdf-cdf"><span class="std std-numref">Fig. 11.9</span></a> we have the PDF, and
on the right we have the CDF. Both the PDF and CDF are represented for
the invertal -4, 4, but notice that the support of the Gaussian
distribution is the entire real line.</p>
<figure class="align-default" id="fig-normal-pdf-cdf">
<a class="reference internal image-reference" href="../_images/normal_pdf_cdf.png"><img alt="../_images/normal_pdf_cdf.png" src="../_images/normal_pdf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.9 </span><span class="caption-text">Representation of <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>, on the left the PDF, on the right
the CDF. The support of the Gaussian distribution is the entire real
line.</span><a class="headerlink" href="#fig-normal-pdf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="student-s-t-distribution">
<span id="students-t-distribution"></span><h4><span class="section-number">11.1.5.3. </span>Student’s t-distribution<a class="headerlink" href="#student-s-t-distribution" title="Permalink to this headline">¶</a></h4>
<p>Historically this distribution arose to estimate the mean of a normally
distributed population when the sample size is small <a class="footnote-reference brackets" href="#id111" id="id25">12</a>. In Bayesian
statistics, a common use case is to generate models that are robust
against aberrant data as we discussed in Section <a class="reference internal" href="chp_04.html#robust-regression"><span class="std std-ref">Robust Regression</span></a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-student-t-pdf">
<span class="eqno">(11.9)<a class="headerlink" href="#equation-eq-student-t-pdf" title="Permalink to this equation">¶</a></span>\[p (x \mid \nu, \mu, \sigma) = \frac {\Gamma (\frac {\nu + 1} {2})} {\Gamma (\frac{\nu} {2}) \sqrt {\pi \nu} \sigma} \left (1+ \frac{1}{\nu} \left (\frac {x- \mu} {\sigma} \right)^2 \right)^{-\frac{\nu + 1}{2}}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Gamma\)</span> is the gamma function <a class="footnote-reference brackets" href="#id112" id="id26">13</a> and <span class="math notranslate nohighlight">\(\nu\)</span> is commonly called
degrees of freedom. We also like the name degree of normality, since as
<span class="math notranslate nohighlight">\(\nu\)</span> increases, the distribution approaches a Gaussian. In the extreme
case of <span class="math notranslate nohighlight">\(\lim_{\nu \to \infty}\)</span> the distribution is exactly equal to a
Gaussian distribution with the same mean and standard deviation equal to
<span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(\nu=1\)</span> we get the Cauchy distribution <a class="footnote-reference brackets" href="#id113" id="id27">14</a>. Which is similar to a
Gaussian but with tails decreasing very slowly, so slowly that this
distribution does not have a defined mean or variance. That is, it is
possible to calculate a mean from a data set, but if the data came from
a Cauchy distribution, the spread around the mean will be high and this
spread will not decrease as the sample size increases. The reason for
this strange behavior is that distributions, like the Cauchy, are
dominated by the tail behavior of the distribution, contrary to what
happens with, for example, the Gaussian distribution.</p>
<p>For this distribution <span class="math notranslate nohighlight">\(\sigma\)</span> is not the standard deviation, which as
already said could be undefined, <span class="math notranslate nohighlight">\(\sigma\)</span> is the scale. As <span class="math notranslate nohighlight">\(\nu\)</span>
increases the scale converges to the standard deviation of a Gaussian
distribution.</p>
<p>On the left panel of <a class="reference internal" href="#fig-student-t-pdf-cdf"><span class="std std-numref">Fig. 11.10</span></a> we have the PDF,
and on the right we have the CDF. Compare with
<a class="reference internal" href="#fig-normal-pdf-cdf"><span class="std std-numref">Fig. 11.9</span></a>, a standard normal and see how the tails
are heavier for the Student T distribution with parameter
<span class="math notranslate nohighlight">\(\mathcal{T}(\nu=4, \mu=0, \sigma=1)\)</span></p>
<figure class="align-default" id="fig-student-t-pdf-cdf">
<a class="reference internal image-reference" href="../_images/student_t_pdf_cdf.png"><img alt="../_images/student_t_pdf_cdf.png" src="../_images/student_t_pdf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.10 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\mathcal{T}(\nu=4, \mu=0, \sigma=1)\)</span> On the left the PDF, on the right
the CDF. The support of the Students T distribution is the entire real
line.</span><a class="headerlink" href="#fig-student-t-pdf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="beta-distribution">
<span id="id28"></span><h4><span class="section-number">11.1.5.4. </span>Beta Distribution<a class="headerlink" href="#beta-distribution" title="Permalink to this headline">¶</a></h4>
<p>The Beta distribution is defined in the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>. It can be
used to model the behavior of random variables limited to a finite
interval, for example, modeling proportions or percentages.</p>
<div class="math notranslate nohighlight" id="equation-eq-beta-pdf">
<span class="eqno">(11.10)<a class="headerlink" href="#equation-eq-beta-pdf" title="Permalink to this equation">¶</a></span>\[p (x \mid \alpha, \beta) = \frac {\Gamma (\alpha + \beta)} {\Gamma(\alpha) \Gamma (\beta)} \, x^{\alpha-1} (1 -x)^{\beta-1}\]</div>
<p>The first term is a normalization constant that ensures that the PDF
integrates to 1. <span class="math notranslate nohighlight">\(\Gamma\)</span> is the Gamma function. When <span class="math notranslate nohighlight">\(\alpha = 1\)</span> and
<span class="math notranslate nohighlight">\(\beta = 1\)</span> the Beta distribution reduces to the standard Uniform
distribution. In <a class="reference internal" href="#fig-beta-pdf-cdf"><span class="std std-numref">Fig. 11.11</span></a> we show a
<span class="math notranslate nohighlight">\(\text{Beta}(\alpha=5, \beta=2)\)</span> distribution.</p>
<figure class="align-default" id="fig-beta-pdf-cdf">
<a class="reference internal image-reference" href="../_images/beta_pdf_cdf.png"><img alt="../_images/beta_pdf_cdf.png" src="../_images/beta_pdf_cdf.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.11 </span><span class="caption-text"><span class="math notranslate nohighlight">\(\text{Beta}(\alpha=5, \beta=2)\)</span> On the left the PDF, on the right the
CDF. The support of the Beta distribution is on the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>.</span><a class="headerlink" href="#fig-beta-pdf-cdf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>If we want to express the Beta distribution as a function of the mean
and the dispersion around the mean, we can do it in the following way.
<span class="math notranslate nohighlight">\(\alpha = \mu \kappa\)</span>, <span class="math notranslate nohighlight">\(\beta = (1 - \mu) \kappa\)</span> where <span class="math notranslate nohighlight">\(\mu\)</span> the mean
and <span class="math notranslate nohighlight">\(\kappa\)</span> a parameter called concentration as <span class="math notranslate nohighlight">\(\kappa\)</span> increases the
dispersion decreases. Also note that <span class="math notranslate nohighlight">\(\kappa = \alpha + \beta\)</span>.</p>
</section>
</section>
<section id="joint-conditional-and-marginal-distributions">
<span id="id29"></span><h3><span class="section-number">11.1.6. </span>Joint, Conditional and Marginal Distributions<a class="headerlink" href="#joint-conditional-and-marginal-distributions" title="Permalink to this headline">¶</a></h3>
<p>Let us assume we have two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> with the same PMF
<span class="math notranslate nohighlight">\(\text{Bin}(1, 0.5)\)</span>. Are they dependent or independent? If <span class="math notranslate nohighlight">\(X\)</span>
represent heads in on coin toss and <span class="math notranslate nohighlight">\(Y\)</span> heads in another coin toss then
they are independent. But if they represent heads and tails,
respectively, on the same coin toss, then they are dependent. Thus even
when individual (formally known as univariate) PMFs/PDFs fully
characterize individual random variables, they do not have information
about how the individual random variables are related to other random
variables. To answer that question we need to know the <strong>joint</strong>
distribution, also known as multivariate distributions. If we consider
that <span class="math notranslate nohighlight">\(p(X)\)</span> provides all the information about the probability of
finding <span class="math notranslate nohighlight">\(X\)</span> on the real line, in a similar way <span class="math notranslate nohighlight">\(p(X, Y)\)</span>, the joint
distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, provides all the information about the
probability of finding the tuple <span class="math notranslate nohighlight">\((X, Y)\)</span> on the plane. Joint
distributions allow us to describe the behavior of multiple random
variables that arise from the same experiment, for example, the
posterior distribution is the joint distribution of all parameters in
the model after we have conditioned the model on observed data.</p>
<p>The joint PMF is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-pmf">
<span class="eqno">(11.11)<a class="headerlink" href="#equation-eq-joint-pmf" title="Permalink to this equation">¶</a></span>\[p_{X,Y}(x, y) = P(X = x, Y = y)\]</div>
<p>The definition for <span class="math notranslate nohighlight">\(n\)</span> discrete random variable is similar, we just need
to include <span class="math notranslate nohighlight">\(n\)</span> terms. Similarly to univariate PMFs valid joint PMFs must
be nonnegative and sum to 1, where the sum is taken over all possible
values.</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-pmf-sum1">
<span class="eqno">(11.12)<a class="headerlink" href="#equation-eq-joint-pmf-sum1" title="Permalink to this equation">¶</a></span>\[\sum_x \sum_y P(X=x, Y=y) = 1\]</div>
<p>In a similar way the joint CDF of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-cdf">
<span class="eqno">(11.13)<a class="headerlink" href="#equation-eq-joint-cdf" title="Permalink to this equation">¶</a></span>\[F_{X,Y}(x, y) = P(X \le x, Y \le y)\]</div>
<p>Given the joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we can get the distribution
of <span class="math notranslate nohighlight">\(X\)</span> by summing over all the possible values of <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-to-marginal">
<span class="eqno">(11.14)<a class="headerlink" href="#equation-eq-joint-to-marginal" title="Permalink to this equation">¶</a></span>\[P(X=x) = \sum_y P(X=x, Y=y)\]</div>
<figure class="align-default" id="fig-joint-dist-marginal">
<a class="reference internal image-reference" href="../_images/joint_dist_marginal.png"><img alt="../_images/joint_dist_marginal.png" src="../_images/joint_dist_marginal.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.12 </span><span class="caption-text">The black lines represent the joint distribution of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The
blue lines in the marginal distribution of <span class="math notranslate nohighlight">\(x\)</span> are obtained by adding
the heights of the lines along the y-axis for every value of <span class="math notranslate nohighlight">\(x\)</span>.</span><a class="headerlink" href="#fig-joint-dist-marginal" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In previous section we called <span class="math notranslate nohighlight">\(P(X=x)\)</span> the PMF of <span class="math notranslate nohighlight">\(X\)</span>, or just the
distribution of <span class="math notranslate nohighlight">\(X\)</span>, when working with joint distributions we often call
it the <strong>marginal</strong> distribution of <span class="math notranslate nohighlight">\(X\)</span>. We do this to emphasize we are
talking about the <em>individual</em> <span class="math notranslate nohighlight">\(X\)</span>, without any reference to <span class="math notranslate nohighlight">\(Y\)</span>. By
summing over all the possible values of <span class="math notranslate nohighlight">\(Y\)</span> we <em>get rid of <span class="math notranslate nohighlight">\(Y\)</span></em>.
Formally this process is known as <strong>marginalizing out <span class="math notranslate nohighlight">\(Y\)</span></strong>. To obtain
the PMF of <span class="math notranslate nohighlight">\(Y\)</span> we can proceed in a similar fashion, but summing over all
possible values of <span class="math notranslate nohighlight">\(X\)</span> instead. In the case of a joint distribution of
more than 2 variables we just need to sum over all <em>the other</em>
variables. <a class="reference internal" href="#fig-joint-dist-marginal"><span class="std std-numref">Fig. 11.12</span></a> illustrates this.</p>
<p>Given the joint distribution it is straightforward to get the marginals.
But going from the marginals to the joint distribution is not generally
possible unless we make further assumptions. In
<a class="reference internal" href="#fig-joint-dist-marginal"><span class="std std-numref">Fig. 11.12</span></a> we can see that there is just one way
to add heights of the bars along the y-axis or x-axis, but to do the
inverse we must <em>split</em> bars and there are infinite ways of making this
split.</p>
<p>We already introduced conditional distributions in Section
<a class="reference internal" href="#conditional-probability"><span class="std std-ref">Conditional Probability</span></a>, and in <a class="reference internal" href="#fig-cond"><span class="std std-numref">Fig. 11.1</span></a> we show that
conditioning is redefining the sample space.
<a class="reference internal" href="#fig-joint-dist-conditional"><span class="std std-numref">Fig. 11.13</span></a> demonstrates conditioning in the
context of a joint distribution of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. To condition on <span class="math notranslate nohighlight">\(Y=y\)</span> we
take the joint distribution at the <span class="math notranslate nohighlight">\(Y=y\)</span> value and forget about the
rest. i.e. those for which <span class="math notranslate nohighlight">\(Y \ne y\)</span>, this is similar as indexing a 2d
array and picking a single column or row. The <em>remaining</em> values of <span class="math notranslate nohighlight">\(X\)</span>,
those in bold in <a class="reference internal" href="#fig-joint-dist-conditional"><span class="std std-numref">Fig. 11.13</span></a> needs to sum 1 to
be a valid PMF, and thus we re-normalize by dividing by <span class="math notranslate nohighlight">\(P(Y=y)\)</span></p>
<figure class="align-default" id="fig-joint-dist-conditional">
<a class="reference internal image-reference" href="../_images/joint_dist_conditional.png"><img alt="../_images/joint_dist_conditional.png" src="../_images/joint_dist_conditional.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.13 </span><span class="caption-text">On the left the joint distribution of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The blue lines
represent the conditional distribution <span class="math notranslate nohighlight">\(p(x \ mid y=3)\)</span>. On the right we
plot the same conditional distribution separately. Notice that there are
as many conditional PMF of <span class="math notranslate nohighlight">\(x\)</span> as values of <span class="math notranslate nohighlight">\(y\)</span> and vice versa. We are
just highlighting one possibility.</span><a class="headerlink" href="#fig-joint-dist-conditional" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We define continuous joint CDFs as in Equation <a class="reference internal" href="#equation-eq-joint-cdf">(11.13)</a> the
same as with discrete variables and the joint PDFs as the derivative of
the CDFs with respect to <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. We require valid joint PDFs to be
nonnegative and integrate to 1. For continuous variables we can
marginalize variables out in a similar fashion we did for discrete ones,
with the difference that instead of a sum we need to compute an
integral.</p>
<div class="math notranslate nohighlight" id="equation-eq-marginal-pdf">
<span class="eqno">(11.15)<a class="headerlink" href="#equation-eq-marginal-pdf" title="Permalink to this equation">¶</a></span>\[pdf_X(x) = \int pdf_{X,Y} (x, y)dy\]</div>
<figure class="align-default" id="fig-joint-marginal-cond-continuous">
<a class="reference internal image-reference" href="../_images/joint_marginal_cond_continuous.png"><img alt="../_images/joint_marginal_cond_continuous.png" src="../_images/joint_marginal_cond_continuous.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.14 </span><span class="caption-text">At the center of the figure we have the joint probability <span class="math notranslate nohighlight">\(p(x, y)\)</span>
represented with a gray scale, darker for higher probability density. At
the top and right <em>margins</em> we have the marginal distributions <span class="math notranslate nohighlight">\(p(x)\)</span>
and <span class="math notranslate nohighlight">\(p(y)\)</span> respectively. The dashed lines represents the conditional
probability <span class="math notranslate nohighlight">\(p(x \mid y)\)</span> for 3 different values of <span class="math notranslate nohighlight">\(y\)</span>. We can think of
these as (renormalized) slices of the joint <span class="math notranslate nohighlight">\(p(x, y)\)</span> at a given value
of <span class="math notranslate nohighlight">\(y\)</span>.</span><a class="headerlink" href="#fig-joint-marginal-cond-continuous" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-colin-joint-marginals"><span class="std std-numref">Fig. 11.15</span></a> show another example of a a join
distribution with its marginals distribution. This is also a clear
example that going from the joint to the marginals is straightforward,
as there is a unique way of doing it, but the inverse is not possible
unless we introduce further assumptions. Joint distributions can also be
a hybrid of discrete and continuous distributions.
<a class="reference internal" href="#fig-mix-joint"><span class="std std-numref">Fig. 11.16</span></a> shows an example.</p>
<figure class="align-default" id="fig-colin-joint-marginals">
<a class="reference internal image-reference" href="../_images/colin_joint_marginals.png"><img alt="../_images/colin_joint_marginals.png" src="../_images/colin_joint_marginals.png" style="width: 5.50in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.15 </span><span class="caption-text">The PyMC3 logo as a sample from a joint distribution with its marginals.
This figure was created with imcmc <a class="reference external" href="https://github.com/ColCarroll/imcmc">https://github.com/ColCarroll/imcmc</a>
a library for turning 2D images into probability distributions and then
sampling from them to create images and gifs.</span><a class="headerlink" href="#fig-colin-joint-marginals" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-mix-joint">
<a class="reference internal image-reference" href="../_images/mix_joint.png"><img alt="../_images/mix_joint.png" src="../_images/mix_joint.png" style="width: 5.50in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.16 </span><span class="caption-text">A hybrid joint distribution in black. The marginals are represented in
blue, with <span class="math notranslate nohighlight">\(X\)</span> being distributed as a Gaussian and <span class="math notranslate nohighlight">\(Y\)</span> as a Poisson. It
is easy to see how for each value of <span class="math notranslate nohighlight">\(Y\)</span> we have a (Gaussian)
conditional distribution.</span><a class="headerlink" href="#fig-mix-joint" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="probability-integral-transform-pit">
<span id="id30"></span><h3><span class="section-number">11.1.7. </span>Probability Integral Transform (PIT)<a class="headerlink" href="#probability-integral-transform-pit" title="Permalink to this headline">¶</a></h3>
<p>The probability integral transform
(PIT), also known as the universality of the Uniform distribution,
states that given a random variable <span class="math notranslate nohighlight">\(X\)</span> with a continuous distribution
with cumulative distribution <span class="math notranslate nohighlight">\(F_X\)</span>, we can compute a random variable <span class="math notranslate nohighlight">\(Y\)</span>
with standard Uniform distribution as:</p>
<div class="math notranslate nohighlight" id="equation-eq-pit">
<span class="eqno">(11.16)<a class="headerlink" href="#equation-eq-pit" title="Permalink to this equation">¶</a></span>\[Y = F_X (X)\]</div>
<p>We can see this is true as follows, by the definition of the CDF of <span class="math notranslate nohighlight">\(Y\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-pit1">
<span class="eqno">(11.17)<a class="headerlink" href="#equation-eq-pit1" title="Permalink to this equation">¶</a></span>\[F_Y (y) = P(Y \leq y)\]</div>
<p>Replacing Equation <a class="reference internal" href="#equation-eq-pit">(11.16)</a> in the previous one</p>
<div class="math notranslate nohighlight" id="equation-eq-pit2">
<span class="eqno">(11.18)<a class="headerlink" href="#equation-eq-pit2" title="Permalink to this equation">¶</a></span>\[\begin{split}P(F_X (X) \leq y) \\\end{split}\]</div>
<p>Taking the inverse of <span class="math notranslate nohighlight">\(F_X\)</span> to both sides of the inequality</p>
<div class="math notranslate nohighlight" id="equation-eq-pit3">
<span class="eqno">(11.19)<a class="headerlink" href="#equation-eq-pit3" title="Permalink to this equation">¶</a></span>\[\begin{split}P(X \leq F^{-1}_X (y)) \\\end{split}\]</div>
<p>By the definition of CDF</p>
<div class="math notranslate nohighlight" id="equation-eq-pit4">
<span class="eqno">(11.20)<a class="headerlink" href="#equation-eq-pit4" title="Permalink to this equation">¶</a></span>\[F_X (F^{-1}_X (y))\]</div>
<p>Simplifying, we get the CDF of a standard Uniform distribution
<span class="math notranslate nohighlight">\(\mathcal{U}\)</span>(0, 1).</p>
<div class="math notranslate nohighlight" id="equation-eq-pit5">
<span class="eqno">(11.21)<a class="headerlink" href="#equation-eq-pit5" title="Permalink to this equation">¶</a></span>\[F_Y(y) = y\]</div>
<p>If we do not know the CDF <span class="math notranslate nohighlight">\(F_X\)</span> but we have samples from <span class="math notranslate nohighlight">\(X\)</span> we can
approximate it with the empirical CDF. <a class="reference internal" href="#fig-pit"><span class="std std-numref">Fig. 11.17</span></a> shows example
of this property generated with Code Block <a class="reference internal" href="#pit"><span class="std std-ref">pit</span></a></p>
<div class="literal-block-wrapper docutils container" id="pit">
<div class="code-block-caption"><span class="caption-number">Listing 11.4 </span><span class="caption-text">pit</span><a class="headerlink" href="#pit" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">200</span><span class="p">))</span>
<span class="n">dists</span> <span class="o">=</span> <span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">expon</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>


<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">xs</span><span class="p">)):</span>
    <span class="n">draws</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">draws</span><span class="p">)</span>
    <span class="c1"># PDF original distribution</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="c1"># Empirical CDF</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
    <span class="c1"># Kernel Density Estimation</span>
    <span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-pit">
<a class="reference internal image-reference" href="../_images/pit.png"><img alt="../_images/pit.png" src="../_images/pit.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.17 </span><span class="caption-text">On the first column we have the PDF of 3 different distributions. To
generate the plots in the middle column, we take 100000 draws from the
corresponding PDF compute the CDF for those draws. We can see these are
the CDF for the Uniform distribution. The last column is similar to the
middle one, except that instead of plotting the empirical CDF we use a a
kernel density estimator to approximate the PDF, which we can see that
is approximately Uniform. The figure was generated with Code Block
<a class="reference internal" href="#pit"><span class="std std-ref">pit</span></a>.</span><a class="headerlink" href="#fig-pit" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The probability integral transform is used as part of tests to evaluate
if a given dataset can be modeled as arising from a specified
distribution (or probabilistic model). In this book we have seen PIT
used behind both visual test <code class="docutils literal notranslate"><span class="pre">az.plot_loo_pit()</span></code> and
<code class="docutils literal notranslate"><span class="pre">az.plot_pbv(kind=&quot;u_values&quot;)</span></code>.</p>
<p>PIT can also be used to sample from distributions. If the random
variable <span class="math notranslate nohighlight">\(X\)</span> is distributed as <span class="math notranslate nohighlight">\(\mathcal{U}(0,1)\)</span>, then <span class="math notranslate nohighlight">\(Y = F^{-1}(X)\)</span>
has the distribution <span class="math notranslate nohighlight">\(F\)</span>. Thus to obtain samples from a distribution we
just need (pseudo)random number generator like <code class="docutils literal notranslate"><span class="pre">np.random.rand()</span></code> and
the inverse CDF of the distribution of interest. This may not be the
most efficient method, but its generality and simplicity are difficult
to beat.</p>
</section>
<section id="expectations">
<span id="id31"></span><h3><span class="section-number">11.1.8. </span>Expectations<a class="headerlink" href="#expectations" title="Permalink to this headline">¶</a></h3>
<p>The expectation is a single number summarizing the center of mass of a
distribution. For example, if <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable, then we
can compute its expectation as:</p>
<div class="math notranslate nohighlight" id="equation-eq-expectation">
<span class="eqno">(11.22)<a class="headerlink" href="#equation-eq-expectation" title="Permalink to this equation">¶</a></span>\[\mathbb{E}(X) = \sum_x x P(X = x)\]</div>
<p>As is often the case in statistics we want to also measure the spread,
or dispersion, of a distribution, for example, to represent uncertainty
around a point estimate like the mean. We can do this with the variance,
which itself is also an expectation:</p>
<div class="math notranslate nohighlight" id="equation-eq-var-as-expectation">
<span class="eqno">(11.23)<a class="headerlink" href="#equation-eq-var-as-expectation" title="Permalink to this equation">¶</a></span>\[\mathbb{V}(X) = \mathbb{E}(X - \mathbb{E}X)^2 = \mathbb{E}(X^2 ) - (\mathbb{E}X)^2\]</div>
<p>The variance often appears <em>naturally</em> in many computations, but to
report results it is often more useful to take the square root of the
variance, called the standard deviation, as this will be in the same
units as the random variable.</p>
<p>Figures <a class="reference internal" href="#fig-discrete-uniform-pmf-cdf"><span class="std std-numref">Fig. 11.4</span></a>,
<a class="reference internal" href="#fig-binomial-pmf-cdf"><span class="std std-numref">Fig. 11.5</span></a>, <a class="reference internal" href="#fig-poisson-pmf-cdf"><span class="std std-numref">Fig. 11.6</span></a>,
<a class="reference internal" href="#fig-uniform-pdf-cdf"><span class="std std-numref">Fig. 11.8</span></a>, <a class="reference internal" href="#fig-normal-pdf-cdf"><span class="std std-numref">Fig. 11.9</span></a>,
<a class="reference internal" href="#fig-student-t-pdf-cdf"><span class="std std-numref">Fig. 11.10</span></a>, and <a class="reference internal" href="#fig-beta-pdf-cdf"><span class="std std-numref">Fig. 11.11</span></a> show the
expectation and the standard deviations for different distributions.
Notice that these are not values computed from samples but properties of
theoretical mathematical objects.</p>
<p>Expectation is linear, meaning that:</p>
<div class="math notranslate nohighlight" id="equation-eq-expectation-linear">
<span class="eqno">(11.24)<a class="headerlink" href="#equation-eq-expectation-linear" title="Permalink to this equation">¶</a></span>\[\mathbb{E}(cX) = c\mathbb{E}(X)\]</div>
<p>where <span class="math notranslate nohighlight">\(c\)</span> is a constant and</p>
<div class="math notranslate nohighlight" id="equation-eq-expectation-sums">
<span class="eqno">(11.25)<a class="headerlink" href="#equation-eq-expectation-sums" title="Permalink to this equation">¶</a></span>\[\mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y)\]</div>
<p>which are true even in the case that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are dependent. Instead,
the variance is not linear:</p>
<div class="math notranslate nohighlight" id="equation-eq-expectation-varc">
<span class="eqno">(11.26)<a class="headerlink" href="#equation-eq-expectation-varc" title="Permalink to this equation">¶</a></span>\[\mathbb{V}(cX) = c^2\mathbb{V}(X)\]</div>
<p>and in general:</p>
<div class="math notranslate nohighlight" id="equation-eq-expectation-ineq">
<span class="eqno">(11.27)<a class="headerlink" href="#equation-eq-expectation-ineq" title="Permalink to this equation">¶</a></span>\[\mathbb{V}(X + Y) \neq \mathbb{V}(X) + \mathbb{V}(Y)\]</div>
<p>except, for example, when <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p>
<p>We denote the <span class="math notranslate nohighlight">\(n\)</span>th moment of a random variable <span class="math notranslate nohighlight">\(X\)</span> is
<span class="math notranslate nohighlight">\(\mathbb{E}(X^n)\)</span>, thus the expected value and the variance are also
known as the first and second moments of a distribution. The third
moment, the skew, tells us about the asymmetry of a distribution. The
skewness of a random variable <span class="math notranslate nohighlight">\(X\)</span> with mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance
<span class="math notranslate nohighlight">\(\sigma^2\)</span> is the third (standardized moment) of <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-skewness">
<span class="eqno">(11.28)<a class="headerlink" href="#equation-eq-skewness" title="Permalink to this equation">¶</a></span>\[\text{skew}(X) = \mathbb{E}\left(\frac{X -\mu}{\sigma}\right)^3\]</div>
<p>The reason to compute the skew as a standardized quantity, i.e. to
subtract the mean and divide by the standard deviation is to make the
skew independent of the localization and scale of <span class="math notranslate nohighlight">\(X\)</span>, this is
reasonable as we already have that information from the mean and
variance and also it will make the skewness independent of the units of
<span class="math notranslate nohighlight">\(X\)</span>, so it becomes easier to compare skewness.</p>
<p>For example, a <span class="math notranslate nohighlight">\(\text{Beta}(2, 2)\)</span> has a 0 skew while for
<span class="math notranslate nohighlight">\(\text{Beta}(2, 5)\)</span> the skew is positive and for <span class="math notranslate nohighlight">\(\text{Beta}(5, 2)\)</span>
negative. For unimodal distributions, a positive skew generally means
that the right tail is longer, and the opposite for a negative skew.
This is not always the case, the reason is that a 0 skew means that the
<em>total mass</em> at the tails on both sides is balanced. So we can also
balance the mass by having one long thin tail an another short and fat
tail.</p>
<p>The fourth moment, known as kurtosis, tells us about the behavior of the
tails or the <em>extreme values</em> <span id="id32">[<a class="reference internal" href="references.html#id94" title="Peter H Westfall. Kurtosis as peakedness, 1905–2014. rip. The American Statistician, 68(3):191–195, 2014.">133</a>]</span>. It is defined as</p>
<div class="math notranslate nohighlight" id="equation-kurtosis">
<span class="eqno">(11.29)<a class="headerlink" href="#equation-kurtosis" title="Permalink to this equation">¶</a></span>\[\text{Kurtosis}(X) = \mathbb{E}\left(\frac{X -\mu}{\sigma}\right)^4 - 3 \]</div>
<p>The reason to subtract 3 is to make the Gaussian have 0 kurtosis, as it
is often the case that kurtosis is discussed in comparison with the
Gaussian distribution, but sometimes it is often computed without the
<span class="math notranslate nohighlight">\(-3\)</span>, so when in doubt ask, or read, for the exact definition used in a
particular case. By examining the definition of kurtosis in Equation
<a class="reference internal" href="#equation-kurtosis">(11.29)</a> we can see that we are essentially computing the expected
value of the standardized data raised to the fourth power. Thus any
standardized values less than 1 contribute virtually nothing to the
kurtosis. Instead the only values that has something to contribute are
the <em>extreme</em> values.</p>
<p>As we increase increase the value of <span class="math notranslate nohighlight">\(\nu\)</span> in a Student t distribution
the kurtosis decreases (it is zero for a Gaussain distribution) and the
kurtosis increases as we decrease <span class="math notranslate nohighlight">\(\nu\)</span>. The kurtosis is only defined
when <span class="math notranslate nohighlight">\(\nu &gt; 4\)</span>, in fact for the Student T distribution the <span class="math notranslate nohighlight">\(n\)</span>th moment
is only defined for <span class="math notranslate nohighlight">\(\nu &gt; n\)</span>.</p>
<p>The stats module of SciPy offers a method <code class="docutils literal notranslate"><span class="pre">stats(moments)</span></code> to compute
the moments of distributions as you can see in Code Block
<a class="reference internal" href="#scipy-unif"><span class="std std-ref">scipy_unif</span></a> where it is used to obtain the
mean and variance. We notice that all we have discussed in this section
is about computing expectation and moments from probability
distributions and not from samples, thus we are talking about properties
of theoretical distributions. Of course in practice we usually want to
estimate the moments of a distribution from data and for that reason
statisticians have studies estimators, for example, the sample mean and
the sample median are estimators of <span class="math notranslate nohighlight">\(\mathbb{E}(X)\)</span>.</p>
</section>
<section id="transformations">
<span id="id33"></span><h3><span class="section-number">11.1.9. </span>Transformations<a class="headerlink" href="#transformations" title="Permalink to this headline">¶</a></h3>
<p>If we have a random variable <span class="math notranslate nohighlight">\(X\)</span> and we apply a function <span class="math notranslate nohighlight">\(g\)</span> to it we
obtain another random variable <span class="math notranslate nohighlight">\(Y = g(X)\)</span>. After doing so we may ask,
given that we know the distribution of <span class="math notranslate nohighlight">\(X\)</span> how can we find out the
distribution of <span class="math notranslate nohighlight">\(Y\)</span>. One easy way of doing it, is by sampling from <span class="math notranslate nohighlight">\(X\)</span>
applying the transformation and then plotting the results. But of course
there are formals ways of doing it. One such way is applying the
<strong>change of variables</strong> technique.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> is a continuous random variable and <span class="math notranslate nohighlight">\(Y = g(X)\)</span>, where <span class="math notranslate nohighlight">\(g\)</span> is a
differentiable and strictly increasing or decreasing function, the PDF
of <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-eq-changeofvariable">
<span class="eqno">(11.30)<a class="headerlink" href="#equation-eq-changeofvariable" title="Permalink to this equation">¶</a></span>\[p_Y(y) = p_X(x) \left| \frac{dx}{dy} \right|\]</div>
<p>We can see this is true as follows. Let <span class="math notranslate nohighlight">\(g\)</span> be strictly increasing, then
the CDF of <span class="math notranslate nohighlight">\(Y\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-eq-changeofvariable-proof0">
<span class="eqno">(11.31)<a class="headerlink" href="#equation-eq-changeofvariable-proof0" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
   F_Y(y) =&amp; P(Y \le y) \\
          =&amp; P(g(X) \le y) \\
          =&amp; P(X \le g^{-1}(y)) \\
          =&amp; F_X(g^{-1}(y)) \\
          =&amp; F_X(x) \\
\end{split}\end{split}\]</div>
<p>and then by the chain rule, the PDF of <span class="math notranslate nohighlight">\(Y\)</span> can be computed from the PDF
of <span class="math notranslate nohighlight">\(X\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-eq-changeofvariable-proof1">
<span class="eqno">(11.32)<a class="headerlink" href="#equation-eq-changeofvariable-proof1" title="Permalink to this equation">¶</a></span>\[p_Y(y) = p_X(x) \frac{dx}{dy}\]</div>
<p>The proof for <span class="math notranslate nohighlight">\(g\)</span> strictly decreasing is similar but we end up with a
minus sign on the right hand term and thus the reason we compute the
absolute value in Equation <a class="reference internal" href="#equation-eq-changeofvariable">(11.30)</a>.</p>
<p>For multivariate random variables (i.e in higher dimensions) instead of
the derivative we need to compute the Jacobian determinant, and thus it
is common to refer the term <span class="math notranslate nohighlight">\(\left| \frac{dx}{dy} \right|\)</span> as the
Jacobian even in the one dimensional case. The absolute value of the
Jacobian determinant at a point <span class="math notranslate nohighlight">\(p\)</span> gives us the factor by which a
function <span class="math notranslate nohighlight">\(g\)</span> expands or shrinks volumes near <span class="math notranslate nohighlight">\(p\)</span>. This interpretation of
the Jacobian is also applicable to probability densities. If the
transformation <span class="math notranslate nohighlight">\(g\)</span> is not linear then the affected probability
distribution will shrink in some regions and expand in others. Thus we
need to properly take into account these deformations when computing <span class="math notranslate nohighlight">\(Y\)</span>
from the known PDF of <span class="math notranslate nohighlight">\(X\)</span>. Slightly rewriting Equation
<a class="reference internal" href="#equation-eq-changeofvariable">(11.30)</a> like below also helps:</p>
<div class="math notranslate nohighlight" id="equation-eq-changeofvariable2">
<span class="eqno">(11.33)<a class="headerlink" href="#equation-eq-changeofvariable2" title="Permalink to this equation">¶</a></span>\[p_Y(y)dy = p_X(x)dx\]</div>
<p>As we can now see that the probability of finding <span class="math notranslate nohighlight">\(Y\)</span> in a tiny interval
<span class="math notranslate nohighlight">\(p_Y(y)dy\)</span> is equal to the probability of finding <span class="math notranslate nohighlight">\(X\)</span> in a tiny interval
<span class="math notranslate nohighlight">\(p_X(x)dx\)</span>. So the Jacobian is telling us how we remap probabilities in
the space associated to <span class="math notranslate nohighlight">\(X\)</span> with those associated with <span class="math notranslate nohighlight">\(Y\)</span>.</p>
</section>
<section id="limits">
<span id="id34"></span><h3><span class="section-number">11.1.10. </span>Limits<a class="headerlink" href="#limits" title="Permalink to this headline">¶</a></h3>
<p>The two best known and most widely used theorems in probability are the
law of large numbers and the central limit theorem. They both tell us
what happens to the sample mean as the sample size increases. They can
both be understood in the context of repeated experiments, where the
outcome of the experiment could be viewed as a sample from some
underlying distribution.</p>
<section id="the-law-of-large-numbers">
<span id="id35"></span><h4><span class="section-number">11.1.10.1. </span>The Law of Large Numbers<a class="headerlink" href="#the-law-of-large-numbers" title="Permalink to this headline">¶</a></h4>
<p>The law of large number tells us that the sample mean of an iid random
variable converges, as the number of samples increase, to the expected
value of the random variable. This is not true for some distributions
such as the Cauchy distribution (which has no mean or finite variance).</p>
<p>The law of large numbers is often misunderstood, leading to the
gambler’s fallacy. An example of this paradox is believing that it is
smart to bet in the lottery on a number that has not appeared for a long
time. The erroneous reasoning here is that if a particular number has
not appeared for a while then there is must be some kind of force that
increases the probability of that number in the next draws. A force that
re-establish the equiprobability of the numbers and the <em>natural order</em>
of the universe.</p>
<figure class="align-default" id="fig-law-of-large-numbers">
<a class="reference internal image-reference" href="../_images/law_of_large_numbers.png"><img alt="../_images/law_of_large_numbers.png" src="../_images/law_of_large_numbers.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.18 </span><span class="caption-text">Running values from a <span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span> distribution. The dashed line
at 0.5 represent the expected value. As the number of draws increases,
the empirical mean approaches the expected value. Each line represents a
different sample.</span><a class="headerlink" href="#fig-law-of-large-numbers" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="the-central-limit-theorem">
<span id="appendix-clt"></span><h4><span class="section-number">11.1.10.2. </span>The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this headline">¶</a></h4>
<p>The central limit theorem states that if we sample <span class="math notranslate nohighlight">\(n\)</span> values
independently from an arbitrary distribution the mean <span class="math notranslate nohighlight">\(\bar X\)</span> of those
values will distribute approximately as a Gaussian as
<span class="math notranslate nohighlight">\({n \rightarrow \infty}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-central-limit">
<span class="eqno">(11.34)<a class="headerlink" href="#equation-eq-central-limit" title="Permalink to this equation">¶</a></span>\[\bar X_n \dot \sim \mathcal{N} \left (\mu, \frac{\sigma^2} {n} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> are the mean and variance of the arbitrary
distribution.</p>
<p>For the central limit theorem to be fulfilled, the following assumptions
must be met:</p>
<ul class="simple">
<li><p>The values are sampled independently</p></li>
<li><p>Each value come from the same distribution</p></li>
<li><p>The mean and standard deviation of the distribution must be finite</p></li>
</ul>
<p>Criteria 1 and 2 can be relaxed <em>quite a bit</em> and we will still get
roughly a Gaussian, but there is no way to escape from Criterion 3. For
distributions such as the Cauchy distribution, which do not have a
defined mean or variance, this theorem does not apply. The average of
<span class="math notranslate nohighlight">\(N\)</span> values from a Cauchy distribution do not follow a Gaussian but a
Cauchy distribution.</p>
<p>The central limit theorem explains the prevalence of the Gaussian
distribution in nature. Many of the phenomena we study can be explained
as fluctuations around a mean, or as the result of the sum of many
different factors.</p>
<p><a class="reference internal" href="#fig-central-limit"><span class="std std-numref">Fig. 11.19</span></a> shows the central limit theorem in action
for 3 different distributions, <span class="math notranslate nohighlight">\(\text{Pois}(2.3)\)</span>, <span class="math notranslate nohighlight">\(\mathcal{U}(0, 1)\)</span>,
<span class="math notranslate nohighlight">\(\text{Beta}(1, 10)\)</span>, as <span class="math notranslate nohighlight">\(n\)</span> increases.</p>
<figure class="align-default" id="fig-central-limit">
<a class="reference internal image-reference" href="../_images/central_limit.png"><img alt="../_images/central_limit.png" src="../_images/central_limit.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.19 </span><span class="caption-text">Histograms of the distributions indicated on the left margin. Each
histogram is based on 1000 simulated values of <span class="math notranslate nohighlight">\(\bar{X_n}\)</span>. As we
increase <span class="math notranslate nohighlight">\(n\)</span> the distribution of <span class="math notranslate nohighlight">\(\bar{X_n}\)</span> approach a Normal
distribution. The black curve corresponds to a Gaussian distribution
according to the central limit theorem.</span><a class="headerlink" href="#fig-central-limit" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="markov-chains">
<span id="id36"></span><h3><span class="section-number">11.1.11. </span>Markov Chains<a class="headerlink" href="#markov-chains" title="Permalink to this headline">¶</a></h3>
<p>A Markov Chain is a sequence of random variables <span class="math notranslate nohighlight">\(X_0, X_1, \dots\)</span> for
which the future state is conditionally independent from all past ones
given the current state. In other words, knowing the current state is
enough to know the probabilities for all future states. This is known as
the Markov property and we can write it as:</p>
<div class="math notranslate nohighlight" id="equation-markov-property">
<span class="eqno">(11.35)<a class="headerlink" href="#equation-markov-property" title="Permalink to this equation">¶</a></span>\[P(X_{n+1} = j \mid X_n = i, X_{n-1} = i_{n-1} , \dots, X_0 = i_0) = P(X_{n+1} = j \mid X_n = i)
    \]</div>
<p>A rather effective way to visualize Markov Chains is imagining you or
some object moving in space <a class="footnote-reference brackets" href="#id114" id="id37">15</a>. The analogy is easier to grasp if the
space is finite, for example, moving a piece in a square board like
checkers or a salesperson visiting different cities. Given this
scenarios you can ask questions like, how likely is to visit one state
(specific squares in the board, cities, etc)? Or maybe more interesting
if we keep moving from state to state how much time will we spend at
each state in the long-run?</p>
<p><a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> shows four
examples of Markov Chains, the first one show a classical example, an
oversimplified weather model, where the states are rainy or sunny, the
second example shows a deterministic die. The last two example are more
abstract as we have not assigned any concrete representation to them.</p>
<figure class="align-default" id="fig-markov-chains-graph">
<a class="reference internal image-reference" href="../_images/markov_chains_graph.png"><img alt="../_images/markov_chains_graph.png" src="../_images/markov_chains_graph.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.20 </span><span class="caption-text">Markov Chains examples. (a) An oversimplified weather model, representing the probability of a rainy or sunny day, the arrows indicate the transition between states, the arrows are annotated with their corresponding transition probabilities. (b) An example of periodic Markov Chain. (c) An example of a disjoint chain. The states 1, 2, and 3 are disjoint from states A and B. If we start at the state 1, 2, or 3 we will never reach state A or B and vice versa. Transition probabilities are omitted in this example. (d) A Markov chain representing the gambler’s ruin problem, two gamblers, A and B, start with <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(N-i\)</span> units of money respectively. At any given money they bet 1 unit, gambler A has probability <span class="math notranslate nohighlight">\(p\)</span> of and probability <span class="math notranslate nohighlight">\(q = 1 - p\)</span> of losing. If <span class="math notranslate nohighlight">\(X_n\)</span> is the total money of gambler A at time <span class="math notranslate nohighlight">\(n\)</span>. Then <span class="math notranslate nohighlight">\(X_0, X_1, \dots\)</span> is a Markov chain as the one represented.</span><a class="headerlink" href="#fig-markov-chains-graph" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>A convenient way to study Markov Chains is to collect the probabilities
of moving between states in one step in a transition matrix
<span class="math notranslate nohighlight">\(\mathbf{T} = (t_{ij})\)</span>. For example, the transition matrix of example A
in <a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> is</p>
<!---
```{math} 
\begin{blockarray}{ccc}
\; & \text{sunny} & \text{rainy} \\
\begin{block}{c(cc)}
\text{sunny} & 0.9 & 0.1 \\
\text{rainy} & 0.8 & 0.2 \\
\end{block}
```
-->
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
0.9 &amp; 0.1 \\
0.8 &amp; 0.2
\end{bmatrix}\end{split}\]</div>
<p>and, for example, the transition matrix of example B in
<a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> is</p>
<!---
```{math} 
\begin{blockarray}{ccccccc}
\; & 0 & 1 & 2 & 3 & 4 & 5\\
\begin{block}{c(cccccc)}
0 & 0 & 1 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 1 & 0 & 0 & 0\\
2 & 0 & 0 & 0 & 1 & 0 & 0\\
3 & 0 & 0 & 0 & 0 & 1 & 0\\
4 & 0 & 0 & 0 & 0 & 0 & 1\\
5 & 1 & 0 & 0 & 0 & 0 & 0\\
\end{block}
\end{blockarray}
```
-->
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
2 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
3 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
4 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
5 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
\end{bmatrix}\end{split}\]</div>
<p>The <span class="math notranslate nohighlight">\(i\)</span>th row of the transition matrix represents the conditional
probability distribution of moving from state <span class="math notranslate nohighlight">\(X_{n}\)</span> to the state
<span class="math notranslate nohighlight">\(X_{n+1}\)</span>. That is, <span class="math notranslate nohighlight">\(p(X_{n+1} \mid X_n = i)\)</span>. For example, if we are at
state <em>sunny</em> we can move to <em>sunny</em> (i.e. stay at the same state) with
probability 0.9 and move to state <em>rainy</em> with probability 0.1. Notice
how the total probability of moving from <em>sunny</em> to somewhere is 1, as
expected for a PMF.</p>
<p>Because of the Markov property we can compute the probability of <span class="math notranslate nohighlight">\(n\)</span>
consecutive steps by taking the <span class="math notranslate nohighlight">\(n\)</span>th power of <span class="math notranslate nohighlight">\(\mathbf{T}\)</span>.</p>
<p>We can also specify the starting point of the Markov chain, i.e. the
initial conditions <span class="math notranslate nohighlight">\(s_i = P(X_0 = i)\)</span> and let
<span class="math notranslate nohighlight">\(\mathbf{s}=(s_1, \dots, s_M)\)</span>. With this information we can compute the
marginal PMF of <span class="math notranslate nohighlight">\(X_n\)</span> as <span class="math notranslate nohighlight">\(\mathbf{s}\mathbf{T}^n\)</span>.</p>
<p>When studying Markov chains it makes sense to define properties of
individual states and also properties on the entire chain. For example,
if a chain returns to a state over and over again we call that state
recurrent. Instead a transient state is one that the chain will
eventually leave forever, in example (d) in
<a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> all states other
than 0 or <span class="math notranslate nohighlight">\(N\)</span> are transient. Also, we can call a chain irreducible if it
is possible to get from any state to any other state in a finite number
of steps example (c) in
<a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> is not
irreducible, as states 1,2 and 3 are disconnected from states A and B.</p>
<p>Understanding the long-term behavior of Markov chains is of interest. In
fact, they were introduced by Andrey Markov with the purpose of
demonstrating that the law of large numbers can be applied also to
non-independent random variables. The previously mentioned concepts of
recurrence and transience are important for understanding this long-term
run behavior. If we have a chain with transient and recurrent states,
the chain may spend time in the transient states, but it will eventually
spend all the eternity in the recurrent states. A natural question we
can ask is how much time the chain is going to be at each state. The
answer is provided by finding the <strong>stationary distribution</strong> of the
chain.</p>
<p>For a finite Markov chain, the stationary distribution <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> is a
PMF such that <span class="math notranslate nohighlight">\(\mathbf{s}\mathbf{T} = \mathbf{s}\)</span> <a class="footnote-reference brackets" href="#id115" id="id38">16</a>. That is a
distribution that is not changed by the transition matrix <span class="math notranslate nohighlight">\(\mathbf{T}\)</span>.
Notice that this does not mean the chain is not moving anymore, it means
that the chain moves in such a way that the time it will spend at each
state is the one defined by <span class="math notranslate nohighlight">\(\mathbf{s}\)</span>. Maybe a physical analogy could
helps here. Imagine we have a glass not completely filled with water at
a given temperature. If we seal it with a cover, the water molecules
will evaporate into the air as moisture. Interestingly it is also the
case that the water molecules in the air will move to the liquid water.
Initially more molecules might be going one way or another, but at a
given point the system will find a dynamic equilibrium, with the same
amount of water molecules moving to the air from the liquid water, as
the number of water molecules moving from the liquid water to the air.
In physics/chemistry this is called a steady-state, locally things are
moving, but globally nothing changes <a class="footnote-reference brackets" href="#id116" id="id39">17</a>. Steady state is also an
alternative name to stationary distribution.</p>
<p>Interestingly, under various conditions, the stationary distribution of
a finite Markov chain exists and is unique, and the PMF of <span class="math notranslate nohighlight">\(X_n\)</span>
converges to <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>. Example (d) in
<a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> does not have a
unique stationary distribution. We notice that once this chain reaches
the states 0 or <span class="math notranslate nohighlight">\(N\)</span>, meaning gambler A or B lost all the money, the
chain stays in that state forever, so both <span class="math notranslate nohighlight">\(s_0=(1, 0, \dots , 0)\)</span> and
<span class="math notranslate nohighlight">\(s_N=(0, 0, \dots , 1)\)</span> are both stationary distributions. On the
contrary example B in
<a class="reference internal" href="#fig-markov-chains-graph"><span class="std std-numref">Fig. 11.20</span></a> has a unique
stationary distribution which is <span class="math notranslate nohighlight">\(s=(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)\)</span>,
event thought the transition is deterministic.</p>
<p>If a PMF <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> satisfies the reversibility condition (also known
as detailed balance), that is <span class="math notranslate nohighlight">\(s_i t_{ij} = s_j t_{ji}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and
<span class="math notranslate nohighlight">\(j\)</span>, we have the guarantee that <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> is a stationary
distribution of the Markov chain with transition matrix
<span class="math notranslate nohighlight">\(\mathbf{T} = t_{ij}\)</span>. Such Markov chains are called reversible. In Section
<a class="reference internal" href="#inference-methods"><span class="std std-ref">Inference Methods</span></a> we will use this property to show why
Metropolis-Hastings is guaranteed to, asymptotically, work.</p>
<p>Markov chains satisfy a central limit theorem which is similar to
Equation <a class="reference internal" href="#equation-eq-central-limit">(11.34)</a> except that instead of dividing by <span class="math notranslate nohighlight">\(n\)</span>
we need to divide by the effective sample size (ESS). In Section <a class="reference internal" href="chp_02.html#ess"><span class="std std-ref">Effective Sample Size</span></a>
we discussed how to estimate the effective sample size from a Markov
Chain and how to use it to diagnose the quality of the chain. The square
root of <span class="math notranslate nohighlight">\(\frac{\sigma^2} {\text{ESS}}\)</span> is the Monte Carlo standard error
(MCSE) that we also discussed in Section <a class="reference internal" href="chp_02.html#monte-carlo-standard-error"><span class="std std-ref">Monte Carlo Standard Error</span></a>.</p>
</section>
</section>
<section id="entropy">
<span id="id40"></span><h2><span class="section-number">11.2. </span>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h2>
<p>In the <em>Zentralfriedhof</em>, Vienna, we can find the grave of Ludwig
Boltzmann. His tombstone has the legend <span class="math notranslate nohighlight">\(S = k \log W\)</span>, which is a
beautiful way of saying that the second law of thermodynamics is a
consequence of the laws of probability. With this equation Boltzmann
contributed to the development of one of the pillars of modern physics,
statistical mechanics. Statistical mechanics describes how macroscopic
observations such as temperature are related to the microscopic world of
molecules. Imagine a glass with water, what we perceive with our senses
is basically the average behavior of a huge number water molecules
inside that glass <a class="footnote-reference brackets" href="#id117" id="id41">18</a>. At a given temperature there is a given number
of arrangements of the water molecules compatible with that temperature
(Figure <a class="reference internal" href="#fig-entropy-t"><span class="std std-numref">Fig. 11.21</span></a>). As we decrease the temperature we will
find that less and less arrangements are possible until we find a single
one. We have just reached 0 Kelvin, the lowest possible temperature in
the universe! If we move into the other direction we will find that
molecules can be found in more and more arrangements.</p>
<figure class="align-default" id="fig-entropy-t">
<a class="reference internal image-reference" href="../_images/entropy_T.png"><img alt="../_images/entropy_T.png" src="../_images/entropy_T.png" style="width: 7.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.21 </span><span class="caption-text">The number of possible arrangements particles can take is related to the
temperature of the system. Here we represent discrete system of 3
equivalent particles, the number of possible arrangements is represented
by the available cells (gray high lines). increasing the temperature is
equivalent to increasing the number of available cells. At <span class="math notranslate nohighlight">\(T=0\)</span> only
one arrangement is possible, as the temperature increase the particles
can occupy more and more states.</span><a class="headerlink" href="#fig-entropy-t" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We can analyze this mental experiment in terms of uncertainty. If we
know a system is at 0 Kelvin we know the system can only be in a single
possible arrangement, our certainty is absolute <a class="footnote-reference brackets" href="#id118" id="id42">19</a>, as we increase
the temperature the number of possible arrangements will increase and
then it will become more and more difficult to say, “Hey look! Water
molecules are in this particular arrangement at this particular time!”
Thus our uncertainty about the microscopic state will increase. We will
still be able to characterize the system by averages such the
temperature, volume, etc, but at the microscopic level the certainty
about particular arrangements will decrease. Thus, we can think of
entropy as a way of measuring uncertainty.</p>
<p>The concept of entropy is not only valid for molecules. It could also be
applies to arrangements of pixels, characters in a text, musical notes,
socks, bubbles in a sourdough bread and more. The reason that entropy is
so flexible is because it quantifies the arrangements of objects - it is
a property of the underlying distributions. The larger the entropy of a
distribution the less informative that distribution will be and the more
evenly it will assign probabilities to its events. Getting an answer of
“<span class="math notranslate nohighlight">\(42\)</span>” is more certain than “<span class="math notranslate nohighlight">\(42 \pm 5\)</span>”, which again more certain
than “any real number”. Entropy can translate this qualitative
observation into numbers.</p>
<p>The concept of entropy applies to continue and discrete distributions,
but it is easier to think about it using discrete states and we will see
some example in the rest of this section. But keep in mind the same
concepts apply to the continuous cases.</p>
<p>For a probability distribution <span class="math notranslate nohighlight">\(p\)</span> with <span class="math notranslate nohighlight">\(n\)</span> possible different events
which each possible event <span class="math notranslate nohighlight">\(i\)</span> having probability <span class="math notranslate nohighlight">\(p_i\)</span>, the entropy is
defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-entropy">
<span class="eqno">(11.36)<a class="headerlink" href="#equation-eq-entropy" title="Permalink to this equation">¶</a></span>\[H(p) = - \mathbb{E}[\log{p}] = -\sum_{i}^n p_i \log{p_i}\]</div>
<p>Equation <a class="reference internal" href="#equation-eq-entropy">(11.36)</a> is just a different way of writing the entropy
engraved on Boltzmann’s tombstone. We annotate entropy using <span class="math notranslate nohighlight">\(H\)</span> instead
of <span class="math notranslate nohighlight">\(S\)</span> and set <span class="math notranslate nohighlight">\(k=1\)</span>. Notice that the multiplicity <span class="math notranslate nohighlight">\(W\)</span> from Boltzmann’s
version is the total number of ways in which different outcomes can
possibly occur:</p>
<div class="math notranslate nohighlight" id="equation-eq-degeneracy">
<span class="eqno">(11.37)<a class="headerlink" href="#equation-eq-degeneracy" title="Permalink to this equation">¶</a></span>\[W = \frac{N!}{n_1!n_2! \cdots n_t!}\]</div>
<p>You can think of this as rolling a t-sided die <span class="math notranslate nohighlight">\(N\)</span> times, where <span class="math notranslate nohighlight">\(n_i\)</span> is
the number of times we obtain side <span class="math notranslate nohighlight">\(i\)</span>. As <span class="math notranslate nohighlight">\(N\)</span> is large we can use
Stirling’s approximation <span class="math notranslate nohighlight">\(x! \approx (\frac{x}{e})^x\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-stirling0">
<span class="eqno">(11.38)<a class="headerlink" href="#equation-eq-stirling0" title="Permalink to this equation">¶</a></span>\[W =  \frac{N^N}{n_1^{n_1} n_2^{n_2} \cdots n_t^{n_t}} e^{(n_1 n_2 \cdots n_t-N)}\]</div>
<p>noticing that <span class="math notranslate nohighlight">\(p_i = \frac{n_i}{N}\)</span> we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-p-as-frac">
<span class="eqno">(11.39)<a class="headerlink" href="#equation-eq-p-as-frac" title="Permalink to this equation">¶</a></span>\[W = \frac{1}{p_1^{n_1} p_2^{n_2} \cdots p_t^{n_t}}\]</div>
<p>And finally by taking the logarithm we obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-entropy-w">
<span class="eqno">(11.40)<a class="headerlink" href="#equation-eq-entropy-w" title="Permalink to this equation">¶</a></span>\[\log W = -\sum_{i}^n p_i \log{p_i}\]</div>
<p>which is exactly the definition of entropy.</p>
<p>We will now show how to compute entropy in Python using Code Block
<a class="reference internal" href="#entropy-dist"><span class="std std-ref">entropy_dist</span></a>, with the result shown in
<a class="reference internal" href="#fig-entropy"><span class="std std-numref">Fig. 11.22</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="entropy-dist">
<div class="code-block-caption"><span class="caption-number">Listing 11.5 </span><span class="caption-text">entropy_dist</span><a class="headerlink" href="#entropy-dist" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">26</span><span class="p">)</span>
<span class="n">q_pmf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">qu_pmf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">q_pmf</span><span class="p">))</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">r_pmf</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_pmf</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">q_pmf</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">ru_pmf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">r_pmf</span><span class="p">))</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">s_pmf</span> <span class="o">=</span> <span class="p">(</span><span class="n">q_pmf</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">q_pmf</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">su_pmf</span> <span class="o">=</span> <span class="p">(</span><span class="n">qu_pmf</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">qu_pmf</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>

<span class="n">zipped</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">([</span><span class="n">q_pmf</span><span class="p">,</span> <span class="n">qu_pmf</span><span class="p">,</span> <span class="n">r_pmf</span><span class="p">,</span> <span class="n">ru_pmf</span><span class="p">,</span> <span class="n">s_pmf</span><span class="p">,</span> <span class="n">su_pmf</span><span class="p">],</span>
             <span class="p">[</span><span class="s2">&quot;q&quot;</span><span class="p">,</span> <span class="s2">&quot;qu&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="s2">&quot;ru&quot;</span><span class="p">,</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;su&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">zipped</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;H = </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">handlelength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-entropy">
<a class="reference internal image-reference" href="../_images/entropy.png"><img alt="../_images/entropy.png" src="../_images/entropy.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.22 </span><span class="caption-text">Discrete distributions defined in Code Block
<a class="reference internal" href="#entropy-dist"><span class="std std-ref">entropy_dist</span></a> and their entropy values <span class="math notranslate nohighlight">\(H\)</span>.</span><a class="headerlink" href="#fig-entropy" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-entropy"><span class="std std-numref">Fig. 11.22</span></a> shows six distributions, one per subplot with its
corresponding entropy. There are a lot of things moving on in this
figure, so before diving in be sure to set aside an adequate amount of
time (this maybe a good time to check your e-mails before going on). The
most peaked, or least spread distribution is <span class="math notranslate nohighlight">\(q\)</span>, and this is the
distribution with the lowest value of entropy among the six plotted
distributions. <span class="math notranslate nohighlight">\(q \sim \text{binom}({n=10, p=0.75})\)</span>, and thus there are
11 possible events. <span class="math notranslate nohighlight">\(qu\)</span> is a Uniform distribution with also 11 possible
events. We can see that the entropy of <span class="math notranslate nohighlight">\(qu\)</span> is larger than <span class="math notranslate nohighlight">\(q\)</span>, in fact
we can compute the entropy for binomial distributions with <span class="math notranslate nohighlight">\(n=10\)</span> and
different values of <span class="math notranslate nohighlight">\(p\)</span> and we will see that none of them have larger
entropy than <span class="math notranslate nohighlight">\(qu\)</span>. We will need to increase <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(\approx 3\)</span> times to
find <em>the first</em> binomial distribution with larger entropy than <span class="math notranslate nohighlight">\(qu\)</span>.
Let us move to the next row. We generate distribution <span class="math notranslate nohighlight">\(r\)</span> by taking <span class="math notranslate nohighlight">\(q\)</span>
and <em>shifting</em> it to the right and then normalizing (to ensure the sum
of all probabilities is 1). As <span class="math notranslate nohighlight">\(r\)</span> is more spread than <span class="math notranslate nohighlight">\(q\)</span> its entropy
is larger. <span class="math notranslate nohighlight">\(ru\)</span> is the Uniform distribution with the same number of
possible events as <span class="math notranslate nohighlight">\(r\)</span> (22), notice we are including as possible values
those <em>in the valley between both peaks</em>. Once again the entropy of the
<em>Uniform</em> version is the one with the largest entropy. So far entropy
seems to be proportional to the variance of a distribution, but before
jumping to conclusions let us check the last two distributions in
<a class="reference internal" href="#fig-entropy"><span class="std std-numref">Fig. 11.22</span></a>. <span class="math notranslate nohighlight">\(s\)</span> is essentially the same as <span class="math notranslate nohighlight">\(r\)</span> but with a
more extensive <em>valley between both peaks</em> and as we can see the entropy
remains the same. The reason is basically that entropy does not care
about those events in the <em>valley</em> with probability zero, it only cares
about possible events. <span class="math notranslate nohighlight">\(su\)</span> is constructed by replacing the two peaks in
<span class="math notranslate nohighlight">\(s\)</span> with <span class="math notranslate nohighlight">\(qu\)</span> (and normalizing). We can see that <span class="math notranslate nohighlight">\(su\)</span> has lower entropy
than <span class="math notranslate nohighlight">\(ru\)</span> even when it looks more spread, after a more careful
inspection we can see that <span class="math notranslate nohighlight">\(su\)</span> spread the total probability between
fewer events (22) than <span class="math notranslate nohighlight">\(ru\)</span> (with 23 events), and thus it makes totally
sense for it to have lower entropy.</p>
</section>
<section id="kullback-leibler-divergence">
<span id="dkl"></span><h2><span class="section-number">11.3. </span>Kullback-Leibler Divergence<a class="headerlink" href="#kullback-leibler-divergence" title="Permalink to this headline">¶</a></h2>
<p>It is common in statistics to use one probability distribution <span class="math notranslate nohighlight">\(q\)</span> to
represent another one <span class="math notranslate nohighlight">\(p\)</span>, we generally do this when we do not know <span class="math notranslate nohighlight">\(p\)</span>
but can approximate it with <span class="math notranslate nohighlight">\(q\)</span>. Or maybe <span class="math notranslate nohighlight">\(p\)</span> is complex and we want to
find a simpler or more convenient distribution <span class="math notranslate nohighlight">\(q\)</span>. In such cases we may
ask how much information are we losing by using <span class="math notranslate nohighlight">\(q\)</span> to represent <span class="math notranslate nohighlight">\(p\)</span>, or
equivalently how much extra uncertainty are we introducing. Intuitively,
we want a quantity that becomes zero only when <span class="math notranslate nohighlight">\(q\)</span> is equal to <span class="math notranslate nohighlight">\(p\)</span> and
be a positive value otherwise. Following the definition of entropy in
Equation <a class="reference internal" href="#equation-eq-entropy">(11.36)</a>, we can achieve this by computing the
expected value of the difference between <span class="math notranslate nohighlight">\(\log(p)\)</span> and <span class="math notranslate nohighlight">\(\log(q)\)</span>. This
is known as the Kullback-Leibler (KL) divergence:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence">
<span class="eqno">(11.41)<a class="headerlink" href="#equation-eq-kl-divergence" title="Permalink to this equation">¶</a></span>\[\mathbb{KL}(p \parallel q) = \mathbb{E}_p[\log{p}-\log{q}]
    \]</div>
<p>Thus the <span class="math notranslate nohighlight">\(\mathbb{KL}(p \parallel q)\)</span> give us the average difference in
log probabilities when using <span class="math notranslate nohighlight">\(q\)</span> to approximate <span class="math notranslate nohighlight">\(p\)</span>. Because the events
appears to us according to <span class="math notranslate nohighlight">\(p\)</span> we need to compute the expectation with
respect to <span class="math notranslate nohighlight">\(p\)</span>. For discrete distributions we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence-discrete">
<span class="eqno">(11.42)<a class="headerlink" href="#equation-eq-kl-divergence-discrete" title="Permalink to this equation">¶</a></span>\[\mathbb{KL}(p \parallel q) = \sum_{i}^n p_i (\log{p_i} - \log{q_i})\]</div>
<p>Using logarithmic properties we can write this into probably the most
common way to represent KL divergence:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence-log">
<span class="eqno">(11.43)<a class="headerlink" href="#equation-eq-kl-divergence-log" title="Permalink to this equation">¶</a></span>\[\mathbb{KL}(p \parallel q)  = \sum_{i}^n p_i \log{\frac{p_i}{q_i}}\]</div>
<p>We can also arrange the term and write the <span class="math notranslate nohighlight">\(\mathbb{KL}(p \parallel q)\)</span>
as:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence-log-dif">
<span class="eqno">(11.44)<a class="headerlink" href="#equation-eq-kl-divergence-log-dif" title="Permalink to this equation">¶</a></span>\[\mathbb{KL}(p \parallel q) = - \sum_{i}^n p_i (\log{q_i} - \log{p_i})\]</div>
<p>and when we expand the above rearrangement we find that:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence-cross-entropy">
<span class="eqno">(11.45)<a class="headerlink" href="#equation-eq-kl-divergence-cross-entropy" title="Permalink to this equation">¶</a></span>\[\mathbb{KL}(p \parallel q) =  \overbrace{-\sum_{i}^n p_i \log{q_i}}^{H(p, q)} -  \overbrace{\left(-\sum_{i}^n p_i \log{p_i}\right)}^{H(p)}\]</div>
<p>As we already saw in previous section, <span class="math notranslate nohighlight">\(H(p)\)</span> is the entropy of <span class="math notranslate nohighlight">\(p\)</span>.
<span class="math notranslate nohighlight">\(H(p,q) = - \mathbb{E}_p[\log{q}]\)</span> is like the entropy of <span class="math notranslate nohighlight">\(q\)</span> but
evaluated according to the values of <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>Reordering above we obtain:</p>
<div class="math notranslate nohighlight" id="equation-eq-cross-entropy">
<span class="eqno">(11.46)<a class="headerlink" href="#equation-eq-cross-entropy" title="Permalink to this equation">¶</a></span>\[H(p, q) = H(p) + D_\text{KL}(p \parallel q)\]</div>
<p>This shows that the KL divergences can be effectively interpreted as the
extra entropy with respect to <span class="math notranslate nohighlight">\(H(p)\)</span>, when using <span class="math notranslate nohighlight">\(q\)</span> to represent <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>To gain a little bit of intuition we are going to compute a few values
for the KL divergence and plot them., We are going to use the same
distributions as in <a class="reference internal" href="#fig-entropy"><span class="std std-numref">Fig. 11.22</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="kl-varies-dist">
<div class="code-block-caption"><span class="caption-number">Listing 11.6 </span><span class="caption-text">kl_varies_dist</span><a class="headerlink" href="#kl-varies-dist" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dists</span> <span class="o">=</span> <span class="p">[</span><span class="n">q_pmf</span><span class="p">,</span> <span class="n">qu_pmf</span><span class="p">,</span> <span class="n">r_pmf</span><span class="p">,</span> <span class="n">ru_pmf</span><span class="p">,</span> <span class="n">s_pmf</span><span class="p">,</span> <span class="n">su_pmf</span><span class="p">]</span>
<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;q&quot;</span><span class="p">,</span> <span class="s2">&quot;qu&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="s2">&quot;ru&quot;</span><span class="p">,</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="s2">&quot;su&quot;</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">KL_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dist_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dists</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">dist_j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dists</span><span class="p">):</span>
        <span class="n">KL_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">dist_i</span><span class="p">,</span> <span class="n">dist_j</span><span class="p">)</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">KL_matrix</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;cet_gray&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The result of Code Block <a class="reference internal" href="#kl-varies-dist"><span class="std std-ref">kl_varies_dist</span></a>
is shown in <a class="reference internal" href="#fig-kl-heatmap"><span class="std std-numref">Fig. 11.23</span></a>. There are two features of
<a class="reference internal" href="#fig-kl-heatmap"><span class="std std-numref">Fig. 11.23</span></a> that immediately pop out. First, the figure is
not symmetric, the reason is that <span class="math notranslate nohighlight">\(\mathbb{KL}(p \parallel q)\)</span> is not
necessarily the same as <span class="math notranslate nohighlight">\(\mathbb{KL}(q \parallel p)\)</span>. Second, we have
many white cells. They represent <span class="math notranslate nohighlight">\(\infty\)</span> values. The definition of the
KL divergence uses the following conventions <span id="id43">[<a class="reference internal" href="references.html#id17" title="T.M. Cover and J.A. Thomas. Elements of Information Theory. Wiley, 2012. ISBN 9781118585771.">134</a>]</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence-conventions">
<span class="eqno">(11.47)<a class="headerlink" href="#equation-eq-kl-divergence-conventions" title="Permalink to this equation">¶</a></span>\[0 \log \frac{0}{0} = 0, \quad
0 \log \frac{0}{q(\boldsymbol{x})} = 0, \quad
p(\boldsymbol{x}) \log \frac{p(\boldsymbol{x})}{0} = \infty\]</div>
<figure class="align-default" id="fig-kl-heatmap">
<a class="reference internal image-reference" href="../_images/KL_heatmap.png"><img alt="../_images/KL_heatmap.png" src="../_images/KL_heatmap.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.23 </span><span class="caption-text">KL divergence for all the pairwise combinations of the distributions q,
qu, r, ru, s, and su shown in <a class="reference internal" href="#fig-entropy"><span class="std std-numref">Fig. 11.22</span></a>, the white color is
used to represent infinity values.</span><a class="headerlink" href="#fig-kl-heatmap" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We can motivate the use of a log-score in computing expected log
pointwise predictive density (introduced in Chapter
<a class="reference internal" href="chp_02.html#chap1bis"><span class="std std-ref">2</span></a> Equation
<a class="reference internal" href="chp_02.html#equation-eq-elpd-practice">(2.5)</a>) based on the KL divergence.
Let us assume we have <span class="math notranslate nohighlight">\(k\)</span> models posteriors
<span class="math notranslate nohighlight">\(\{q_{M_1}, q_{M_2}, \cdots q_{M_k}\}\)</span>, let further assume we know the
<em>true</em> model <span class="math notranslate nohighlight">\(M_0\)</span> then we can compute:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence-log-score">
<span class="eqno">(11.48)<a class="headerlink" href="#equation-eq-kl-divergence-log-score" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
        \mathbb{KL}(p_{M_0} \parallel q_{M_1}) =&amp;\; \mathbb{E}[\log{p_{M_0}}] - \mathbb{E}[\log{q_{M_1}}] \\
        \mathbb{KL}(p_{M_0} \parallel q_{M_2}) =&amp;\; \mathbb{E}[\log{p_{M_0}}] - \mathbb{E}[\log{q_{M_2}}] \\
        &amp;\cdots \\
        \mathbb{KL}(p_{M_0} \parallel q_{M_k}) =&amp;\; \mathbb{E}[\log{p_{M_0}}] - \mathbb{E}[\log{q_{M_k}}]
    \end{split}\end{split}\]</div>
<p>This may seems a futile exercise as in real life we do not know the true
model <span class="math notranslate nohighlight">\(M_0\)</span>. The trick is to realize that as <span class="math notranslate nohighlight">\(p_{M_0}\)</span> is the same for
all comparisons, thus building a ranking based on the KL-divergence is
equivalent to doing one based on the log-score.</p>
</section>
<section id="information-criterion">
<span id="id44"></span><h2><span class="section-number">11.4. </span>Information Criterion<a class="headerlink" href="#information-criterion" title="Permalink to this headline">¶</a></h2>
<p>An information criterion is a measure of the predictive accuracy of a
statistical model. It takes into account how well the model fits the
data and penalizes the complexity of the model. There are many different
information criterion based on how they compute these two terms. The
most famous family member, especially for non-Bayesians, is the Akaike
Information Criterion (AIC) <span id="id45">[<a class="reference internal" href="references.html#id23" title="Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle. In Selected papers of hirotugu akaike, pages 199–213. Springer, 1998.">135</a>]</span>. It is defined as the sum of
two terms. The <span class="math notranslate nohighlight">\(\log p(y_i \mid \hat{\theta}_{mle})\)</span> measures how well
the model fits the data and the penalization term <span class="math notranslate nohighlight">\(p_{AIC}\)</span> to account
for the fact that we use the same data to fit the model and to evaluate
the model.</p>
<div class="math notranslate nohighlight" id="equation-eq-aic">
<span class="eqno">(11.49)<a class="headerlink" href="#equation-eq-aic" title="Permalink to this equation">¶</a></span>\[AIC = -2 \sum_{i}^{n} \log p(y_i \mid \hat{\theta}_{mle}) + 2 p_{AIC}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\theta}_{mle}\)</span> is the maximum-likelihood estimation of
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> and <span class="math notranslate nohighlight">\(p_{AIC}\)</span> is just the number of parameters in
the model.</p>
<p>AIC is quite popular in non-Bayesian settings, but is not well equipped
to deal with the generality of Bayesian models. It does not use the full
posterior distribution, thus discarding potentially useful information.
On average, AIC will behave worse and worse as we move from flat prior
into weakly-informative or informative priors, and/or if we add more
structure into our model, like with hierarchical models. AIC assumes
that the posterior can be well represented (at least asymptotically) by
a Gaussian distribution, but this is not true for a number of models,
including hierarchical models, mixture models, neural networks, etc. In
summary we want to use some better alternatives.</p>
<p>The Widely applicable Information Crieria (WAIC <a class="footnote-reference brackets" href="#id119" id="id46">20</a>)
<span id="id47">[<a class="reference internal" href="references.html#id14" title="Sumio Watanabe and Manfred Opper. Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory. Journal of machine learning research, 2010.">136</a>]</span> can be regarded as a fully Bayesian extension
of AIC. It also contains two terms, roughly with the same interpretation
as the Akaike criterion. The most important difference is that the terms
are computed using the full posterior distribution.</p>
<div class="math notranslate nohighlight" id="equation-eq-waic">
<span class="eqno">(11.50)<a class="headerlink" href="#equation-eq-waic" title="Permalink to this equation">¶</a></span>\[WAIC =  \sum_i^n \log \left(\frac{1}{s} \sum_{j}^S p(y_i \mid \boldsymbol{\theta}^j) \right) \; - \sum_i^n  \left(\mathop{\mathbb{V}}_{j}^s \log p(Y_i \mid \boldsymbol{\theta}^j) \right)
    \]</div>
<p>The first term in Equation <a class="reference internal" href="#equation-eq-waic">(11.50)</a> is just the log-likelihood as
in AIC but evaluated pointwise, i.e, at each <span class="math notranslate nohighlight">\(i\)</span> observed data-point
over the <span class="math notranslate nohighlight">\(n\)</span> observations. We are taking into account the uncertainty in
the posterior by taking the average over the <span class="math notranslate nohighlight">\(s\)</span> samples from the
posterior. This first term is a practical way to compute the theoretical
expected log pointwise predictive density (ELPD) as defined in Equation
<a class="reference internal" href="chp_02.html#equation-eq-elpd">(2.4)</a> and its approximation in Equation
<a class="reference internal" href="chp_02.html#equation-eq-elpd-practice">(2.5)</a>.</p>
<p>The second term might look a little bit weird, as is the variance over
the <span class="math notranslate nohighlight">\(s\)</span> posterior samples (per observation). Intuitively, we can see
that for each observation the variance will be low if the log-likelihood
across the posterior distribution is similar and it will be larger if
the log-likelihood varies more for different samples from the posterior
distribution. The more observations we find to be sensitive to the
<em>details</em> of the posterior the larger the penalization will be. We can
also see this from another equivalent perspective; A more flexible model
is one that can effectively accommodate more datasets. For example, a
model that included straight but also upward curves is more flexible
than one that only allows straight lines; and thus the log-likelihood of
those observation evaluated across the posterior on the later model will
have, on average, a higher variance. If the more flexible model is not
able to compensate this penalization with a higher estimated ELPD then
the simpler model will we ranked as a better choice. Thus the variance
term in Equation <a class="reference internal" href="#equation-eq-waic">(11.50)</a> prevents overfitting by penalizing an
overly complex model and it can be loosely interpreted as the effective
number of parameters as in AIC.</p>
<p>Neither AIC nor WAIC are attempting to measure whether the model is
<em>true</em>, they are only a relative measure to compare alternative models.
From a Bayesian perspective the prior is part of the model, but WAIC is
evaluated over the posterior, and the prior effect is only indirectly
taken into account by the way it affects the resulting posterior. There
are other information criteria like BIC and WBIC that attempts to answer
that question and can be seen as approximations to the Marginal
Likelihood, but we do not discuss them in this book.</p>
</section>
<section id="loo-in-depth">
<span id="loo-depth"></span><h2><span class="section-number">11.5. </span>LOO in Depth<a class="headerlink" href="#loo-in-depth" title="Permalink to this headline">¶</a></h2>
<p>As discussed in Section <a class="reference internal" href="chp_02.html#cv-and-loo"><span class="std std-ref">Cross-validation and LOO</span></a> in this book we use the
term LOO to refer to a particular method to approximate Leave-One-Out
Cross-Validation (LOO-CV) known as Pareto Smooth Importance Sampling
Leave Once Out Cross Validation (PSIS-LOO-CV). In this section we are
going to discuss a few details of this method.</p>
<p>LOO is an alternative to WAIC, in fact it can be shown that
asymptotically they converge to the same numerical value
<span id="id48">[<a class="reference internal" href="references.html#id14" title="Sumio Watanabe and Manfred Opper. Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory. Journal of machine learning research, 2010.">136</a>, <a class="reference internal" href="references.html#id21" title="Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out cross-validation and waic. Statistics and computing, 27(5):1413–1432, 2017.">137</a>]</span>. Nevertheless, LOO
presents two importance advantages for practitioners. It is more robust
in finite samples settings, and it provides useful diagnostics during
computation <span id="id49">[<a class="reference internal" href="references.html#id20" title="Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization in bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2):389–402, 2019.">16</a>, <a class="reference internal" href="references.html#id21" title="Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out cross-validation and waic. Statistics and computing, 27(5):1413–1432, 2017.">137</a>]</span>.</p>
<p>Under LOO-CV the expected log pointwise predictive density for a new
dataset is:</p>
<div class="math notranslate nohighlight">
\[\text{ELPD}_\text{LOO-CV} = \sum_{i=1}^{n} \log
    \int \ p(y_i \mid \boldsymbol{\theta}) \; p(\boldsymbol{\theta} \mid y_{-i}) d\boldsymbol{\theta}
    \tag{\ref{eq:elpd_loo_cv}}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{-i}\)</span> represents the dataset excluding the <span class="math notranslate nohighlight">\(i\)</span> observation.</p>
<p>Given that in practice we do not know the value of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>
we can approximate Equation <span class="math notranslate nohighlight">\(\ref{eq:elpd_loo_cv}\)</span> using <span class="math notranslate nohighlight">\(s\)</span> samples
from the posterior:</p>
<div class="math notranslate nohighlight" id="equation-eq-loo-cv-naive">
<span class="eqno">(11.51)<a class="headerlink" href="#equation-eq-loo-cv-naive" title="Permalink to this equation">¶</a></span>\[\sum_{i}^{n} \log
    \left(\frac{1}{s}\sum_j^s \ p(y_i \mid \boldsymbol{\theta_{-i}^j}) \right)
    \]</div>
<p>Notice that this term looks similar to the first term in Equation
<a class="reference internal" href="#equation-eq-waic">(11.50)</a>, except we are computing <span class="math notranslate nohighlight">\(n\)</span> posteriors removing one
observation each time. For this reason, and contrary to WAIC, we do not
need to add a penalization term. Computing <span class="math notranslate nohighlight">\(\text{ELPD}_\text{LOO-CV}\)</span>
in <a class="reference internal" href="#equation-eq-loo-cv-naive">(11.51)</a> is very costly as we need to compute <span class="math notranslate nohighlight">\(n\)</span>
posteriors. Fortunately if the <span class="math notranslate nohighlight">\(n\)</span> observations are conditionally
independent we can approximate Equation <a class="reference internal" href="#equation-eq-loo-cv-naive">(11.51)</a> with
Equation <a class="reference internal" href="#equation-eq-loo">(11.52)</a> <span id="id50">[<a class="reference internal" href="references.html#id21" title="Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out cross-validation and waic. Statistics and computing, 27(5):1413–1432, 2017.">137</a>, <a class="reference internal" href="references.html#id24" title="W.R. Gilks, S. Richardson, and D. Spiegelhalter. Markov Chain Monte Carlo in Practice. Chapman &amp; Hall/CRC Interdisciplinary Statistics. CRC Press, 1995. ISBN 9781482214970.">138</a>]</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-loo">
<span class="eqno">(11.52)<a class="headerlink" href="#equation-eq-loo" title="Permalink to this equation">¶</a></span>\[\text{ELPD}_{psis-loo} = \sum_i^n \log \sum_j^s w_i^j p(y_i \mid \boldsymbol{\theta}^j)
    \]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> is a vector of normalized weights.</p>
<p>To compute <span class="math notranslate nohighlight">\(w\)</span> we used importance sampling, this is a technique for
estimating properties of a particular distribution <span class="math notranslate nohighlight">\(f\)</span> of interest,
given that we only have samples from a different distribution <span class="math notranslate nohighlight">\(g\)</span>. Using
importance sampling makes sense when sampling from <span class="math notranslate nohighlight">\(g\)</span> is easier than
sampling from <span class="math notranslate nohighlight">\(f\)</span>. If we have a set of samples from the random variable
<span class="math notranslate nohighlight">\(X\)</span> and we are able to evaluate <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(f\)</span> pointwise, we can compute
the importance weights as:</p>
<div class="math notranslate nohighlight" id="equation-eq-importance-weights">
<span class="eqno">(11.53)<a class="headerlink" href="#equation-eq-importance-weights" title="Permalink to this equation">¶</a></span>\[w_i =  \frac{f(x_i)}{g(x_i)}\]</div>
<p>Computationally, it goes as follow:</p>
<ul class="simple">
<li><p>Draw <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(x_i\)</span> from <span class="math notranslate nohighlight">\(g\)</span></p></li>
<li><p>Calculate the probability of each sample <span class="math notranslate nohighlight">\(g(x_i)\)</span></p></li>
<li><p>Evaluate <span class="math notranslate nohighlight">\(f\)</span> over the <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(f(x_i)\)</span></p></li>
<li><p>Calculate the importance weights <span class="math notranslate nohighlight">\(w_i = \frac{f(x_i)}{g(x_i)}\)</span></p></li>
<li><p>Return <span class="math notranslate nohighlight">\(N\)</span> samples from <span class="math notranslate nohighlight">\(g\)</span> with weights <span class="math notranslate nohighlight">\(w\)</span>, <span class="math notranslate nohighlight">\((x_i, w_i)\)</span>, that
could be plug into some estimator</p></li>
</ul>
<p><a class="reference internal" href="#fig-importance-sampling"><span class="std std-numref">Fig. 11.24</span></a> shows an example of approximating the
same target distribution (dashed line) by using two different proposal
distributions. On the first row the proposal is wider than the target
distribution. On the second row the proposal is narrower than the target
distribution. As we can see the approximation is better in the first
case. This is a general feature of importance sampling.</p>
<figure class="align-default" id="fig-importance-sampling">
<a class="reference internal image-reference" href="../_images/importance_sampling.png"><img alt="../_images/importance_sampling.png" src="../_images/importance_sampling.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.24 </span><span class="caption-text">Importance sampling. On the left we have KDEs of samples from the
proposal distributions <span class="math notranslate nohighlight">\(g\)</span>, on the right the dashed line represents the
target distribution and the continuous line the approximated
distribution after re-weighting the samples from the proposal
distribution with the weights computed as in Equation
<a class="reference internal" href="#equation-eq-importance-weights">(11.53)</a>.</span><a class="headerlink" href="#fig-importance-sampling" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Going back to LOO, the distribution that we have computed is the
posterior distribution. In order to evaluate the model we want samples
from the leave-one-out posterior distribution, thus the importance
weights we want to compute are:</p>
<div class="math notranslate nohighlight" id="equation-eq-loocv-weights">
<span class="eqno">(11.54)<a class="headerlink" href="#equation-eq-loocv-weights" title="Permalink to this equation">¶</a></span>\[w_i^j = \frac{p(\theta^j \mid y{-i} )}{p(\theta^j \mid y)} \propto \frac{1}{p(y_i \mid \theta^j)}\]</div>
<p>Notice that this proportionality is great news, because it allows us to
compute <span class="math notranslate nohighlight">\(w\)</span> almost for free. However, the posterior is likely to have
thinner tails than the leave-one-out distributions, which as we saw in
<a class="reference internal" href="#fig-importance-sampling"><span class="std std-numref">Fig. 11.24</span></a> can result in poor estimation.
Mathematically the problem is that the importance weights can have high
or even infinite variance. In order to keep the variance in check, LOO
applies a smoothing procedure that involves replacing the largest
importance weights with values from an estimated Pareto distribution.
This help to make LOO much more robust <span id="id51">[<a class="reference internal" href="references.html#id21" title="Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out cross-validation and waic. Statistics and computing, 27(5):1413–1432, 2017.">137</a>]</span>.
Moreover, the estimated <span class="math notranslate nohighlight">\(\hat \kappa\)</span> parameter of the Pareto
distribution can be used to detect highly influential observations, i.e.
observations that have a large effect on the predictive distribution
when they are left out. In general, higher values of <span class="math notranslate nohighlight">\(\hat \kappa\)</span> can
indicate problems with the data or model, especially when
<span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.7\)</span> <span id="id52">[<a class="reference internal" href="references.html#id20" title="Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization in bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2):389–402, 2019.">16</a>, <a class="reference internal" href="references.html#id19" title="Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto smoothed importance sampling. arXiv preprint arXiv:1507.02646, 2021.">22</a>]</span>.</p>
</section>
<section id="jeffreys-prior-derivation">
<span id="id53"></span><h2><span class="section-number">11.6. </span>Jeffreys’ Prior Derivation<a class="headerlink" href="#jeffreys-prior-derivation" title="Permalink to this headline">¶</a></h2>
<p>In this section we will show how to find the Jeffreys’ prior for the
binomial likelihood, first for the number of successes parameter
<span class="math notranslate nohighlight">\(\theta\)</span>, and then for the odds parameter <span class="math notranslate nohighlight">\(\kappa\)</span>, where
<span class="math notranslate nohighlight">\(\kappa = \frac{\theta}{1-\theta}\)</span>.</p>
<p>Recall from Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a>, for the one-dimensional case JP
for <span class="math notranslate nohighlight">\(\theta\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[p(\theta) \propto \sqrt{I(\theta)}\]</div>
<p>where <span class="math notranslate nohighlight">\(I(\theta)\)</span> is the Fisher information:</p>
<div class="math notranslate nohighlight">
\[I(\theta) = - \mathbb{E_{Y}}\left[\frac{d^2}{d\theta^2} \log p(Y \mid \theta)\right]\]</div>
<section id="jeffreys-prior-for-the-binomial-likelihood-in-terms-of-theta">
<span id="id54"></span><h3><span class="section-number">11.6.1. </span>Jeffreys’ Prior for the Binomial Likelihood in Terms of <span class="math notranslate nohighlight">\(\theta\)</span><a class="headerlink" href="#jeffreys-prior-for-the-binomial-likelihood-in-terms-of-theta" title="Permalink to this headline">¶</a></h3>
<p>Binomial likelihood could be expressed as:</p>
<div class="math notranslate nohighlight" id="equation-eq-binomial-kernel">
<span class="eqno">(11.55)<a class="headerlink" href="#equation-eq-binomial-kernel" title="Permalink to this equation">¶</a></span>\[p(Y \mid \theta) \propto \theta^{y} (1-\theta)^{n-y}\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the number of successes, <span class="math notranslate nohighlight">\(n\)</span> the total number of trials,
and thus <span class="math notranslate nohighlight">\(n-y\)</span> is the numbers of failures. We write is as a
proportionality as the Binomial coefficient in the likelihood does not
depend on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>To compute the Fisher information we need to take the logarithm of the
likelihood:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-0">
<span class="eqno">(11.56)<a class="headerlink" href="#equation-eq-jp-0" title="Permalink to this equation">¶</a></span>\[\ell = \log(p(Y \mid \theta)) \propto y \log(\theta) + (n-y) \log(1-\theta)\]</div>
<p>And then compute the second derivative:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-1">
<span class="eqno">(11.57)<a class="headerlink" href="#equation-eq-jp-1" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\begin{split}
\frac{d \ell}{d\theta} &amp;= \frac{y}{\theta} - \frac{n-y}{1-\theta} \\
\frac{d^{2} \ell}{d \theta^{2}} &amp;= -\frac{y}{\theta^{2}} - \frac{n-y}{ (1-\theta)^{2}}
\end{split}\end{aligned}\end{split}\]</div>
<p>The Fisher information is the expected value of the second derivative of
the likelihood, then:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-2">
<span class="eqno">(11.58)<a class="headerlink" href="#equation-eq-jp-2" title="Permalink to this equation">¶</a></span>\[I(\theta) = - \mathbb{E}_{Y}\left[-\frac{y}{\theta^{2}} + \frac{n-y}{ (1-\theta)^{2}} \right]\]</div>
<p>As <span class="math notranslate nohighlight">\(\mathbb{E}[y] = n\theta\)</span>, we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-3">
<span class="eqno">(11.59)<a class="headerlink" href="#equation-eq-jp-3" title="Permalink to this equation">¶</a></span>\[I(\theta)= \frac{n\theta}{\theta^{2}} - \frac{n - n \theta}{(1-\theta)^{2}}\]</div>
<p>which we can rewrite as:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-4">
<span class="eqno">(11.60)<a class="headerlink" href="#equation-eq-jp-4" title="Permalink to this equation">¶</a></span>\[I(\theta)= \frac{n}{\theta} - \frac{n (1 -\theta)}{(1-\theta)^{2}} = \frac{n}{\theta} - \frac{n}{(1-\theta)}\]</div>
<p>We can express these fractions in terms of a common denominator,</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-5">
<span class="eqno">(11.61)<a class="headerlink" href="#equation-eq-jp-5" title="Permalink to this equation">¶</a></span>\[I(\theta)= n \left[ \frac{1 - \theta}{\theta (1 - \theta)} - \frac{\theta}{\theta (1-\theta)}\right]\]</div>
<p>By regrouping:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-6">
<span class="eqno">(11.62)<a class="headerlink" href="#equation-eq-jp-6" title="Permalink to this equation">¶</a></span>\[I(\theta) = n \frac{1}{\theta (1-\theta)}\]</div>
<p>If we omit <span class="math notranslate nohighlight">\(n\)</span> then we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-fisher-info">
<span class="eqno">(11.63)<a class="headerlink" href="#equation-eq-fisher-info" title="Permalink to this equation">¶</a></span>\[I(\theta) \propto \frac{1}{\theta (1-\theta)} = \theta^{-1} (1-\theta)^{-1}\]</div>
<p>Finally, we need to take the square root of the Fisher information in
Equation <a class="reference internal" href="#equation-eq-fisher-info">(11.63)</a>, which resulting the Jeffreys’ prior for
<span class="math notranslate nohighlight">\(\theta\)</span> of Binomial likelihood as follow:</p>
<div class="math notranslate nohighlight" id="equation-eq-alice-prior">
<span class="eqno">(11.64)<a class="headerlink" href="#equation-eq-alice-prior" title="Permalink to this equation">¶</a></span>\[\begin{aligned}
p(\theta) \propto \theta^{-0.5} (1-\theta)^{-0.5}
\end{aligned}\]</div>
</section>
<section id="jeffreys-prior-for-the-binomial-likelihood-in-terms-of-kappa">
<span id="id55"></span><h3><span class="section-number">11.6.2. </span>Jeffreys’ Prior for the Binomial Likelihood in Terms of <span class="math notranslate nohighlight">\(\kappa\)</span><a class="headerlink" href="#jeffreys-prior-for-the-binomial-likelihood-in-terms-of-kappa" title="Permalink to this headline">¶</a></h3>
<p>Let us now see how to obtain the Jeffreys’ prior for the Binomial
likelihood in terms the odds <span class="math notranslate nohighlight">\(\kappa\)</span>. We begin by replacing
<span class="math notranslate nohighlight">\(\theta = \frac{\kappa}{\kappa + 1}\)</span> in expression
<a class="reference internal" href="#equation-eq-binomial-kernel">(11.55)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-7">
<span class="eqno">(11.65)<a class="headerlink" href="#equation-eq-jp-7" title="Permalink to this equation">¶</a></span>\[p(Y \mid \kappa) \propto \left({\frac{\kappa}{\kappa + 1}}\right)^{y} \left(1-{\frac{\kappa}{\kappa +1}}\right)^{n-y}\]</div>
<p>Which can also be written as:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-8">
<span class="eqno">(11.66)<a class="headerlink" href="#equation-eq-jp-8" title="Permalink to this equation">¶</a></span>\[p(Y \mid \kappa) \propto \kappa^y (\kappa + 1)^{-y} (\kappa +1)^{-n + y}\]</div>
<p>and further simplified into:</p>
<div class="math notranslate nohighlight" id="equation-eq-likelihood-binom-odds">
<span class="eqno">(11.67)<a class="headerlink" href="#equation-eq-likelihood-binom-odds" title="Permalink to this equation">¶</a></span>\[p(Y \mid \kappa) \propto \kappa^y (\kappa + 1)^{-n}\]</div>
<p>Now we need to take the logarithm:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-9">
<span class="eqno">(11.68)<a class="headerlink" href="#equation-eq-jp-9" title="Permalink to this equation">¶</a></span>\[\ell = \log(p(Y \mid \kappa)) \propto y \log{\kappa} -n \log{(\kappa + 1)}\]</div>
<p>we then compute the second derivative:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-10">
<span class="eqno">(11.69)<a class="headerlink" href="#equation-eq-jp-10" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\begin{split}
\frac{d \ell}{d{\kappa}} &amp;= \frac{y}{\kappa} - \frac{n}{\kappa + 1} \\
\frac{d^2 \ell}{d {\kappa^2}} &amp;= -\frac{y}{\kappa^2} + \frac{n}{(\kappa+1)^2}
\end{split}\end{aligned}\end{split}\]</div>
<p>The Fisher information is the expected value of the second derivative of
the likelihood, then:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-11">
<span class="eqno">(11.70)<a class="headerlink" href="#equation-eq-jp-11" title="Permalink to this equation">¶</a></span>\[I(\kappa) = - \mathbb{E}_Y\left[-\frac{y}{\kappa^2} + \frac{n}{ (\kappa+1)^2} \right]\]</div>
<p>As <span class="math notranslate nohighlight">\(\mathbb{E}[y] = n \theta = n \frac{\kappa}{\kappa + 1}\)</span>, we can
write:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-12">
<span class="eqno">(11.71)<a class="headerlink" href="#equation-eq-jp-12" title="Permalink to this equation">¶</a></span>\[I(\kappa) = \frac{n}{\kappa (\kappa + 1)} - \frac{n}{(\kappa + 1)^2}\]</div>
<p>We can express these fractions in terms of a common denominator,</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-13">
<span class="eqno">(11.72)<a class="headerlink" href="#equation-eq-jp-13" title="Permalink to this equation">¶</a></span>\[I(\kappa) = \frac{n (\kappa + 1)}{\kappa (\kappa + 1)^2} - \frac{n \kappa}{\kappa (\kappa + 1)^2}\]</div>
<p>Then we combine into a single fraction</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-14">
<span class="eqno">(11.73)<a class="headerlink" href="#equation-eq-jp-14" title="Permalink to this equation">¶</a></span>\[I(\kappa) = \frac{n (\kappa + 1) - n \kappa}{\kappa (\kappa + 1)^2}\]</div>
<p>We then distribute <span class="math notranslate nohighlight">\(n\)</span> over <span class="math notranslate nohighlight">\((\kappa + 1)\)</span> and we simplify:</p>
<div class="math notranslate nohighlight" id="equation-eq-jp-15">
<span class="eqno">(11.74)<a class="headerlink" href="#equation-eq-jp-15" title="Permalink to this equation">¶</a></span>\[I(\kappa) = \frac{n}{\kappa (\kappa + 1)^2}\]</div>
<p>Finally, by taking the square root, we get the Jeffreys’ prior for the
Binomial likelihood when parameterized by the odds:</p>
<div class="math notranslate nohighlight" id="equation-eq-bob-prior">
<span class="eqno">(11.75)<a class="headerlink" href="#equation-eq-bob-prior" title="Permalink to this equation">¶</a></span>\[p(\kappa) \propto \kappa^{-0.5} (1 + \kappa)^{-1}\]</div>
</section>
<section id="jeffreys-posterior-for-the-binomial-likelihood">
<span id="id56"></span><h3><span class="section-number">11.6.3. </span>Jeffreys’ Posterior for the Binomial Likelihood<a class="headerlink" href="#jeffreys-posterior-for-the-binomial-likelihood" title="Permalink to this headline">¶</a></h3>
<p>To obtain the Jeffrey’s posterior when the likelihood is parameterized
in terms of <span class="math notranslate nohighlight">\(\theta\)</span> we can combine Equation <a class="reference internal" href="#equation-eq-binomial-kernel">(11.55)</a>
with Equation <a class="reference internal" href="#equation-eq-alice-prior">(11.64)</a></p>
<div class="math notranslate nohighlight" id="equation-eq-alice-posterior">
<span class="eqno">(11.76)<a class="headerlink" href="#equation-eq-alice-posterior" title="Permalink to this equation">¶</a></span>\[p(\theta \mid Y) \propto  \theta^{y} (1-\theta)^{n-y} \theta^{-0.5} (1-\theta)^{-0.5} = \theta^{y-0.5} (1-\theta)^{n-y-0.5}\]</div>
<p>Similarly, the Jeffreys’ posterior when the likelihood is parameterized
in terms of <span class="math notranslate nohighlight">\(\kappa\)</span> we can combine <a class="reference internal" href="#equation-eq-likelihood-binom-odds">(11.67)</a> with
<a class="reference internal" href="#equation-eq-bob-prior">(11.75)</a></p>
<div class="math notranslate nohighlight" id="equation-eq-bob-posterior">
<span class="eqno">(11.77)<a class="headerlink" href="#equation-eq-bob-posterior" title="Permalink to this equation">¶</a></span>\[p(\kappa \mid Y) \propto \kappa^y (\kappa + 1)^{-n}  \kappa^{-0.5} (1 + \kappa)^{-1} = \kappa^{(y-0.5)}  (\kappa + 1)^{(-n-1)})\]</div>
</section>
</section>
<section id="marginal-likelihood">
<span id="id57"></span><h2><span class="section-number">11.7. </span>Marginal Likelihood<a class="headerlink" href="#marginal-likelihood" title="Permalink to this headline">¶</a></h2>
<p>For some models, such as those using conjugate priors, the marginal
likelihood is analytically tractable. For the rest, numerically
computing this integral is notoriously difficult, since this involves a
high-dimensional integration over a usually complicated and highly
variable function <span id="id58">[<a class="reference internal" href="references.html#id30" title="Nial Friel and Jason Wyse. Estimating the evidence–a review. Statistica Neerlandica, 66(3):288–308, 2012.">139</a>]</span>. In this section we will try to gain
intuition into why this is generally a hard task.</p>
<p>Numerically, and in low dimensions, we can compute the marginal
likelihood by evaluating the product of the prior and the likelihood,
over a grid and then applying the trapezoid rule, or some other similar
method. As we will see in Section <a class="reference internal" href="#high-dimensions"><span class="std std-ref">Moving out of Flatland</span></a> using grids does not
scale well with dimension, as the number of required grid points
increase rapidly as we increase the number of variables in our model.
Thus grid-based methods becomes impractical for problems with more than
a few variables. Monte Carlo integration can also be problematic, at
least in the most naive implementations (see Section <a class="reference internal" href="#high-dimensions"><span class="std std-ref">Moving out of Flatland</span></a>).
For that reason many dedicated methods have been proposed to compute the
marginal likelihood <span id="id59">[<a class="reference internal" href="references.html#id30" title="Nial Friel and Jason Wyse. Estimating the evidence–a review. Statistica Neerlandica, 66(3):288–308, 2012.">139</a>]</span>. Here we will only discuss one of
them. Our main concern is not learning how to compute the marginal
likelihood in practice, but instead to illustrate why is hard to do it.</p>
<section id="the-harmonic-mean-estimator">
<span id="harmonic-mean"></span><h3><span class="section-number">11.7.1. </span>The Harmonic Mean Estimator<a class="headerlink" href="#the-harmonic-mean-estimator" title="Permalink to this headline">¶</a></h3>
<p>A rather infamous estimator of the marginal likelihood is the harmonic
mean estimator <span id="id60">[<a class="reference internal" href="references.html#id29" title="Radford M Neal. Contribution to the discussion of “approximate bayesian inference with the weighted likelihood bootstrap” by michael a. newton and adrian e. raftery. Journal of the Royal Statistical Society. Series B (Methodological), 56:41–42, 1994.">140</a>]</span>. A very appealing feature of this estimator
is that it only requires <span class="math notranslate nohighlight">\(s\)</span> samples from the posterior:</p>
<div class="math notranslate nohighlight" id="equation-eq-harmonic-mean-approx">
<span class="eqno">(11.78)<a class="headerlink" href="#equation-eq-harmonic-mean-approx" title="Permalink to this equation">¶</a></span>\[p(Y) \approx \left(\frac{1}{s} \sum_{i=1}^{s} \frac{1}{p(Y \mid \boldsymbol{\theta}_i)} \right)^{-1}\]</div>
<p>We can see that we are averaging the inverse of the likelihood over
samples taken from the posterior, then computing the inverse of the
result. In principle, this is a valid Monte Carlo estimator of the
following expectation:</p>
<div class="math notranslate nohighlight" id="equation-eq-harmonic-mean-expectation">
<span class="eqno">(11.79)<a class="headerlink" href="#equation-eq-harmonic-mean-expectation" title="Permalink to this equation">¶</a></span>\[\mathbb{E} \left[\frac{1}{p(Y \mid \boldsymbol{\theta})}\right] = \int_{\boldsymbol{\Theta}} \frac{1}{p(Y \mid \boldsymbol{\theta)}} p(\boldsymbol{\theta} \mid Y) d\boldsymbol{\theta}\]</div>
<p>Notice that Equation <a class="reference internal" href="#equation-eq-harmonic-mean-expectation">(11.79)</a> is a particular
instance of Equation
<a class="reference internal" href="chp_01.html#equation-eq-posterior-expectation">(1.5)</a> which may seems
to indicate we are doing something right by being very Bayesian.</p>
<p>If we expand the posterior term we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-harmonic-mean-approx1">
<span class="eqno">(11.80)<a class="headerlink" href="#equation-eq-harmonic-mean-approx1" title="Permalink to this equation">¶</a></span>\[\mathbb{E} \left[\frac{1}{p(Y \mid \boldsymbol{\theta})}\right] = \int_{\boldsymbol{\Theta}} \frac{1}{p(Y \mid \boldsymbol{\theta})} \frac{{p(Y \mid \boldsymbol{\theta})} p(\theta)}{p(Y)} d\boldsymbol{\theta}\]</div>
<p>which we can simplify into:</p>
<div class="math notranslate nohighlight" id="equation-eq-harmonic-mean-approx2">
<span class="eqno">(11.81)<a class="headerlink" href="#equation-eq-harmonic-mean-approx2" title="Permalink to this equation">¶</a></span>\[\mathbb{E} \left[\frac{1}{p(Y \mid \boldsymbol{\theta})}\right] =  \frac{1}{p(Y)} \underbrace{\int_{\boldsymbol{\Theta}} p(\boldsymbol{\theta}) d\boldsymbol{\boldsymbol{\theta}}}_{=1} = \frac{1}{p(Y)}\]</div>
<p>We are assuming the prior is proper, and thus its integral should be 1.
We can see that Equation <a class="reference internal" href="#equation-eq-harmonic-mean-approx">(11.78)</a> is in fact an
approximation of the marginal likelihood.</p>
<p>Unfortunately the good news does not last too long. The number of
samples <span class="math notranslate nohighlight">\(s\)</span> needed to feed into Equation <a class="reference internal" href="#equation-eq-harmonic-mean-approx">(11.78)</a>
in order to get close to the right answer is generally very large, to
the point that the harmonic mean estimator is not very useful in
practice <span id="id61">[<a class="reference internal" href="references.html#id30" title="Nial Friel and Jason Wyse. Estimating the evidence–a review. Statistica Neerlandica, 66(3):288–308, 2012.">139</a>, <a class="reference internal" href="references.html#id29" title="Radford M Neal. Contribution to the discussion of “approximate bayesian inference with the weighted likelihood bootstrap” by michael a. newton and adrian e. raftery. Journal of the Royal Statistical Society. Series B (Methodological), 56:41–42, 1994.">140</a>]</span>. Intuitively we can see that the sum
will be dominated by samples with very low likelihood. Even worse, the
harmonic mean estimator can have infinite variance. Infinite variance
means that even if we increase <span class="math notranslate nohighlight">\(s\)</span> we will not get a better answer, thus
sometimes even a huge amount of samples could still be insufficient. The
other problem with the harmonic mean estimator is that it is rather
insensitive to changes in the prior. But even the exact marginal
likelihood is in fact very sensitive to changes in the prior
distribution (as we will show later, see <a class="reference internal" href="#fig-posterior-ml"><span class="std std-numref">Fig. 11.26</span></a>).
These two problems will be exacerbated when the likelihood turns to be
much more concentrated with respect to the prior, or when the likelihood
and prior concentrate into different regions of the parameter space.</p>
<p>By using samples from a more peaked posterior, with respect to the
prior, we will be missing all the regions from the prior that have low
posterior density. In a sketchy way we can think of Bayesian Inference
as using data to update the prior into a posterior. Prior and posterior
will only be similar if the data is not very informative.</p>
<p><a class="reference internal" href="#fig-harmonic-mean-heatmap"><span class="std std-numref">Fig. 11.25</span></a> shows a heatmap with the relative
error of computing the harmonic mean estimator compared to the
analytical value. We can see than even for a simple 1D problem like the
Beta-Binomial model the harmonic estimator can fail spectacularly.</p>
<figure class="align-default" id="fig-harmonic-mean-heatmap">
<a class="reference internal image-reference" href="../_images/harmonic_mean_heatmap.png"><img alt="../_images/harmonic_mean_heatmap.png" src="../_images/harmonic_mean_heatmap.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.25 </span><span class="caption-text">Heatmap showing the relative error when using the harmonic mean
estimator to approximate the marginal likelihood of a Beta-Binomial
model. Rows corresponds to different prior distributions. Each column is
a different observed scenario, with the number in parentheses
corresponding to the number of success and failures.</span><a class="headerlink" href="#fig-harmonic-mean-heatmap" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As we will see in Section <a class="reference internal" href="#high-dimensions"><span class="std std-ref">Moving out of Flatland</span></a> when we increase the
dimensionality of our models the posterior concentrates more an more
into a thin hyper-shell. Getting samples from outside this thin shell is
irrelevant to compute a good posterior approximation. On the contrary
when computing the marginal likelihood obtaining samples just from this
thin shell is not enough. Instead, we need to take samples over the
entire prior distribution and this can be a really hard task to do in a
proper way.</p>
<p>There are a few computational methods better suited to compute marginal
likelihood, but even those are not bullet-proof. In Chapter
<a class="reference internal" href="chp_08.html#chap8"><span class="std std-ref">8</span></a> we discuss the Sequential Monte Carlo (SMC) method
mainly for the purpose of doing Approximate Bayesian Computation, but
this method can also compute the marginal likelihood. The main reason
why it works is because SMC uses a series of intermediate distributions
to represent the transition from the prior to the posterior
distributions. Having these <em>bridging</em> distribution alleviates the
problem of sampling from a wide prior and evaluating in a much more
concentrated posterior.</p>
</section>
<section id="marginal-likelihood-and-model-comparison">
<span id="bayes-factors"></span><h3><span class="section-number">11.7.2. </span>Marginal Likelihood and Model Comparison<a class="headerlink" href="#marginal-likelihood-and-model-comparison" title="Permalink to this headline">¶</a></h3>
<p>When performing inference the marginal likelihood is generally regarded
as a normalization constant and often could be omitted or canceled out
during computation. Instead, the marginal likelihood is often seen as
crucial during model comparison <span id="id62">[<a class="reference internal" href="references.html#id156" title="Quentin F Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S Leslie, Jonathan J Forster, Eric-Jan Wagenmakers, and Helen Steingroever. A tutorial on bridge sampling. Journal of mathematical psychology, 81:80–97, 2017.">141</a>, <a class="reference internal" href="references.html#id158" title="Danielle Navarro. A personal essay on bayes factors. PsyArXiv, 2020.">142</a>, <a class="reference internal" href="references.html#id157" title="Daniel J Schad, Bruno Nicenboim, Paul-Christian Bürkner, Michael Betancourt, and Shravan Vasishth. Workflow techniques for the robust use of bayes factors. arXiv preprint arXiv:2103.08744, 2021.">143</a>]</span>.
To better understand why let us write Bayes’ theorem in a way that
explicitly shows that our inferences are model dependent:</p>
<div class="math notranslate nohighlight" id="equation-eq-bayes-theorem-m">
<span class="eqno">(11.82)<a class="headerlink" href="#equation-eq-bayes-theorem-m" title="Permalink to this equation">¶</a></span>\[p(\boldsymbol{\theta} \mid Y, M) = {\frac {p(Y \mid \boldsymbol{\theta}, M)\; p(\boldsymbol{\theta} \mid M)}{p(Y \mid M)}}\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> represents the data and <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> represents the
parameters in model <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>If we have a set of <span class="math notranslate nohighlight">\(k\)</span> models and our main objective is to choose only
one of them, we can choose the one with the largest value of the
marginal likelihood <span class="math notranslate nohighlight">\(p(Y \mid M)\)</span>. Choosing the model with the largest
marginal likelihood is perfectly justified from Bayes theorem under the
assumption of a discrete Uniform prior distribution for the <span class="math notranslate nohighlight">\(k\)</span> models
under comparison.</p>
<div class="math notranslate nohighlight" id="equation-eq-posterior-model">
<span class="eqno">(11.83)<a class="headerlink" href="#equation-eq-posterior-model" title="Permalink to this equation">¶</a></span>\[p(M \mid Y) \propto p(Y \mid M)\; p(M)\]</div>
<p>If all models have the same a priori probability then computing
<span class="math notranslate nohighlight">\(p(Y \mid M)\)</span> is equivalent to computing <span class="math notranslate nohighlight">\(p(M \mid Y)\)</span>. Notice that we
are talking about the prior probability we assign to models <span class="math notranslate nohighlight">\(p(M)\)</span> and
not about the priors we assign to parameters for each model
<span class="math notranslate nohighlight">\(p(\theta \mid M)\)</span>.</p>
<p>As the value of <span class="math notranslate nohighlight">\(p(Y \mid M_k)\)</span> does not tell us anything by-itself, in
practice people usually compute the ratio of two marginal likelihoods.
This ratio is called Bayes factor:</p>
<div class="math notranslate nohighlight" id="equation-eq-bayes-factor">
<span class="eqno">(11.84)<a class="headerlink" href="#equation-eq-bayes-factor" title="Permalink to this equation">¶</a></span>\[BF = \frac{p(Y \mid M_0)}{p(Y \mid M_1)}\]</div>
<p>Values of <span class="math notranslate nohighlight">\(BF &gt; 1\)</span> indicates that model <span class="math notranslate nohighlight">\(M_0\)</span> it is better at explaining
the data when compared with model <span class="math notranslate nohighlight">\(M_1\)</span>. In practice it is common to use
rules of thumb indicating when a BF is small, large, not that large, etc
<a class="footnote-reference brackets" href="#id120" id="id63">21</a>.</p>
<p>Bayes factor is appealing because it is a direct application of Bayes’
theorem as we can see from Equation <a class="reference internal" href="#equation-eq-posterior-model">(11.83)</a>, but this
is also true for the harmonic mean estimator (see Section <a class="reference internal" href="#harmonic-mean"><span class="std std-ref">The Harmonic Mean Estimator</span></a>)
and that does not automatically makes it a good estimator. Bayes factor
is also appealing because, contrary to the likelihood of a model, the
marginal likelihood does not necessarily increases with the complexity
of the model. The intuitive reason is that the larger the number of
parameters the more <em>spread out</em> the prior will be with respect to the
likelihood. Or in other words a more <em>spread out</em> prior is one that
admits more datasets, as plausible, than a more concentrated one. This
will be reflected in the marginal likelihood as we will get a smaller
value with a wider prior than with a more concentrated prior.</p>
<p>Besides the computational problem, the marginal likelihood has a feature
that it is usually considered as a bug. It is <em>very sensitive</em> to the
choice of priors. By <em>very sensitive</em> we mean changes that while
irrelevant for inference, have a practical effect in the value of the
marginal likelihood. To exemplify this, assume we have the model:</p>
<div class="math notranslate nohighlight" id="equation-eq-normal-normal">
<span class="eqno">(11.85)<a class="headerlink" href="#equation-eq-normal-normal" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \mu \sim&amp;\; \mathcal{N}(0, \sigma_0) \\
    Y \sim&amp;\; \mathcal{N}(\mu, \sigma_1)
\end{split}\end{split}\]</div>
<p>The marginal log-likelihood for this model is can be computed
analytically as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">σ_0</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">σ_1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
<span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="n">σ_0</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">σ_1</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>-1.2655121234846454
</pre></div>
</div>
<p>If you change the value of the prior parameter <span class="math notranslate nohighlight">\(\sigma_0\)</span> to 2.5 instead
of 1, the marginal likelihood will be about 2 times smaller and by
changing it to 10 it will be about 7 times smaller. You can use a PPL to
compute the posterior for this model and see for yourself how
influential is the change of the prior in the posterior. Additionally
you can check <a class="reference internal" href="#fig-posterior-ml"><span class="std std-numref">Fig. 11.26</span></a> in the next section.</p>
</section>
<section id="bayes-factor-vs-waic-and-loo">
<span id="id64"></span><h3><span class="section-number">11.7.3. </span>Bayes Factor vs WAIC and LOO<a class="headerlink" href="#bayes-factor-vs-waic-and-loo" title="Permalink to this headline">¶</a></h3>
<p>In this book we do not use Bayes factors to compare model, we prefer
instead the use of LOO. So it maybe useful to better understand how BFs
are related to these other estimators. If we omit the details we can say
that:</p>
<ul class="simple">
<li><p>WAIC is the posterior-averaged log-likelihood</p></li>
<li><p>LOO is the posterior-averaged log-likelihood</p></li>
<li><p>The marginal likelihood is the prior-averaged (log)likelihood <a class="footnote-reference brackets" href="#id121" id="id65">22</a>.</p></li>
</ul>
<p>Let us discuss how this helps to understand the similarities and
differences between these three quantities. All of them use a log-score
as a measure of fitness with different computation. WAIC use a
penalization term computed from the posterior variance. While both LOO
and marginal likelihood avoids needing to use an explicit penalization
term. LOO achieves this by approximating a leave-one-out
cross-validation procedure. That is, it approximates using a dataset to
fit the data and a different dataset to evaluate its fit. The
penalization in marginal likelihood comes from averaging over the entire
prior, with the spread of the prior (relatively) to the likelihood
working as built-in penalizer. The penalization used in the marginal
likelihood it seems to be somehow similar to the penalization in WAIC,
although WAIC uses the variance of the posterior and so is close to the
penalization in cross validation. Because, as previously discussed, a
more spread prior admits more datasets as plausible than a more
concentrated one, computing marginal likelihood is like implicitly
averaging over all the datasets admitted by the prior.</p>
<p>An alternative, and equivalent, way to conceptualize the marginal
likelihood is to notice that it is the prior predictive distribution
evaluated at a particular dataset <span class="math notranslate nohighlight">\(Y\)</span>. Thus, it is telling us how likely
the data is under our model. And the model includes the prior and the
likelihood.</p>
<p>For WAIC and LOO the role of the prior is indirect. The prior affects
the value of WAIC and LOO only by its effect on the posterior. The more
informative the data with respect to the prior, or in other words the
greater the difference between prior and posterior, the less sensitive
will be WAIC and LOO to the details of the prior. Instead, marginal
likelihood use priors directly as we need to average the likelihood over
the prior. Conceptually we can say that Bayes factors are focused on
identifying the best model (and the prior is part of the model) while
WAIC and LOO are focused on which (fitted) model and parameter will give
the best predictions. <a class="reference internal" href="#fig-posterior-ml"><span class="std std-numref">Fig. 11.26</span></a> shows 3 posteriors for
the model defined in Equation <a class="reference internal" href="#equation-eq-normal-normal">(11.85)</a>, for <span class="math notranslate nohighlight">\(\sigma_0=1\)</span>,
<span class="math notranslate nohighlight">\(\sigma_0=10\)</span> and <span class="math notranslate nohighlight">\(\sigma_0=100\)</span>. As we can see, the posteriors are very
close to each other, especially the last two. We can see that the values
of WAIC and LOO only slightly change for the different posteriors, while
the log marginal likelihood is sensitive to the choice of the prior. The
posterior and log marginal likelihoods were computed analytically, WAIC
and LOO were computed from samples from the posterior (for details see
the accompanying code).</p>
<figure class="align-default" id="fig-posterior-ml">
<a class="reference internal image-reference" href="../_images/ml_waic_loo.png"><img alt="../_images/ml_waic_loo.png" src="../_images/ml_waic_loo.png" style="width: 7.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.26 </span><span class="caption-text">Prior (gray line) and posterior (blue line) for the model in Equation
<a class="reference internal" href="#equation-eq-normal-normal">(11.85)</a>. WAIC and LOO reflect that the posterior
distributions are almost identical, while the marginal likelihood
reflects that the prior are different.</span><a class="headerlink" href="#fig-posterior-ml" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The above discussion helps to explain why Bayes factors are widely used
in some fields and disliked in others. When priors are closer to
reflecting some underlying <em>true</em> model, the sensitivity of the marginal
likelihood to the prior specification is less worrisome. When priors are
mainly used for their regularizing properties and when possible to
provide some background knowledge, that sensitivity could be seen as
problematic.</p>
<p>As a result, we think WAIC, and especially LOO, has more practical
value, as their computation is generally more robust, and without the
need to use special inference methods. And in the case of LOO, we also
have good diagnostics.</p>
</section>
</section>
<section id="moving-out-of-flatland">
<span id="high-dimensions"></span><h2><span class="section-number">11.8. </span>Moving out of Flatland<a class="headerlink" href="#moving-out-of-flatland" title="Permalink to this headline">¶</a></h2>
<p>In Flatland: A Romance of Many Dimensions by Edwin Abbott
<span id="id66">[<a class="reference internal" href="references.html#id154" title="E.A. Abbott and R. Jann. Flatland: A Romance of Many Dimensions. Oxford World's Classics. OUP Oxford, 2008. ISBN 9780199537501.">144</a>]</span> it tells the story of a square living in flatland, a
two dimensional world inhabited by n-side polygons and where status is
defined by the number of sides; with women being simple line-segments,
and priests insisting they are circles even when then are just
high-order polygons. The novel, first published in 1984, works equally
well as social satire about the difficulties to understand ideas beyond
our common experience.</p>
<p>As it happens to Square in flatland we are now going to evidence the
weirdness of higher-dimensional spaces.</p>
<p>Suppose we want to estimate the value of <span class="math notranslate nohighlight">\(\pi\)</span>. A simple procedure to do
this is as follows. Inscribe a circle into a square, generate <span class="math notranslate nohighlight">\(N\)</span> points
uniformly lying in that square and then count the proportion that fall
inside the circle. Technically this is a Monte Carlo integration as we
are calculating the value of a definite integral by using a
(pseudo)random number generator.</p>
<p>The area of the circle and square are proportional to the number of
points inside the circle and the total points. If the square has side
<span class="math notranslate nohighlight">\(2R\)</span>, it area will be <span class="math notranslate nohighlight">\((2R)^2\)</span> and the circle inscribe it inside of the
square will have area <span class="math notranslate nohighlight">\(\pi R^2\)</span>. The we have that:</p>
<div class="math notranslate nohighlight" id="equation-eq-circ-mc">
<span class="eqno">(11.86)<a class="headerlink" href="#equation-eq-circ-mc" title="Permalink to this equation">¶</a></span>\[\frac{\text{inside}}{N} \propto \frac{\pi R^2}{(2R)^2}\]</div>
<p>By simplifying and rearranging we get that we can approximate <span class="math notranslate nohighlight">\(\pi\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-eq-pi-mc">
<span class="eqno">(11.87)<a class="headerlink" href="#equation-eq-pi-mc" title="Permalink to this equation">¶</a></span>\[\hat \pi = 4 \frac{\text{Count}_{inside}}{N}\]</div>
<p>We can implement this in a few lines of Python code as in Code Block
<a class="reference internal" href="#montecarlo"><span class="std std-ref">montecarlo</span></a> and the simulated points with the
estimated value of <span class="math notranslate nohighlight">\(\pi\)</span> and the error of the approximation is shown in
<a class="reference internal" href="#fig-monte-carlo"><span class="std std-numref">Fig. 11.27</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="montecarlo">
<div class="code-block-caption"><span class="caption-number">Listing 11.7 </span><span class="caption-text">montecarlo</span><a class="headerlink" href="#montecarlo" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="n">inside</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>
<span class="n">pi</span> <span class="o">=</span> <span class="n">inside</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">*</span><span class="mi">4</span><span class="o">/</span><span class="n">N</span>
<span class="n">error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">((</span><span class="n">pi</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-monte-carlo">
<a class="reference internal image-reference" href="../_images/monte_carlo.png"><img alt="../_images/monte_carlo.png" src="../_images/monte_carlo.png" style="width: 5.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.27 </span><span class="caption-text">Estimating <span class="math notranslate nohighlight">\(\pi\)</span> using Monte Carlo samples, legend shows the estimation
and percentage error.</span><a class="headerlink" href="#fig-monte-carlo" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As the draws are iid, we can apply the central limit theorem here and
then we know the error is reduced at a rate <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{N}}\)</span>),
meaning that for each additional decimal place of accuracy we want we
will need to increase the number of draws <code class="docutils literal notranslate"><span class="pre">N</span></code> by a factor of 100.</p>
<p>What we have just done is an example of a Monte Carlo method <a class="footnote-reference brackets" href="#id122" id="id67">23</a>,
basically any method that uses (pseudo)random samples to compute
something. And technically what we have done is a Monte Carlo
integration as we are calculating the value of a definite integral (an
area) by using samples. Monte Carlo methods are everywhere in
statistics.</p>
<p>In Bayesian statistics we need to compute integrals to obtain posteriors
or compute expectations from it. You may suggest that we can use a
variation of this idea to compute quantities more interesting than
<span class="math notranslate nohighlight">\(\pi\)</span>. It turns out that this method will generally not work very well
as we increase the dimensionality of the problem. In Code Block
<a class="reference internal" href="#inside-out"><span class="std std-ref">inside_out</span></a> we count the number of points
inside a circle when sampled from a square as we did before but from
dimension 2 to 15. The result is in <a class="reference internal" href="#fig-inside-out"><span class="std std-numref">Fig. 11.28</span></a>, weirdly,
we see that as we increase the dimension of the problem and even when
the hypersphere is <em>touching</em> the walls of the hypercube, the proportion
of points inside drops rapidly. In a sense in higher dimensions all the
volume of the hypercube is at the corners <a class="footnote-reference brackets" href="#id123" id="id68">24</a>.</p>
<div class="literal-block-wrapper docutils container" id="inside-out">
<div class="code-block-caption"><span class="caption-number">Listing 11.8 </span><span class="caption-text">inside_out</span><a class="headerlink" href="#inside-out" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="n">dims</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">prop</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">total</span><span class="p">))</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">dims</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">prop</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inside</span> <span class="o">/</span> <span class="n">total</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-inside-out">
<a class="reference internal image-reference" href="../_images/inside_out.png"><img alt="../_images/inside_out.png" src="../_images/inside_out.png" style="width: 5.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.28 </span><span class="caption-text">As we increase the dimensions the chance of getting a point inside an
hypersphere inscribed into a hyper-cube goes to zero. This shows that in
higher dimensions, almost all of the volume of an hypercube is in its
corners.</span><a class="headerlink" href="#fig-inside-out" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Let us see another examples using Multivariate Gaussian.
<a class="reference internal" href="#fig-distance-to-mode"><span class="std std-numref">Fig. 11.29</span></a> shows that as we increase the
dimensionality of a Gaussian, most of the mass of that Gaussian is
located further and further away from the mode. In fact, most of the
mass is around an <em>annulus</em> at radius <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> from the mode, in other
words as we increase the dimensionality of the Gaussian the mode becomes
less and less typical. In higher dimensions the mode, which is also the
mean, is actually an outlier. The reason is that it is very unusual for
any given point to be average in all dimensions!</p>
<p>We can also see this from another perspective. The mode is always the
point with highest density, even if in high dimensional space. The key
insight is noting that it is unique (like the point from flatland!). If
we move away from the mode we will find points that are individually
less likely but there are a lot of them. As we saw in Section <a class="reference internal" href="#cont-rvs"><span class="std std-ref">Continuous Random Variables and Distributions</span></a> a
probability is computed as the integral of the density over a volume
(actually an interval in the one dimensional case), so to find out where
all the mass of a distribution is we have to balance both the density
and their volume. As we increase the dimension of the Gaussian we will
be most likely to pick a point from an <em>annulus</em> that excludes the mode.
The region of the space containing most of the mass of a probability
distribution is known as the typical set. In Bayesian statistics we care
about it, because if we are going to approximate a high dimensional
posterior with samples it suffices that the samples come from the
typical set.</p>
<figure class="align-default" id="fig-distance-to-mode">
<a class="reference internal image-reference" href="../_images/distance_to_mode.png"><img alt="../_images/distance_to_mode.png" src="../_images/distance_to_mode.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.29 </span><span class="caption-text">As we increase the dimension of a Gaussian most of the mass is
distributed farther and farther away from the mode of that Gaussian.</span><a class="headerlink" href="#fig-distance-to-mode" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="inference-methods">
<span id="id69"></span><h2><span class="section-number">11.9. </span>Inference Methods<a class="headerlink" href="#inference-methods" title="Permalink to this headline">¶</a></h2>
<p>There a a myriad of methods to compute the posterior. If we exclude the
exact analytical solutions we already discussed in Chapter
<a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a> when we discussed conjugate priors, we can classify
inference methods into 3 large groups:</p>
<ol class="simple">
<li><p>Deterministic integration methods, that we have not yet seen in the
book, but we will do next</p></li>
<li><p>Simulations methods, also introduced in Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a>
and the methods of choice through out the entire book and finally</p></li>
<li><p>Approximation methods, for example, the ABC method discussed in
Chapter <a class="reference internal" href="chp_08.html#chap8"><span class="std std-ref">8</span></a>, in the case that the likelihood function
does not have a closed form expression.</p></li>
</ol>
<p>While some methods could be combinations of these categories, we still
think it is useful as to order the plethora of available methods.</p>
<p>For a good chronological tour of Bayesian computation methods over the
past two and a half centuries, with an special emphasis on those that
transformed Bayesian inference we recommend you read Computing Bayes:
Bayesian Computation from 1763 to the 21st Century <span id="id70">[<a class="reference internal" href="references.html#id116" title="Gael M Martin, David T Frazier, and Christian P Robert. Computing bayes: bayesian computation from 1763 to the 21st century. arXiv preprint arXiv:2004.06425, 2020.">145</a>]</span></p>
<section id="grid-method">
<span id="id71"></span><h3><span class="section-number">11.9.1. </span>Grid Method<a class="headerlink" href="#grid-method" title="Permalink to this headline">¶</a></h3>
<p>The grid method is a simple brute-force approach. We want to know the
value of posterior distribution over its domain to be able to use it
(finding the maximum, computing expectation, etc). Even if you are not
able to compute the whole posterior, you may be able to evaluate the
prior and the likelihood density function point-wise; this is a pretty
common scenario, if not the most common one. For a single parameter
model, the grid approximation is:</p>
<ul class="simple">
<li><p>Find a reasonable interval for the parameter (the prior should give
some hints).</p></li>
<li><p>Define a grid of points (generally equidistant) on that interval.</p></li>
<li><p>For each point in the grid, multiply the likelihood and the prior.
Optionally, we may normalize the computed values so the posterior
sum to 1 by dividing the result at each point by the sum of all
points</p></li>
</ul>
<p>Code Block <a class="reference internal" href="#id72"><span class="std std-ref">grid_method</span></a> computes the
posterior the Beta-Binomial model:</p>
<div class="literal-block-wrapper docutils container" id="id72">
<div class="code-block-caption"><span class="caption-number">Listing 11.9 </span><span class="caption-text">grid_method</span><a class="headerlink" href="#id72" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">posterior_grid</span><span class="p">(</span><span class="n">ngrid</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">β</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">trials</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">ngrid</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">α</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">trials</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
    <span class="n">posterior</span> <span class="o">/=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">posterior</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-grid-method">
<a class="reference internal image-reference" href="../_images/grid_method.png"><img alt="../_images/grid_method.png" src="../_images/grid_method.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.30 </span><span class="caption-text">By evaluating the prior and the likelihood pointwise over a grid we can
approximate the posterior.</span><a class="headerlink" href="#fig-grid-method" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We can get a better approximation by increasing the number of points of
the grid. In fact if we use infinite number of points we will get the
exact posterior, at the cost of needing infinite computing resources.
The biggest caveat of the grid approach is that this method scales
poorly with the number of parameters as explained in
<a class="reference internal" href="#high-dimensions"><span class="std std-ref">Moving out of Flatland</span></a>.</p>
</section>
<section id="metropolis-hastings">
<span id="sec-metropolis-hastings"></span><h3><span class="section-number">11.9.2. </span>Metropolis-Hastings<a class="headerlink" href="#metropolis-hastings" title="Permalink to this headline">¶</a></h3>
<p>We introduced Metropolis-Hastings algorithm
<span id="id73">[<a class="reference internal" href="references.html#id106" title="Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. The journal of chemical physics, 21(6):1087–1092, 1953.">7</a>, <a class="reference internal" href="references.html#id107" title="WK HASTINGS. Monte carlo sampling methods using markov chains and their applications. Biometrika, 57(1):97–109, 1970.">8</a>, <a class="reference internal" href="references.html#id108" title="Marshall N Rosenbluth. Genesis of the monte carlo algorithm for statistical mechanics. In AIP Conference Proceedings, volume 690, 22–30. American Institute of Physics, 2003.">9</a>]</span> very early in Section <a class="reference internal" href="chp_01.html#sampling-methods-intro"><span class="std std-ref">A DIY Sampler, Do Not Try This at Home</span></a>
and show a simple Python implementation in Code Block
<a class="reference internal" href="chp_01.html#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a>. We will now provide more detail about why this
method works. We will do it using the language of Markov Chains
introduced in Section <a class="reference internal" href="#markov-chains"><span class="std std-ref">Markov Chains</span></a>.</p>
<p>The Metropolis-Hastings algorithm is a general method that allow us to
start with any irreducible Markov chain on the state space of interest
and then modify it into a new Markov chain that has the stationary
distribution that we really care. In other words we take samples from an
easy to sample distribution like a Multivariate Normal and we turn those
samples into samples from our target distribution. The way we modify the
original chain is by being selective, we only accept some of the samples
and reject the others. As we saw in Chapter <a class="reference internal" href="chp_01.html#chap1"><span class="std std-ref">1</span></a>. The
probability of accepting a new proposal is:</p>
<div class="math notranslate nohighlight">
\[p_a (x_{i + 1} \mid x_i) = \min \left (1, \frac{p(x_{i + 1}) \; 
q(x_i \mid x_{i + 1})} {p(x_i) \; q (x_{i + 1} \mid x_i)} \right)\]</div>
<p>Let us rewrite this in a shorter form, for easier manipulation.</p>
<div class="math notranslate nohighlight" id="equation-eq-acceptance-prob1">
<span class="eqno">(11.88)<a class="headerlink" href="#equation-eq-acceptance-prob1" title="Permalink to this equation">¶</a></span>\[a_{ij} = \min \left (1, \frac{p_j q_{ji}}{{p_i q_{ij}}} \right)\]</div>
<p>That is we propose with probability <span class="math notranslate nohighlight">\(q_{ij}\)</span> (read the subscript <span class="math notranslate nohighlight">\(ij\)</span> as
from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>) and accepts the proposal with probability <span class="math notranslate nohighlight">\(a_{ij}\)</span>. One
of the nice feature of this method is that we do not need to know the
normalizing constant of the distribution we want to sample, as it will
be cancelled out when we compute <span class="math notranslate nohighlight">\(\frac{p_j}{p_i}\)</span>. This is very
important because in may many problems, including Bayesian inference,
computing that normalization constant (the marginal likelihood) is very
difficult.</p>
<p>We will now show that the Metropolis-Hastings chain is reversible with
stationary distribution <span class="math notranslate nohighlight">\(p\)</span> as we mentioned in Section <a class="reference internal" href="#markov-chains"><span class="std std-ref">Markov Chains</span></a>. We
need to proof that the detailed balance condition i.e. the reversibility
condition holds, that is:</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{T}\)</span> be the transition matrix, we just need to show that
<span class="math notranslate nohighlight">\(p_i t_{ij} = p_j t_{ji}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>, this is trivial when
<span class="math notranslate nohighlight">\(i = j\)</span> so we assume that <span class="math notranslate nohighlight">\(i \neq j\)</span>, we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-transition">
<span class="eqno">(11.89)<a class="headerlink" href="#equation-eq-transition" title="Permalink to this equation">¶</a></span>\[t_{ij} = q_{ij} a_{ij}\]</div>
<p>Meaning that the probability to transition from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span> is the
probability of proposing the move times the probability of accepting it.
Let us first see the case where the probability of acceptance is less
that 1, this happens when <span class="math notranslate nohighlight">\(p_j q_{ji} \le p_i q_{ij}\)</span>, then we have that</p>
<div class="math notranslate nohighlight" id="equation-eq-acceptance-ij">
<span class="eqno">(11.90)<a class="headerlink" href="#equation-eq-acceptance-ij" title="Permalink to this equation">¶</a></span>\[a_{ij} = \frac{p_j q_{ji}}{p_i q_{ij}}\]</div>
<p>and also</p>
<div class="math notranslate nohighlight" id="equation-eq-acceptance-ji">
<span class="eqno">(11.91)<a class="headerlink" href="#equation-eq-acceptance-ji" title="Permalink to this equation">¶</a></span>\[a_{ji} = 1\]</div>
<p>Using Equation <a class="reference internal" href="#equation-eq-transition">(11.89)</a>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-transition2">
<span class="eqno">(11.92)<a class="headerlink" href="#equation-eq-transition2" title="Permalink to this equation">¶</a></span>\[p_i t_{ij} = p_i q_{ij} a_{ij}\]</div>
<p>replacing <span class="math notranslate nohighlight">\(a_{ij}\)</span> in Equation <a class="reference internal" href="#equation-eq-acceptance-ij">(11.90)</a></p>
<div class="math notranslate nohighlight" id="equation-eq-transition3">
<span class="eqno">(11.93)<a class="headerlink" href="#equation-eq-transition3" title="Permalink to this equation">¶</a></span>\[p_i t_{ij} = p_i q_{ij} \frac{p_j q_{ji}}{p_i q_{ij}}\]</div>
<p>simplifying above we get:</p>
<div class="math notranslate nohighlight" id="equation-eq-transition4">
<span class="eqno">(11.94)<a class="headerlink" href="#equation-eq-transition4" title="Permalink to this equation">¶</a></span>\[p_i t_{ij} = p_j q_{ji}\]</div>
<p>Because <span class="math notranslate nohighlight">\(a_{ji} = 1\)</span> we can include it without changing the validity of
the equation.</p>
<div class="math notranslate nohighlight" id="equation-eq-transition5">
<span class="eqno">(11.95)<a class="headerlink" href="#equation-eq-transition5" title="Permalink to this equation">¶</a></span>\[p_i t_{ij} = p_j q_{ji} a_{ji}\]</div>
<p>which finally we get that</p>
<div class="math notranslate nohighlight" id="equation-eq-transition6">
<span class="eqno">(11.96)<a class="headerlink" href="#equation-eq-transition6" title="Permalink to this equation">¶</a></span>\[p_i t_{ij} = p_j t_{ji}\]</div>
<p>By symmetry when <span class="math notranslate nohighlight">\(p_j q_{ji} &gt; p_i q_{ij}\)</span> we will arrive at the same
result. As the reversibility condition holds, <span class="math notranslate nohighlight">\(p\)</span> is the stationary
distribution of our Markov chain with transition matrix <span class="math notranslate nohighlight">\(\mathbf{T}\)</span>.</p>
<p>The above proof gives us the theoretical confidence that we can use
Metropolis-Hastings to sample from virtually any distribution we want.
We can also see that while this is a very general result, it does not
help us to choose a proposal distribution. In practice the proposal
distribution is very important as the efficiency of the method depends
heavily on this choice. In general it is observed that if the proposal
makes large jumps the probability of acceptance is very low, and the
method spend most of the time rejecting new states and thus stuck in one
place. On the contrary if the proposal takes too small jumps the
acceptance rate is high but the exploration is poor, as the new states
are in a small neighborhood of the old state. A good proposal
distribution is one that generates new putative states far away from the
old state with high acceptance rate. This is generally difficult to do
if we do not know the geometry of the posterior distribution, but that
is precisely what we want to find out. In practice useful
Metropolis-Hastings methods are those that are adaptive
<span id="id74">[<a class="reference internal" href="references.html#id101" title="Heikki Haario, Eero Saksman, and Johanna Tamminen. An adaptive metropolis algorithm. Bernoulli, pages 223–242, 2001.">146</a>, <a class="reference internal" href="references.html#id104" title="Christophe Andrieu and Johannes Thoms. A tutorial on adaptive mcmc. Statistics and computing, 18(4):343–373, 2008.">147</a>, <a class="reference internal" href="references.html#id103" title="Gareth O Roberts and Jeffrey S Rosenthal. Examples of adaptive mcmc. Journal of computational and graphical statistics, 18(2):349–367, 2009.">148</a>, <a class="reference internal" href="references.html#id102" title="Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, and Arthur Gretton. Kernel adaptive metropolis-hastings. In International conference on machine learning, 1665–1673. PMLR, 2014.">149</a>]</span>. For example,
we can use a Multivariate Gaussian distribution as proposal
distribution. During tuning we can compute the empirical covariance from
the posterior samples and use it as the covariance matrix of the
proposal distribution. We can also scale the covariance matrix so that
the average acceptance rate approach a predefined acceptance rate
<span id="id75">[<a class="reference internal" href="references.html#id105" title="Andrew Gelman, Walter R Gilks, and Gareth O Roberts. Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110–120, 1997.">150</a>, <a class="reference internal" href="references.html#id109" title="Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling for various metropolis-hastings algorithms. Statistical science, 16(4):351–367, 2001.">151</a>, <a class="reference internal" href="references.html#id110" title="Mylene Bedard. Optimal acceptance rates for metropolis algorithms: moving beyond 0.234. Stochastic Processes and their Applications, 118(12):2198–2222, 2008.">152</a>]</span>. In fact there is evidence
that under certain circumstances and when the dimensionality of the
posterior increases the optimal acceptance rate converges to the magic
number 0.234 <span id="id76">[<a class="reference internal" href="references.html#id105" title="Andrew Gelman, Walter R Gilks, and Gareth O Roberts. Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110–120, 1997.">150</a>]</span>. In practice it seems that an acceptance
rate around 0.234 or a little bit higher gives more or less the same
performance but the general validity and useful of this result has also
been disputed <span id="id77">[<a class="reference internal" href="references.html#id111" title="Chris Sherlock. Optimal scaling of the random walk metropolis: general criteria for the 0.234 acceptance rule. Journal of Applied Probability, 50(1):1–15, 2013.">153</a>, <a class="reference internal" href="references.html#id112" title="Christopher CJ Potter and Robert H Swendsen. 0.234: the myth of a universal acceptance ratio for monte carlo simulations. Physics Procedia, 68:120–124, 2015.">154</a>]</span>.</p>
<p>In the next section we will discuss a clever way to generate proposals
that help to correct most of the problems with basic
Metropolis-Hastings.</p>
</section>
<section id="hamiltonian-monte-carlo">
<span id="hmc"></span><h3><span class="section-number">11.9.3. </span>Hamiltonian Monte Carlo<a class="headerlink" href="#hamiltonian-monte-carlo" title="Permalink to this headline">¶</a></h3>
<p>Hamiltonian Monte Carlo (HMC) <a class="footnote-reference brackets" href="#id124" id="id78">25</a>
<span id="id79">[<a class="reference internal" href="references.html#id95" title="Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216–222, 1987.">155</a>, <a class="reference internal" href="references.html#id96" title="S. Brooks, A. Gelman, G. Jones, and X.L. Meng. Handbook of Markov Chain Monte Carlo. Chapman &amp; Hall/CRC Handbooks of Modern Statistical Methods. CRC Press, 2011. ISBN 9781420079425.">156</a>, <a class="reference internal" href="references.html#id97" title="Michael Betancourt. A conceptual introduction to hamiltonian monte carlo. arXiv preprint arXiv:1701.02434, 2017.">157</a>]</span> is a type of MCMC method that
makes use of gradients to generate new proposed states. The gradients of
the log-probability of the posterior evaluated at some state provides
information of the geometry of the posterior density function. HMC
attempts to avoid the random walk behavior typical of
Metropolis-Hastings by using the gradient to propose new positions far
from the current one with high acceptance probability. This allows HMC
to better scale to higher dimensions and in principle more complex
geometries, than alternatives.</p>
<p>In simple terms, a Hamiltonian is a description of the total energy of a
physical system. We can decompose the total energy into two terms, the
kinetic and the potential energy. For a real system like rolling a ball
down a hill, the potential energy is given by the position of the ball.
The higher the ball the higher the potential energy. The kinetic energy
is given by the velocity of the ball, or more correctly by its momentum
(which takes into account both the velocity and the mass of the object).
We will assume the total energy preserves, meaning that if the system
gains kinetic energy then is because it has lost the same amount of
potential energy. We can write the Hamiltonian of such a systems as:</p>
<div class="math notranslate nohighlight" id="equation-eq-hamiltonian">
<span class="eqno">(11.97)<a class="headerlink" href="#equation-eq-hamiltonian" title="Permalink to this equation">¶</a></span>\[H(\mathbf{q}, \mathbf{p})  = K(\mathbf{p}, \mathbf{q}) + V(\mathbf{q})\]</div>
<p>where <span class="math notranslate nohighlight">\(K(\mathbf{p}, \mathbf{q})\)</span> is called the kinetic energy, and
<span class="math notranslate nohighlight">\(V(\mathbf{q})\)</span> is the potential energy. The probability of finding the
ball at a particular position with a particular momentum is then given
by:</p>
<div class="math notranslate nohighlight" id="equation-eq-canonical">
<span class="eqno">(11.98)<a class="headerlink" href="#equation-eq-canonical" title="Permalink to this equation">¶</a></span>\[p(\mathbf{q}, \mathbf{p}) = e^{-H(\mathbf{q}, \mathbf{p})}\]</div>
<p>To simulate such a systems we need to solve the so called Hamiltonian
equations:</p>
<div class="math notranslate nohighlight" id="equation-eq-hamiltonian-equations">
<span class="eqno">(11.99)<a class="headerlink" href="#equation-eq-hamiltonian-equations" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{d \mathbf{q}}{dt} =&amp; \quad \frac{\partial H}{\partial \mathbf{p}} = \frac{\partial K}{\partial \mathbf{p}} + \frac{\partial V}{\partial \mathbf{p}} \\
\frac{d \mathbf{p}}{dt} =&amp; -\frac{\partial H}{\partial \mathbf{q}}= -\frac{\partial K}{\partial \mathbf{q}} - \frac{\partial V}{\partial \mathbf{q}}\end{aligned}\end{split}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\frac{\partial V}{\partial \mathbf{p}} = \mathbf{0}\)</span>.</p>
<p>Because we are not interested in modeling an idealized ball rolling down
an idealized hill, but to model an idealized particle along the
posterior distribution, we need to make a few adjustments. First the
potential energy is given by the probability density we are trying to
sample from <span class="math notranslate nohighlight">\(p(\mathbf{q})\)</span>. For the momentum we are just going to
invoke an auxiliary variable. That is, a made up variable that will help
us. If we choose <span class="math notranslate nohighlight">\(p(\mathbf{p} \mid \mathbf{q})\)</span> then we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-auxiliary">
<span class="eqno">(11.100)<a class="headerlink" href="#equation-eq-auxiliary" title="Permalink to this equation">¶</a></span>\[p(\mathbf{q}, \mathbf{p}) =  p(\mathbf{p} | \mathbf{q}) p(\mathbf{q})\]</div>
<p>This ensures us that we can recover our target distribution by
marginalize out the momentum. By introducing the auxiliary variable, we
can keep working with the physical analogy, and later remove the
auxiliary variable and go back to our problem, sampling the posterior.
If we replace Equation <a class="reference internal" href="#equation-eq-auxiliary">(11.100)</a> in Equation <a class="reference internal" href="#equation-eq-canonical">(11.98)</a>
we got:</p>
<div class="math notranslate nohighlight" id="equation-eq-hamiltonian-kv">
<span class="eqno">(11.101)<a class="headerlink" href="#equation-eq-hamiltonian-kv" title="Permalink to this equation">¶</a></span>\[H(\mathbf{q}, \mathbf{p}) = \overbrace{-\log p(\mathbf{p} \mid \mathbf{q})}^{K(\mathbf{p}, \mathbf{q})} \overbrace{- \log p(\mathbf{q})}^{ + V(\mathbf{q})}\]</div>
<p>As explained previously, the potential energy <span class="math notranslate nohighlight">\(V(\mathbf{q})\)</span> is given
by the <span class="math notranslate nohighlight">\(p(\mathbf{q})\)</span> the density function of the target posterior
distribution, and we are free to choose the kinetic energy. If we choose
it to be Gaussian, and drop the normalization constant, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-kinetic-energy">
<span class="eqno">(11.102)<a class="headerlink" href="#equation-eq-kinetic-energy" title="Permalink to this equation">¶</a></span>\[K(\mathbf{p}, \mathbf{q}) = \frac{1}{2}\mathbf{p}^T M^{-1}\mathbf{p} + \log |M|\]</div>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is the <strong>precision matrix</strong> that parameterized the Gaussian
distribution (also referred to as the mass matrix in Hamiltonian Monte
Carlo literature). And if we choose <span class="math notranslate nohighlight">\(M = I\)</span>, i.e. the identity matrix
which is <span class="math notranslate nohighlight">\(n \times n\)</span> square matrix with ones on the main diagonal and
zeros elsewhere, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-kinetic-energy2">
<span class="eqno">(11.103)<a class="headerlink" href="#equation-eq-kinetic-energy2" title="Permalink to this equation">¶</a></span>\[K(\mathbf{p}, \mathbf{q}) = \frac{1}{2}\mathbf{p}^T \mathbf{p}\]</div>
<p>This makes calculations easier as now</p>
<div class="math notranslate nohighlight" id="equation-eq-kinetic-momentum">
<span class="eqno">(11.104)<a class="headerlink" href="#equation-eq-kinetic-momentum" title="Permalink to this equation">¶</a></span>\[\frac{\partial K}{\partial \mathbf{p}} = \mathbf{p}\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-eq-kinetic-position">
<span class="eqno">(11.105)<a class="headerlink" href="#equation-eq-kinetic-position" title="Permalink to this equation">¶</a></span>\[\frac{\partial K}{\partial \mathbf{q}} = \mathbf{0}\]</div>
<p>We can then simplify Hamilton’s equations to:</p>
<div class="math notranslate nohighlight" id="equation-eq-hamiltonian-equations2">
<span class="eqno">(11.106)<a class="headerlink" href="#equation-eq-hamiltonian-equations2" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\frac{d \mathbf{q}}{dt} =&amp; \mathbf{p} \\
\frac{d \mathbf{p}}{dt} =&amp; - \frac{\partial V}{\partial \mathbf{q}}\end{aligned}\end{split}\]</div>
<p>Summarizing,the HMC algorithm is then:</p>
<ol class="simple">
<li><p>Sample a <span class="math notranslate nohighlight">\(\mathbf{p} \sim \mathcal{N}(0, I)\)</span></p></li>
<li><p>Simulate <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span> and <span class="math notranslate nohighlight">\(\mathbf{p}_t\)</span> for some amount of time
<span class="math notranslate nohighlight">\(T\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{q}_T\)</span> is our new proposed state</p></li>
<li><p>Use the Metropolis acceptance criterion to accept or reject
<span class="math notranslate nohighlight">\(\mathbf{q}_T\)</span>.</p></li>
</ol>
<p>Why we still need to use the Metropolis acceptance criterion?
Intuitively because we can think of HMC as a Metropolis-Hasting
algorithm with a better proposal distribution. But there is also a very
good numerical justification, because this steps corrects for errors
introduced by the numerical simulation of the Hamiltonian equations.</p>
<p>To compute the Hamiltonian equations we have to compute a trajectory of
the particle, i.e. all the intermediate points between one state and the
next. In practice this involves computing a series of small
<em>integration</em> steps using an integrator method. The most popular one is
the leapfrog integrator. Leapfrog integration is equivalent to updating
positions <span class="math notranslate nohighlight">\(q_t\)</span> momentum <span class="math notranslate nohighlight">\(q_t\)</span> at interleaved time points, staggered in
such a way that they <em>leapfrog</em> over each other.</p>
<p>Code Block <a class="reference internal" href="#leapfrog"><span class="std std-ref">leapfrog</span></a> shows a leapfrog
integrator implemented in Python <a class="footnote-reference brackets" href="#id125" id="id80">26</a>. The arguments are: <code class="docutils literal notranslate"><span class="pre">q</span></code> and <code class="docutils literal notranslate"><span class="pre">p</span></code>
the initial position and momentum respectively. <code class="docutils literal notranslate"><span class="pre">dVdq</span></code> is a Python
function that returns the gradient of the position of some target
density function at position <code class="docutils literal notranslate"><span class="pre">q</span></code>
<span class="math notranslate nohighlight">\(\frac{\partial V}{\partial \mathbf{q}}\)</span>. We used JAX <span id="id81">[<a class="reference internal" href="references.html#id155" title="James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. 2018. URL: http://github.com/google/jax.">116</a>]</span>
auto-differentiation ability to generate this function. <code class="docutils literal notranslate"><span class="pre">path_len</span></code>
indicates how long to integrate for and <code class="docutils literal notranslate"><span class="pre">step_size</span></code> how large each
integration step should be. As a result we obtain a new position and
momentum as output of the function <code class="docutils literal notranslate"><span class="pre">leapfrog</span></code>.</p>
<div class="literal-block-wrapper docutils container" id="leapfrog">
<div class="code-block-caption"><span class="caption-number">Listing 11.10 </span><span class="caption-text">leapfrog</span><a class="headerlink" href="#leapfrog" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leapfrog</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dVdq</span><span class="p">,</span> <span class="n">path_len</span><span class="p">,</span> <span class="n">step_size</span><span class="p">):</span>
    <span class="n">p</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">dVdq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># half step</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">path_len</span> <span class="o">/</span> <span class="n">step_size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">q</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">p</span>  <span class="c1"># whole step</span>
        <span class="n">p</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">dVdq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># whole step</span>
    <span class="n">q</span> <span class="o">+=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">p</span>  <span class="c1"># whole step</span>
    <span class="n">p</span> <span class="o">-=</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">dVdq</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># half step</span>

    <span class="k">return</span> <span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">p</span>  <span class="c1"># momentum flip at end</span>
</pre></div>
</div>
</div>
<p>Note that in function <code class="docutils literal notranslate"><span class="pre">leapfrog</span></code> we flip the sign of the output
momentum. This is the simplest way to achieve a reversible
Metropolis-Hastings proposal, as it augment the numerical integration
with a negative step.</p>
<p>We have now all the ingredients to implement a HMC method in Python, as
in Code Block <a class="reference internal" href="#hamiltonian-mc"><span class="std std-ref">hamiltonian_mc</span></a>. Like our previous
Metropolis-Hasting example in Code Block <a class="reference internal" href="chp_01.html#metropolis-hastings"><span class="std std-ref">metropolis_hastings</span></a> this is
not meant to be use for serious model inference but instead a simple
example to demonstrate the method. The arguments are <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> the
number of samples to return, <code class="docutils literal notranslate"><span class="pre">negative_log_prob</span></code> the negative log
probability to sample from, <code class="docutils literal notranslate"><span class="pre">initial_position</span></code> the initial position to
start sampling, <code class="docutils literal notranslate"><span class="pre">path_len</span></code>, <code class="docutils literal notranslate"><span class="pre">step_size</span></code>, as a result we obtain sample
from the target distribution.</p>
<div class="literal-block-wrapper docutils container" id="hamiltonian-mc">
<div class="code-block-caption"><span class="caption-number">Listing 11.11 </span><span class="caption-text">hamiltonian_mc</span><a class="headerlink" href="#hamiltonian-mc" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hamiltonian_monte_carlo</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="n">negative_log_prob</span><span class="p">,</span> <span class="n">initial_position</span><span class="p">,</span> 
    <span class="n">path_len</span><span class="p">,</span> <span class="n">step_size</span><span class="p">):</span>
    <span class="c1"># autograd magic</span>
    <span class="n">dVdq</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">negative_log_prob</span><span class="p">)</span>

    <span class="c1"># collect all our samples in a list</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">initial_position</span><span class="p">]</span>

    <span class="c1"># Keep a single object for momentum resampling</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># If initial_position is a 10d vector and n_samples is 100, we want</span>
    <span class="c1"># 100 x 10 momentum draws. We can do this in one call to momentum.rvs, and</span>
    <span class="c1"># iterate over rows</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,)</span> <span class="o">+</span> <span class="n">initial_position</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">p0</span> <span class="ow">in</span> <span class="n">momentum</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">):</span>
        <span class="c1"># Integrate over our path to get a new position and momentum</span>
        <span class="n">q_new</span><span class="p">,</span> <span class="n">p_new</span> <span class="o">=</span> <span class="n">leapfrog</span><span class="p">(</span>
            <span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">p0</span><span class="p">,</span> <span class="n">dVdq</span><span class="p">,</span> <span class="n">path_len</span><span class="o">=</span><span class="n">path_len</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Check Metropolis acceptance criterion</span>
        <span class="n">start_log_p</span> <span class="o">=</span> <span class="n">negative_log_prob</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">momentum</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">p0</span><span class="p">))</span>
        <span class="n">new_log_p</span> <span class="o">=</span> <span class="n">negative_log_prob</span><span class="p">(</span><span class="n">q_new</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">momentum</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">p_new</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">())</span> <span class="o">&lt;</span> <span class="n">start_log_p</span> <span class="o">-</span> <span class="n">new_log_p</span><span class="p">:</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">q_new</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<p><a class="reference internal" href="#fig-normal-leapgrog"><span class="std std-numref">Fig. 11.31</span></a> shows 3 different trajectories around the
same 2D normal distribution. For practical sampling we do not want the
trajectories to be circular, because they will arrive at the same
position that we started at. Instead we want to move as far as possible
from our starting point, for example, by avoiding U-turns in the
trajectory, and hence the name of one of the most popular dynamic HMC
method No U-Turn Sampling (NUTS).</p>
<figure class="align-default" id="fig-normal-leapgrog">
<a class="reference internal image-reference" href="../_images/normal_leapfrog.png"><img alt="../_images/normal_leapfrog.png" src="../_images/normal_leapfrog.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.31 </span><span class="caption-text">Three HMC trajectories <em>around</em> a 2D multivariate normal. The momentum
is indicated by the size and direction of the arrows, with small arrows
indicating small kinetic energy. All these trajectories are computed in
such a way that they end at their starting position, which completing an
elliptical trajectory.</span><a class="headerlink" href="#fig-normal-leapgrog" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We show another example in <a class="reference internal" href="#fig-funnel-leapgrog"><span class="std std-numref">Fig. 11.32</span></a>, which contains
3 different trajectory around the same Neal’s funnel, a common geometry
arising in (centered) hierarchical models as we showed in Section <a class="reference internal" href="chp_04.html#model-geometry"><span class="std std-ref">Posterior Geometry Matters</span></a>.
This is an example of
a trajectory failing to properly simulate following the correct
distribution, we call such trajectories divergent trajectories, or
simply divergences. They are useful diagnostics as explained in Section
<a class="reference internal" href="chp_02.html#divergences"><span class="std std-ref">Divergences</span></a>. Usually, Symplectic integrators like
leapfrog integrator are highly accurate even for long trajectories, as
they tend to be tolerant of small errors and <em>oscillate</em> around the
correct trajectory. Moreover, these small errors can be corrected
exactly by applying the metropolis criteria to accept or reject the
Hamiltonian proposal. However, there is an importance exception to this
ability to generate small, easy to fix errors: when the exact
trajectories lie on regions of high curvature, the numerical
trajectories generated by symplectic integrators can diverge, generating
trajectory that rapidly get off towards the boundaries of the
distribution we are trying to explore.</p>
<figure class="align-default" id="fig-funnel-leapgrog">
<a class="reference internal image-reference" href="../_images/funnel_leapfrog.png"><img alt="../_images/funnel_leapfrog.png" src="../_images/funnel_leapfrog.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.32 </span><span class="caption-text">Three HMC trajectories <em>around</em> a 2D Neal’s funnel. This kind geometry
turns up in centered hierarchical models. We can see that all these trajectories went wrong.
We call this kind of trajectories divergences and can be used as a diagnostic of HMC samplers.</span><a class="headerlink" href="#fig-funnel-leapgrog" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Both Figures <a class="reference internal" href="#fig-normal-leapgrog"><span class="std std-numref">Fig. 11.31</span></a> and
<a class="reference internal" href="#fig-funnel-leapgrog"><span class="std std-numref">Fig. 11.32</span></a> highlight the fact that an efficient HMC
method requires proper tuning of its hyperparameters. HMC has three
hyparameters:</p>
<ul class="simple">
<li><p>the time discretization (step size of the leapfrog)</p></li>
<li><p>the integration time (number of leapfrog steps)</p></li>
<li><p>the precision matrix <span class="math notranslate nohighlight">\(M\)</span> that parameterized the kinetic energy</p></li>
</ul>
<p>For example, if the step size is too large, the leapfrog integrator will
be inaccurate and too many proposals will be rejected. However, if it is
too small we will waste computation resources. If the number of steps is
too small, the simulated trajectory at each iteration will be too short
and sampling will fall back to random walk. But if it is too large the
trajectory might runs in circles and we again waste computation
resources. If the estimated covariance (inverse of the precision matrix)
is too different from the posterior covariance, the proposal momentum
will be suboptimal and the movement in the position space will be too
large or too small in some dimension.</p>
<p>Adaptive dynamics Hamiltonian Monte Carlo methods, like those used by
default in PyMC3, Stan and other PPLs can adapt these hyperparameters
automatically during the warm-up or tuning phase. The step size can be
learning automatically by adjusting it to match a predefined
acceptance-rate target. For example, in PyMC3 you set the argument
<code class="docutils literal notranslate"><span class="pre">target_accept</span></code> <a class="footnote-reference brackets" href="#id126" id="id82">27</a> The precision matrix <span class="math notranslate nohighlight">\(M\)</span> or its inverse can be
estimated from the samples during warm-up phase and the number of steps
can be dynamically adapted at each MCMC step using the NUTS algorithm
<span id="id83">[<a class="reference internal" href="references.html#id98" title="Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. Journal of Machine Learning Research, 15(47):1593–1623, 2014.">158</a>]</span>. In order to avoid too long trajectory that could go near
the initialization point, NUTS extends the trajectory backward and
forwards until a U-turn criterion is met. Additionally, NUTS applies a
multinomial sampling to choose from all the generated points from the
trajectory, as this provides a better criteria for efficient exploration
of the target distribution (sampling from the trajectory could be done
with fixed integration time HMC as well).</p>
</section>
<section id="sequential-monte-carlo">
<span id="smc-details"></span><h3><span class="section-number">11.9.4. </span>Sequential Monte Carlo<a class="headerlink" href="#sequential-monte-carlo" title="Permalink to this headline">¶</a></h3>
<p>Sequential Monte Carlo is a family of Monte Carlo methods also known as
particle filters. It has wide application to Bayesian inference for
static models and dynamic models such as sequential time series
inference and signal processing
<span id="id84">[<a class="reference internal" href="references.html#id38" title="N. Chopin and O. Papaspiliopoulos. An Introduction to Sequential Monte Carlo. Springer Series in Statistics. Springer International Publishing, 2020. ISBN 9783030478445.">63</a>, <a class="reference internal" href="references.html#id37" title="Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential monte carlo samplers. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411–436, 2006.">159</a>, <a class="reference internal" href="references.html#id36" title="Jianye Ching and Yi-Chu Chen. Transitional markov chain monte carlo method for bayesian model updating, model class selection, and model averaging. Journal of engineering mechanics, 133(7):816–832, 2007.">160</a>, <a class="reference internal" href="references.html#id35" title="Christian A Naesseth, Fredrik Lindsten, and Thomas B Schön. Elements of sequential monte carlo. arXiv preprint arXiv:1903.04797, 2019.">161</a>]</span>. There are many
variations and implementation under the same or similar name, with
different application. Thus you might at times find the literature a bit
confusing. We will give a brief description of the SMC/SMC-ABC method as
implemented in PyMC3 and TFP. For a detailed discussion of SMC methods
under a unified framework we recommend the book An Introduction to
Sequential Monte Carlo <span id="id85">[<a class="reference internal" href="references.html#id38" title="N. Chopin and O. Papaspiliopoulos. An Introduction to Sequential Monte Carlo. Springer Series in Statistics. Springer International Publishing, 2020. ISBN 9783030478445.">63</a>]</span>.</p>
<p>First note that we can write the posterior in the following way:</p>
<div class="math notranslate nohighlight" id="equation-eq-powered-posterior">
<span class="eqno">(11.107)<a class="headerlink" href="#equation-eq-powered-posterior" title="Permalink to this equation">¶</a></span>\[p(\boldsymbol{\theta} \mid Y)_{\beta}  \propto 
    p(Y \mid \boldsymbol{\theta})^{\beta} \; p(\boldsymbol{\theta})\]</div>
<p>When <span class="math notranslate nohighlight">\(\beta = 0\)</span> we see that <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} \mid Y)_{\beta}\)</span> is
the prior and when <span class="math notranslate nohighlight">\(\beta = 1\)</span> we see that
<span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} \mid Y)_{\beta}\)</span> is the <em>true</em> posterior <a class="footnote-reference brackets" href="#id127" id="id86">28</a>.</p>
<p>SMC proceeds by increasing the value of <span class="math notranslate nohighlight">\(\beta\)</span> in <span class="math notranslate nohighlight">\(s\)</span> successive stages
<span class="math notranslate nohighlight">\(\{\beta_0=0 &lt; \beta_1  &lt; ...  &lt; \beta_s=1\}\)</span>. Why is this a good idea?
There are two related ways to justify it. First, the stepping stones
analogy. Instead of directly trying to sample from the posterior we
begin by sampling from the prior, which is generally easier to do. Then
we add some intermediate distributions until we reach the posterior (see
<a class="reference internal" href="chp_08.html#fig-smc-tempering"><span class="std std-numref">Fig. 8.2</span></a>). Second is the temperature
analogy. The <span class="math notranslate nohighlight">\(\beta\)</span> parameters is analogue to the inverse temperature
of a physical system, as we decrease its value (increase the
temperature) the system is able to access to more states, and as we
decrease its value (decrease the temperature) the system “freezes” into
the posterior <a class="footnote-reference brackets" href="#id128" id="id87">29</a>. <a class="reference internal" href="chp_08.html#fig-smc-tempering"><span class="std std-numref">Fig. 8.2</span></a> shows an
hypothetical sequence of tempered posteriors. The use of the temperature
(or its inverse) as an auxiliary parameter is known as tempering, the
term annealing is also common <a class="footnote-reference brackets" href="#id129" id="id88">30</a>.</p>
<p>The SMC method, as implemented in PyMC3 and TFP, can be summarized as
follows:</p>
<ol class="simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\beta\)</span> at zero.</p></li>
<li><p>Generate <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(s_{\beta}\)</span> from the tempered posterior.</p></li>
<li><p>Increase <span class="math notranslate nohighlight">\(\beta\)</span> in order to keep the effective sample size <a class="footnote-reference brackets" href="#id130" id="id89">31</a> at
a predefined value.</p></li>
<li><p>Compute a set of <span class="math notranslate nohighlight">\(N\)</span> importance weights <span class="math notranslate nohighlight">\(W\)</span>. The weights are
computed according to the new and old tempered posterior.</p></li>
<li><p>Obtain <span class="math notranslate nohighlight">\(s_w\)</span> by resampling <span class="math notranslate nohighlight">\(s_{\beta}\)</span> according to <span class="math notranslate nohighlight">\(W\)</span>.</p></li>
<li><p>Run <span class="math notranslate nohighlight">\(N\)</span> MCMC chains for <span class="math notranslate nohighlight">\(k\)</span> steps, starting each one from a
different sample in <span class="math notranslate nohighlight">\(s_w\)</span> and retaining only the samples in the last
step.</p></li>
<li><p>Repeat from step 3 until <span class="math notranslate nohighlight">\(\beta=1\)</span></p></li>
</ol>
<p>The resampling step works by removing samples with a low probability and
replacing them with samples with a higher probability. This step
decreases the diversity of the samples. Then, the MCMC step perturbs the
samples, hopefully increasing the diversity and therefore helping SMC to
explore the parameter space. Any valid MCMC transition kernel could be
used in SMC, and depending on your problem you might find some perform
better than others. For example, with ABC methods we generally need to
rely on gradient-free methods such as Random Walk Metropolis-Hasting as
the simulators are generally not differentiable.</p>
<p>The efficiency of the tempered method depends heavily on the
intermediate values of <span class="math notranslate nohighlight">\(\beta\)</span>. The smaller the difference between two
successive values of <span class="math notranslate nohighlight">\(\beta\)</span>, the closer the two successive tempered
posteriors will be, and thus the easier the transition from one stage to
the next. But if the steps are too small, we will need many intermediate
stages, and beyond some point this will waste a lot of computational
resources without really improving the accuracy of the results. Another
important factor is the efficiency of the MCMC transitional kernel that
adds diversity to the samples. To help improve the efficiency of the
transition, PyMC3 and TFP uses the samples from the previous stage to
tune the proposal distribution of the current stage and also the number
of steps taken by the MCMC, with the number of steps being the same
across all chains.</p>
</section>
<section id="variational-inference">
<span id="vi-details"></span><h3><span class="section-number">11.9.5. </span>Variational Inference<a class="headerlink" href="#variational-inference" title="Permalink to this headline">¶</a></h3>
<p>While we do not use variational inference in this book, it is a useful
approach to know about. Compared to MCMC, VI tends to be easier to scale
to large data and is faster to run computationally, but with less
theoretical guarantees of convergence <span id="id90">[<a class="reference internal" href="references.html#id162" title="Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but did it work?: evaluating variational inference. In International Conference on Machine Learning, 5581–5590. PMLR, 2018.">162</a>]</span>.</p>
<p>As we previously mentioned in Section <a class="reference internal" href="#dkl"><span class="std std-ref">Kullback-Leibler Divergence</span></a>, we can use one distribution to
approximate another and then use the Kullback-Leibler (KL) divergence to
measure how good the approximation is. Turns out we can use this
approach to do Bayesian inference as well! Such approach is called
variational inference (VI) <span id="id91">[<a class="reference internal" href="references.html#id159" title="David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: a review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017.">163</a>]</span>. The goal of VI is to
approximate the target probability density, in our case the posterior
distribution <span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} \mid Y)\)</span>, with a surrogate
distribution <span class="math notranslate nohighlight">\(q(\boldsymbol{\theta})\)</span>. In practice we usually choose
<span class="math notranslate nohighlight">\(q(\boldsymbol{\theta})\)</span> to be of simpler form than
<span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} \mid Y)\)</span>, and we find the member of that family
of distributions, which is the closest to the target in the KL
divergence sense, using optimization. With small rewrite to Equation
<a class="reference internal" href="#equation-eq-kl-divergence">(11.41)</a>, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence2">
<span class="eqno">(11.108)<a class="headerlink" href="#equation-eq-kl-divergence2" title="Permalink to this equation">¶</a></span>\[\mathbb{KL}(q(\boldsymbol{\theta}) \parallel p(\boldsymbol{\theta} \mid Y)) = \mathbb{E}_q[\log{q(\boldsymbol{\theta})}-\log{p(\boldsymbol{\theta} \mid Y)}]
    \]</div>
<p>However, this objective is hard to compute because it requires the
marginal likelihood of <span class="math notranslate nohighlight">\(p(Y)\)</span>. To see that let us expand Equation
<a class="reference internal" href="#equation-eq-kl-divergence2">(11.108)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-kl-divergence2-expanded">
<span class="eqno">(11.109)<a class="headerlink" href="#equation-eq-kl-divergence2-expanded" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{split}
    \mathbb{KL}(q(\boldsymbol{\theta}) \parallel p(\boldsymbol{\theta} \mid Y)) &amp;= \mathbb{E}[\log{q(\boldsymbol{\theta})}] - \mathbb{E}[\log{p(\boldsymbol{\theta} \mid Y)}] \\
   &amp;= \mathbb{E}[\log{q(\boldsymbol{\theta})}] - \mathbb{E}[\log{p(\boldsymbol{\theta},  Y)}] + \log{p(Y)}
\end{split}\end{split}\]</div>
<p>Luckily, since <span class="math notranslate nohighlight">\(\log{p(Y)}\)</span> is a constant with respect to
<span class="math notranslate nohighlight">\(q(\boldsymbol{\theta})\)</span>, we can omit it during optimization. Thus, in
practice, we maximize the evidence lower bound (ELBO) as shown in
Equation <a class="reference internal" href="#equation-eq-elbo-vi">(11.110)</a>, which is equivalent to minimizing the KL
divergence:</p>
<div class="math notranslate nohighlight" id="equation-eq-elbo-vi">
<span class="eqno">(11.110)<a class="headerlink" href="#equation-eq-elbo-vi" title="Permalink to this equation">¶</a></span>\[\text{ELBO}(q) = \mathbb{E}[\log{p(\boldsymbol{\theta},  Y)}] - \mathbb{E}[\log{q(\boldsymbol{\theta})}]
    \]</div>
<p>The last piece of the puzzle is to figure out how to compute the
expectation in Equation <a class="reference internal" href="#equation-eq-elbo-vi">(11.110)</a>. Instead of solving an
expensive integration, we compute the average using Monte Carlo samples
drawn from the surrogate distribution <span class="math notranslate nohighlight">\(q(\boldsymbol{\theta})\)</span> and plug
them into <a class="reference internal" href="#equation-eq-elbo-vi">(11.110)</a>.</p>
<p>The performance of VI depends on many factors. One of them being the
family of surrogate distributions we choose from. For example, a more
expressive surrogate distribution helps capture more complex, nonlinear
dependencies among components of the target posterior distribution, and
thus usually gives better result (see <a class="reference internal" href="#fig-vi-in-tfp"><span class="std std-numref">Fig. 11.33</span></a>).
Automatically choosing a good surrogate family distribution and
efficiently optimizing it is currently an active research area. Code
Block <a class="reference internal" href="#vi-in-tfp"><span class="std std-ref">vi_in_tfp</span></a> shows a simple example of
using VI in TFP, with two different types of surrogate posterior
distributions. The result is shown in <a class="reference internal" href="#fig-vi-in-tfp"><span class="std std-numref">Fig. 11.33</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="vi-in-tfp">
<div class="code-block-caption"><span class="caption-number">Listing 11.12 </span><span class="caption-text">vi_in_tfp</span><a class="headerlink" href="#vi-in-tfp" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tfpe</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">experimental</span>
<span class="c1"># An arbitrary density function as target</span>
<span class="n">target_logprob</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="o">-</span><span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="mf">1.5</span><span class="o">*</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Set up two different surrogate posterior distribution</span>
<span class="n">event_shape</span> <span class="o">=</span> <span class="p">[(),</span> <span class="p">()]</span>  <span class="c1"># theta is 2 scalar</span>
<span class="n">mean_field_surrogate_posterior</span> <span class="o">=</span> <span class="n">tfpe</span><span class="o">.</span><span class="n">vi</span><span class="o">.</span><span class="n">build_affine_surrogate_posterior</span><span class="p">(</span>
    <span class="n">event_shape</span><span class="o">=</span><span class="n">event_shape</span><span class="p">,</span> <span class="n">operators</span><span class="o">=</span><span class="s2">&quot;diag&quot;</span><span class="p">)</span>
<span class="n">full_rank_surrogate_posterior</span> <span class="o">=</span> <span class="n">tfpe</span><span class="o">.</span><span class="n">vi</span><span class="o">.</span><span class="n">build_affine_surrogate_posterior</span><span class="p">(</span>
    <span class="n">event_shape</span><span class="o">=</span><span class="n">event_shape</span><span class="p">,</span> <span class="n">operators</span><span class="o">=</span><span class="s2">&quot;tril&quot;</span><span class="p">)</span>

<span class="c1"># Optimization</span>
<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">posterior_samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">approx</span> <span class="ow">in</span> <span class="p">[</span><span class="n">mean_field_surrogate_posterior</span><span class="p">,</span> <span class="n">full_rank_surrogate_posterior</span><span class="p">]:</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">vi</span><span class="o">.</span><span class="n">fit_surrogate_posterior</span><span class="p">(</span>
        <span class="n">target_logprob</span><span class="p">,</span> <span class="n">approx</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span>
        <span class="n">sample_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="c1"># approx is a tfp distribution, we can sample from it after training</span>
    <span class="n">posterior_samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">approx</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10000</span><span class="p">))</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-vi-in-tfp">
<a class="reference internal image-reference" href="../_images/vi_in_tfp.png"><img alt="../_images/vi_in_tfp.png" src="../_images/vi_in_tfp.png" style="width: 8.00in;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11.33 </span><span class="caption-text">Using variational inference to approximate a target density function.
The target density is a 2D banana shaped function plotted using contour
lines. Two types of surrogate posterior distributions are used for the
approximation: on the left panel a mean-field Gaussian (one univariate
Gaussian for each dimension with trainable location and scale) and on
the right panel a full-rank Gaussian (a 2D multivariate Gaussian with
trainable mean and covariance matrix) <span id="id92">[<a class="reference internal" href="references.html#id163" title="Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic differentiation variational inference. The Journal of Machine Learning Research, 18(1):430–474, 2017.">164</a>]</span>.
Samples from the approximation after optimization are plotted as dots
overlay on top of the true density. Comparing the two, you can see that
while both approximations does not fully capture the shape of the target
density, full-rank Gaussian is a better approximation thanks to its more
complex structure.</span><a class="headerlink" href="#fig-vi-in-tfp" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="programming-references">
<span id="programming-ref"></span><h2><span class="section-number">11.10. </span>Programming References<a class="headerlink" href="#programming-references" title="Permalink to this headline">¶</a></h2>
<p>Part of computational Bayes is well, the computer and the software tools
now available. Using these tools help modern Bayesian practitioner share
models, reduce mistakes, and speed up the model building and inference
process. To have the computer work for us we need to program it but
often this is easier said than done. To use them effectively still
requires thought and understanding. In this last section we will provide
some high level guidance for the major concepts.</p>
<section id="which-programming-language">
<span id="id93"></span><h3><span class="section-number">11.10.1. </span>Which Programming Language?<a class="headerlink" href="#which-programming-language" title="Permalink to this headline">¶</a></h3>
<p>There are many programming languages. We primarily used Python, but
other popular languages such as Julia, R, C/C++ exist with specialized
application for Bayesian computation as well. So which programming
language should you use? There is no universal right or wrong answer
here. Instead you should always consider the complete ecosystem. In this
book we use Python because packages like ArviZ, Matplotlib, and Pandas
to make data processing and displaying easy. This is not exclusive to
Python. For a Bayesian specifically consider the PPLs available in that
particular language, because if none exists then you might want to
reconsider your programming language of choice. Also consider the
community you want to work with and what language they use. One of the
authors of this book lives in Southern California, so knowing English,
and a bit of Spanish makes a lot of sense, because with those two he
could communicate in common situations. It is the same with programming
languages, if your future lab group uses R then learning R is a good
idea.</p>
<p>Computational purists may exclaim that some languages are faster than
others computationally. This of course is true but we advise not getting
too wrapped up in the discussion of “which is the fastest ppl”. In real
life scenarios different models take different amount of time to run.
Additionally there is “human time” which is the time for one to iterate
and come up with a model, and “model run time” which is the time it
takes for the computer to return a useful result. These are not the same
and in different situations one is more important than the other. This
is all to say, do not worry too much about picking the “right” up front
language, if you learn one effectively the concepts will transfer to
another.</p>
</section>
<section id="version-control">
<span id="id94"></span><h3><span class="section-number">11.10.2. </span>Version Control<a class="headerlink" href="#version-control" title="Permalink to this headline">¶</a></h3>
<p>Version control is not necessary but is absolutely recommended and will
incur great benefits if utilized. When working alone, version control
lets you iterate through model designs without worry about losing your
code or making changes or experiment that breaks your model. This by
itself lets you iterate quicker and with more confidence, as well as the
ability to switch back and forth between different model definitions.
When working with others, version control enables collaboration and code
sharing, that would be challenging or impossible to perform without the
snapshotting or comparison features that version control systems allows.
There are many different version control systems (Mercurial, SVN,
Perforce) but git is currently the most popular. Version control is not
typically tied to a particular programming language.</p>
</section>
<section id="dependency-management-and-package-repositories">
<span id="id95"></span><h3><span class="section-number">11.10.3. </span>Dependency Management and Package Repositories<a class="headerlink" href="#dependency-management-and-package-repositories" title="Permalink to this headline">¶</a></h3>
<p>Nearly all code relies on other code to run (it is turtles all the way
down). PPLs in particular rely on a lot of different libraries to run.
We strongly suggest familiarizing yourself with a requirements
management tool that helps you see, list, and freeze the packages your
analysis relies on. Additionally package repositories are where these
requirement packages are fetched from. These are typically specific to
the language, for example, one requirements management tool in Python is
pip, and a popular cloud repository is pypi. In Scala sbt is one tool
that helps with dependencies and Maven is a popular package repository.
All mature languages will have this tooling, but you must make the
conscious choice to use them.</p>
</section>
<section id="environment-management">
<span id="id96"></span><h3><span class="section-number">11.10.4. </span>Environment Management<a class="headerlink" href="#environment-management" title="Permalink to this headline">¶</a></h3>
<p>All code executes in an environment. Most people forget this until their
code suddenly stops working, or does not work on another computer.
Environment management is the set of tools used to create a reproducible
computation environment. This is of particular importance to Bayesian
Modelers who deal with enough randomness in their models, and do not
want the computer to add an additional layer of variability.
Unfortunately Environment Management is also one of the most confusing
portions of programming. In general there are two rough types of
environment control, language specific and language agnostic. In Python
virtualenv is a python specific environment manager, whereas
containerization and virtualization are language agnostic. We have no
specific suggestion here as the choice depends largely on your comfort
with these tool, and also where you plan to run the code. We do
absolutely recommend making a deliberate choice here though, as it makes
sure you have reproducible results.</p>
</section>
<section id="text-editor-vs-integrated-development-environment-vs-notebook">
<span id="dev-environment"></span><h3><span class="section-number">11.10.5. </span>Text Editor vs Integrated Development Environment vs Notebook<a class="headerlink" href="#text-editor-vs-integrated-development-environment-vs-notebook" title="Permalink to this headline">¶</a></h3>
<p>When writing code, you must well, write it somewhere. For data minded
folks there are typically three interfaces for this.</p>
<p>The first and simplest is a text editor. The most basic text editors
allow you to, surprise, edit text and save it. With these editors you
can write a python program, save it, and then run it. Typically text
editors are very “light weight” and do not include much extra
functionality besides basic things like find and replace. Think of text
editors like a bicycle. They are simple, their interface is basically a
handlebar and some pedals, and they will get you from here to there but
its mostly on you to do the work.</p>
<p>Integrated Development Environments (IDE) by comparison the modern
airplanes. They have an insane amount of functionality, a lot buttons,
and a lot of automation. IDEs let you edit text at their core, but as
the name suggests they integrate many other aspects of development as
well. For example, functionality for running code, unit-testing, linting
code, version control, code version comparison, and much much more. IDEs
are typically most useful when writing a lot of complicated code that
spans many modules.</p>
<p>While we would love to provide a simple definition of a text editor vs
IDE, the line these days is very blurry. Our suggestion is start more on
the text editor side and move to IDEs once familiar with how code works.
Otherwise it will be hard for you to tell what the IDE is doing for you
“under the hood”.</p>
<p>Notebooks are an entirely different interface. Notebooks are special in
they mix code, output, and documentation, as well as allowing nonlinear
code execution. For this book the majority of the code, and figures, are
presented in Jupyter Notebook files. We also provide links to Google
Colab which a cloud notebook environment. Notebooks are typically best
used for exploratory data analysis, and explanatory type situations,
such as this book. They are not so well suited for running production
code.</p>
<p>Our suggestion for notebooks is similar to IDEs. If new to statistical
computing start with a text editor first. Once you have a strong handle
of how to run code from individual files, then move to notebook
environments, either a cloud hosted Google colab, or Binder instance, or
a local Jupyter Notebook</p>
</section>
<section id="the-specific-tools-used-for-this-book">
<span id="id97"></span><h3><span class="section-number">11.10.6. </span>The Specific Tools Used for this Book<a class="headerlink" href="#the-specific-tools-used-for-this-book" title="Permalink to this headline">¶</a></h3>
<p>Here is what we used for this book. This does not mean these are the
only tools you can use, these are just the ones we used.</p>
<ul class="simple">
<li><p><strong>Programming Language</strong>: Python</p></li>
<li><p><strong>Probabilistic Programming Languages</strong>: PyMC3, TensorFlow
Probability. Stan and Numpyro are displayed briefly as well.</p></li>
<li><p><strong>Version control</strong>: git</p></li>
<li><p><strong>Dependency Management</strong>: pip and conda</p></li>
<li><p><strong>Package repository</strong>: pypi, conda-forge</p></li>
<li><p><strong>Environment Management</strong>: conda</p></li>
<li><p><strong>General Documentation</strong>: LaTeX (for book writing), Markdown (for
code package), Jupyter Notebooks</p></li>
</ul>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id98"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>Most of the territory of what we now call Spain and Portugal was
part of Al-Andalus and Arabic state, this had a tremendous influence
in the Spanish/Portuguese culture, including food, music, language
and also in the genetic makeup.</p>
</dd>
<dt class="label" id="id99"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>For those who are interested in delving further into the subject,
we recommend reading the book Introduction to Probability by Joseph
K. Blitzstein and Jessica Hwang <span id="id100">[<a class="reference internal" href="references.html#id10" title="J.K. Blitzstein and J. Hwang. Introduction to Probability, Second Edition. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2019. ISBN 9780429766732.">5</a>]</span>.</p>
</dd>
<dt class="label" id="id101"><span class="brackets"><a class="fn-backref" href="#id5">3</a></span></dt>
<dd><p>From this definition John K. Kruschke wonderfully states that
Bayesian inference is reallocation of credibility (probability)
across possibilities <span id="id102">[<a class="reference internal" href="references.html#id93" title="J.K. Kruschke. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press. Academic Press, 2015. ISBN 9780124058880.">165</a>]</span>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets"><a class="fn-backref" href="#id9">4</a></span></dt>
<dd><p>If we need to locate the circumference relative to other objects
in the plane, we would also need the coordinates of the center, but
let us omit that detail for now.</p>
</dd>
<dt class="label" id="id104"><span class="brackets"><a class="fn-backref" href="#id11">5</a></span></dt>
<dd><p>Increase or remain constant but never decrease.</p>
</dd>
<dt class="label" id="id105"><span class="brackets"><a class="fn-backref" href="#id12">6</a></span></dt>
<dd><p>Loosely speaking, a right-continuous function has no jump when the
limit point is approached from the right.</p>
</dd>
<dt class="label" id="id106"><span class="brackets"><a class="fn-backref" href="#id16">7</a></span></dt>
<dd><p>The result of one outcome does not affect the others.</p>
</dd>
<dt class="label" id="id107"><span class="brackets"><a class="fn-backref" href="#id18">8</a></span></dt>
<dd><p>Or more precisely if we take the limit of the Bin<span class="math notranslate nohighlight">\((n, p)\)</span>
distribution as <span class="math notranslate nohighlight">\(n \to \infty\)</span> and <span class="math notranslate nohighlight">\(p \to 0\)</span> with <span class="math notranslate nohighlight">\(np\)</span> fixed we get
a Poisson distribution.</p>
</dd>
<dt class="label" id="id108"><span class="brackets"><a class="fn-backref" href="#id19">9</a></span></dt>
<dd><p>A proper discussion that avoids non-sensical statements would
require a discussion of measure theory. But we will side-step this
requirement.</p>
</dd>
<dt class="label" id="id109"><span class="brackets"><a class="fn-backref" href="#id20">10</a></span></dt>
<dd><p>You can use check this statement yourself with the help of SciPy.</p>
</dd>
<dt class="label" id="id110"><span class="brackets"><a class="fn-backref" href="#id24">11</a></span></dt>
<dd><p>Not only on planet Earth, but even on other planets judging by
the Gaussian-shaped UFOs we have observed (just kidding, this is of
course a joke, just as ufology).</p>
</dd>
<dt class="label" id="id111"><span class="brackets"><a class="fn-backref" href="#id25">12</a></span></dt>
<dd><p>This distribution was discovered by William Gosset while trying
to improve the methods of quality control in a brewery. Employees of
that company were allow to publish scientific papers as long as they
did not use the word beer, the company name, and their own surname.
Thus Gosset publish under the name Student.</p>
</dd>
<dt class="label" id="id112"><span class="brackets"><a class="fn-backref" href="#id26">13</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Gamma_function">https://en.wikipedia.org/wiki/Gamma_function</a></p>
</dd>
<dt class="label" id="id113"><span class="brackets"><a class="fn-backref" href="#id27">14</a></span></dt>
<dd><p><span class="math notranslate nohighlight">\(\nu\)</span> can take values below 1.</p>
</dd>
<dt class="label" id="id114"><span class="brackets"><a class="fn-backref" href="#id37">15</a></span></dt>
<dd><p>See, for example, <a class="reference external" href="https://www.youtube.com/watch?v=i5oND7rHtFs">https://www.youtube.com/watch?v=i5oND7rHtFs</a></p>
</dd>
<dt class="label" id="id115"><span class="brackets"><a class="fn-backref" href="#id38">16</a></span></dt>
<dd><p>For those familiar with eigenvectors and eigenvalues this should
ring a bell.</p>
</dd>
<dt class="label" id="id116"><span class="brackets"><a class="fn-backref" href="#id39">17</a></span></dt>
<dd><p>Another analogy comes from politics, when politicians/government
changes but pressing issues like inequality or climate change are
not properly addressed.</p>
</dd>
<dt class="label" id="id117"><span class="brackets"><a class="fn-backref" href="#id41">18</a></span></dt>
<dd><p>To be precise we should include the molecules of glass and the
molecules in the air, and… but let just focus on the water.</p>
</dd>
<dt class="label" id="id118"><span class="brackets"><a class="fn-backref" href="#id42">19</a></span></dt>
<dd><p>Do not let that Heisenberg guy and his uncertainty principle
spoil the party</p>
</dd>
<dt class="label" id="id119"><span class="brackets"><a class="fn-backref" href="#id46">20</a></span></dt>
<dd><p>Generally pronounced as W-A-I-C, even when something like
wæ[i]{.smallcaps}k is less of a mouthful</p>
</dd>
<dt class="label" id="id120"><span class="brackets"><a class="fn-backref" href="#id63">21</a></span></dt>
<dd><p>We do not like these rules of thumb, but you can check, for
example, here
<a class="reference external" href="https://en.wikipedia.org/wiki/Bayes_factor#Interpretation">https://en.wikipedia.org/wiki/Bayes_factor#Interpretation</a></p>
</dd>
<dt class="label" id="id121"><span class="brackets"><a class="fn-backref" href="#id65">22</a></span></dt>
<dd><p>In practice it is very common to actually compute the marginal
likelihood in log-scale for computational stability. In such a case
a Bayes factor becomes a difference of two log marginal likelihoods</p>
</dd>
<dt class="label" id="id122"><span class="brackets"><a class="fn-backref" href="#id67">23</a></span></dt>
<dd><p>The names derived from a famous casino with that name in the
Principality of Monaco.</p>
</dd>
<dt class="label" id="id123"><span class="brackets"><a class="fn-backref" href="#id68">24</a></span></dt>
<dd><p>This video shows a closely related example in a very calm and
clear way <a class="reference external" href="https://www.youtube.com/watch?v=zwAD6dRSVyI">https://www.youtube.com/watch?v=zwAD6dRSVyI</a></p>
</dd>
<dt class="label" id="id124"><span class="brackets"><a class="fn-backref" href="#id78">25</a></span></dt>
<dd><p>The name Hybrid Monte Carlo is also used because is was
originally conceived as a hybrid method combining molecular
mechanics, a widely-used simulation technique for molecular systems,
and Metropolis-Hastings.</p>
</dd>
<dt class="label" id="id125"><span class="brackets"><a class="fn-backref" href="#id80">26</a></span></dt>
<dd><p>Code copied from our good friend Colin Carroll’s blogpost on HMC
<a class="reference external" href="https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/">https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/</a></p>
</dd>
<dt class="label" id="id126"><span class="brackets"><a class="fn-backref" href="#id82">27</a></span></dt>
<dd><p>This value is in the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>, and by default this value
is 0.8. See Section <a class="reference internal" href="chp_02.html#divergences"><span class="std std-ref">Divergences</span></a>.</p>
</dd>
<dt class="label" id="id127"><span class="brackets"><a class="fn-backref" href="#id86">28</a></span></dt>
<dd><p>We mean true purely from a mathematical point of view, without
any reference to how adequate is such posterior to any particular
practical problem.</p>
</dd>
<dt class="label" id="id128"><span class="brackets"><a class="fn-backref" href="#id87">29</a></span></dt>
<dd><p>See <a class="reference internal" href="#entropy"><span class="std std-ref">Entropy</span></a> for more details on this analogy with physical
system.</p>
</dd>
<dt class="label" id="id129"><span class="brackets"><a class="fn-backref" href="#id88">30</a></span></dt>
<dd><p>These terms are borrowed from metallurgy in particular describing
specific processes where alloyed metal is heated and cooled to
obtain a particular molecular structure.</p>
</dd>
<dt class="label" id="id130"><span class="brackets"><a class="fn-backref" href="#id89">31</a></span></dt>
<dd><p>This effective sample size is computed from the importance
weights which is different from the ESS we have been computing to
diagnosing MCMC samplers, that is computed from the autocorrelation
of the samples.</p>
</dd>
</dl>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_10.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">10. </span>Probabilistic Programming Languages</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="glossary.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Glossary</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      Por Martin, Kumar, Lao<br/>
    
        &copy; Derechos de autor 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>