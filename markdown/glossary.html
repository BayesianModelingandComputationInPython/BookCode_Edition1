
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>12. Glossary &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13. References" href="references.html" />
    <link rel="prev" title="11. Appendiceal Topics" href="chp_11.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="references.html">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/glossary.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="glossary">
<span id="id1"></span><h1><span class="section-number">12. </span>Glossary<a class="headerlink" href="#glossary" title="Permalink to this headline">¶</a></h1>
<p><strong>Autocorrelation</strong>: Autocorrelation is the correlation of a signal
with a lagged copy of itself. Conceptually, you can think of it as how
similar observations are as a function of the time lag between them.
Large autocorrelation is a concern in MCMC samples as it reduces the
effective sample size.</p>
<p><strong>Aleatoric Uncertainty</strong>: Aleatoric uncertainty is related to the
notion that there are some quantities that affect a measurement or
observation that are intrinsically unknowable or random. For example,
even if we were able to exactly replicate condition such as direction,
altitude and force when shooting an arrow with a bow. The arrow will
still not hit the same point, because there are other conditions that we
do not control like fluctuations of the atmosphere or vibrations of the
arrow shaft, that are random.</p>
<p><strong>Bayesian Inference</strong>: Bayesian Inference is a particular form of
statistical inference based on combining probability distributions in
order to obtain other probability distributions. In other words is the
formulation and computation of conditional probability or probability
densities,
<span class="math notranslate nohighlight">\(p(\boldsymbol{\theta} \mid \boldsymbol{Y}) \propto p(\boldsymbol{Y} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta})\)</span>.</p>
<p><strong>Bayesian workflow</strong>: Designing a good enough model for a given problem
requires significant statistical and domain knowledge expertise. Such
design is typically carried out through an iterative process called
Bayesian workflow. This process includes the three steps of model
building <span id="id2">[<a class="reference internal" href="references.html#id78">17</a>]</span>: inference, model checking/improvement, and model
comparison. In this context the purpose of model comparison is not
necessarily restricted to pick the <em>best</em> model, but more importantly to
better understand the models.</p>
<p><strong>Causal inference</strong>: or Observational causal inference. The procedures
and tools used to estimate the impact of a treatment (or intervention)
in some system without testing the intervention. That is from
observational data instead of experimental data.</p>
<p><strong>Covariance Matrix and Precision Matrix</strong>: The covariance matrix is a
square matrix that contains the covariance between each pair of elements
of a collection of random variable. The diagonal of the covariance
matrix is the variance of the random variable. The precision matrix is
the matrix inverse of the covariance matrix.</p>
<p><strong>Design Matrix</strong>: In the context of regression analysis a design matrix
is a matrix of values of the explanatory variables. Each row represents
an individual object, with the successive columns corresponding to the
variables and their specific values for that observation. It can contain
indicator variables (ones and zeros) indicating group membership, or it
can contain continuous values.</p>
<p><strong>Decision tree</strong>: A decision tree is a flowchart-like structure in
which each internal node represents a “test” on an attribute (e.g.
whether a coin flip comes up heads or tails), each branch represents the
outcome of the test, and each leaf node represents a class label
(decision taken after computing all attributes). The paths from root to
leaf represent classification rules. The values at the leaf nodes can be
continuous if the tree is used for regression.</p>
<p><strong>dse</strong>: The standard error of component-wise differences of <code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code>
between two models. This error is smaller than the standard error (<code class="docutils literal notranslate"><span class="pre">se</span></code>
in <code class="docutils literal notranslate"><span class="pre">az.compare</span></code>) for individual models. The reason being that generally
some observations are as easy/hard to predict for all models and thus
this introduce correlations.</p>
<p><strong>d_loo</strong>: The difference in <code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code> for two models. If more than
two models are compared, the difference is computed relative to the
model with highest <code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code>).</p>
<p><strong>Epistimic Uncertainty</strong>: Epistemic uncertainty is related to the lack
of knowledge of the states of a system by some observer. It is related
to the knowledge that we could have in principle but not in practice and
not about the intrinsic unknowable quantity of nature (contrast with
aleatory uncertainty). For example, we may be uncertain of the weight of
an item because we do not have an scale at hand, so we estimate the
weight by lifting it, or we may have one scale but with a precision
limited to the kilogram. We could also have epistemic uncertainty if we
design an experiment or perform a computation ignoring factors. For
example, to estimate how much time we will have to drive to another
city, we may omit the time spent at tolls, or we may assume excellent
weather or road conditions etc. In other words, epistemic uncertainty is
about ignorance and in opposition to aleatoric, uncertainty, we can in
principle reduce it by obtaining more information.</p>
<p><strong>Statistic</strong>: A statistic (not plural) or sample statistic is any
quantity computed from a sample. Sample statistics are computed for
several reasons including estimating a population (or data generating
process) parameter, describing a sample, or evaluating a hypothesis. The
sample mean (also known as empirical mean) is a statistic, the sample
variance (or empirical variance) is another example. When a statistic is
used to estimate a population (or data generating process) parameter,
the statistic is called an estimator. Thus, the sample mean can be an
estimator and the posterior mean can be another estimator.</p>
<p><strong>ELPD</strong>: Expected Log-pointwise Predictive Density (or expected log
pointwise predictive probabilities for discrete model). This quantity is
generally estimated by cross-validation or using methods such as WAIC
(<code class="docutils literal notranslate"><span class="pre">elpd_waic</span></code>) or LOO (<code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code>). As probability densities can be
smaller or larger than 1, the ELPD can be negative or positive for
continuous variables and non-negative for discrete variables.</p>
<p><strong>Exchangeability</strong>: A sequence of Random variables is exchangeable if
their joint probability distribution does not change when the positions
in the sequence is altered. Exchangeable random variables are not
necessarily iid, but iid are exchangeable.</p>
<p><strong>Exploratory Analysis of Bayesian Models</strong>: The collection of tasks
necessary to perform a successful Bayesian data analysis that are not
the inference itself. This includes. Diagnosing the quality of the
inference results obtained using numerical methods. Model criticism,
including evaluations of both model assumptions and model predictions.
Comparison of models, including model selection or model averaging.
Preparation of the results for a particular audience.</p>
<p><strong>Hamiltonian Monte Carlo</strong> Hamiltonian Monte Carlo (HMC) is a Markov
chain Monte Carlo (MCMC) method that uses the gradient to efficiently
explore a probability distribution function. In Bayesian statistics this
is most commonly used to obtain samples from the posterior distribution.
HMC methods are instances of the Metropolis–Hastings algorithm, where
the proposed new points are computed from a Hamiltonian, this allows the
methods to proposed new states to be far from the current one with high
acceptance probability. The evolution of the system is simulated using a
time-reversible and volume-preserving numerical integrator (most
commonly the leapfrog integrator). The efficiency of the HMC method is
highly dependant on certain hyperparameters of the method. Thus, the
most useful methods in Bayesian statistics are adaptive dynamics
versions of HMC that can adjust those hyperparameters automatically
during the warm-up or tuning phase.</p>
<p><strong>Heteroscedasticity</strong>: A sequence of random variables is
heteroscedastic if its random variables do not have the same variance,
i.e. if they are not homoscedastic. This is also known as heterogeneity
of variance.</p>
<p><strong>Homoscedasticity</strong>: A sequence of random variables is homoscedastic if
all its random variables have the same finite variance. This is also
known as homogeneity of variance. The complementary notion is called
heteroscedasticity.</p>
<p><strong>iid</strong>: Independent and identically distributed. A collection of random
variables is independent and identically distributed if each random
variable has the same probability distribution as the others and all are
mutually independent. If a collection of random variables is iid it is
also exchangeable, but the converse is not necessarily true.</p>
<p><strong>Individual Conditional Expectation</strong> ICE: An ICE shows the dependence
between the response variable and a covariate of interest. This is done
for each sample separately with one line per sample. This contrast to
PDPs where the average effect of the covariate is represented.</p>
<p><strong>Inference</strong>: Colloquially, inference is reaching a conclusion based on
evidence and reasoning. In this book refer to inference we generally
mean about Bayesian Inference, which has a more restricted and precise
definition. Bayesian Inference is the process of conditioning models to
the available data and obtaining posterior distributions. Thus, in order
to reach a conclusion based on evidence and reasoning, we need to
perform more steps that mere Bayesian inference. Hence the importance of
discussing Bayesian analysis in terms of exploratory analysis of
Bayesian models or more generally in term of Bayesian workflows.</p>
<p><strong>Imputation</strong>: Replacing missing data values through a method of
choice. Common methods may include most common occurrence or
interpolation based on other (present) observed data.</p>
<p><strong>KDE</strong>: Kernel Density Estimation. A non-parametric method to estimate
the probability density function of a random variable from a finite set
of samples. We often use the term KDE to talk about the estimated
density and not the method.</p>
<p><strong>LOO</strong>: Short for Pareto smoothed importance sampling leave one out
cross-validation (PSIS-LOO-CV). In the literature “LOO” may be
restricted to leave one out cross-validation.</p>
<p><strong>Maximum a Posteriori (MAP)</strong> An estimator of an unknown quantity, that
equals the mode of the posterior distribution. The MAP estimator
requires optimization of the posterior, unlike the posterior mean which
requires integration. If the priors are flat, or in the limit of
infinite sample size, the MAP estimator is equivalent to the Maximum
Likelihood estimator.</p>
<p><strong>Odds</strong> A measure of the likelihood of a particular outcome. They are
calculated as the ratio of the number of events that produce that
outcome to the number that do not. Odds are commonly used in gambling.</p>
<p><strong>Overfitting</strong>: A model overfits when produces predictions too closely
to the dataset used for fitting the model failing to fit new datasets.
In terms of the number of parameters an overfitted model contains more
parameters than can be justified by the data. An arbitrary
over-complex model will fit not only the data but also the noise,
leading to poor predictions.</p>
<p><strong>Partial Dependence Plots</strong> PDP: A PDP shows the dependence between the
response variable and a set of covariates of interest, this is done by
marginalizing over the values of all other covariates. Intuitively, we
can interpret the partial dependence as the expected value of the
response variable as function of the covariates of interest.</p>
<p><strong>Pareto k estimates</strong> <span class="math notranslate nohighlight">\(\hat k\)</span>: A diagnostic for Pareto smoothed
importance sampling (PSIS), which is used by LOO. The Pareto k
diagnostic estimates how far an individual leave-one-out observation is
from the full distribution. If leaving out an observation changes the
posterior too much then importance sampling is not able to give reliable
estimates. If <span class="math notranslate nohighlight">\(\hat \kappa &lt; 0.5\)</span>, then the corresponding component of
<code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code> is estimated with high accuracy. If <span class="math notranslate nohighlight">\(0.5&lt; \hat \kappa &lt;0.7\)</span>
the accuracy is lower, but still useful in practice. If
<span class="math notranslate nohighlight">\(\hat \kappa &gt; 0.7\)</span>, then importance sampling is not able to provide a
useful estimate for that observation. The <span class="math notranslate nohighlight">\(\hat \kappa\)</span> values are also
useful as a measure of influence of an observation. Highly influential
observations have high <span class="math notranslate nohighlight">\(\hat \kappa\)</span> values. Very high <span class="math notranslate nohighlight">\(\hat \kappa\)</span>
values often indicate model misspecification, outliers, or mistakes in
the data processing.</p>
<p><strong>Point estimate</strong> A single value, generally but not necessarily in
parameter space, used as a summary of <em>best estimate</em> of an unknown
quantity. A point estimate can be contrasted with an interval estimate
like highest density intervals, which provides a range or interval of
values describing the unknown quantity. We can also contrast a point
estimate with distributional estimates, like the posterior distribution
or its marginals.</p>
<p><strong>p_loo</strong>: The difference between <code class="docutils literal notranslate"><span class="pre">elpd_loo</span></code>: and the
non-cross-validated log posterior predictive density. It describes how
much more difficult it is to predict future data than the observed data.
Asymptotically under certain regularity conditions, <code class="docutils literal notranslate"><span class="pre">p_loo</span></code> can be
interpreted as the effective number of parameters. In well behaving
cases <code class="docutils literal notranslate"><span class="pre">p_loo</span></code> should be lower than the number of parameters in the model
and smaller than the number observations in the data. If not, this is an
indication that the model has very weak predictive capability and may
thus indicate a severe model misspecification. See high Pareto k
diagnostic values.</p>
<p><strong>Probabilistic Programming Language</strong>: A programming syntax composed of
primitives that allows one to define Bayesian models and perform
inference automatically. Typically a Probabilistic Programming Language
also includes functionality to generate prior or posterior predictive
samples or even to analysis result from inference.</p>
<p><strong>Prior predictive distribution</strong>: The expected distribution of the data
according to the model (prior and likelihood). That is, the data the
model is expecting to see before seeing any data. See Equation
<a class="reference internal" href="chp_01.html#equation-eq-prior-pred-dist">(1.7)</a>. The prior predictive
distribution can be used for prior elicitation, as it is generally
easier to think in terms of the observed data, than to think in terms of
model parameters.</p>
<p><strong>Posterior predictive distribution</strong>: This is the distribution of
(future) data according to the posterior, which in turn is a consequence
of the model (prior and likelihood) and observed data. In other words,
these are the model’s predictions. See Equation
<a class="reference internal" href="chp_01.html#equation-eq-post-pred-dist">(1.8)</a>. Besides generating
predictions, the posterior predictive distribution can be used to asses
the model fit, by comparing it with the observed data.</p>
<p><strong>Residuals</strong>: The difference between an observed value and the
estimated value of the quantity of interest. If a model assumes that the
variance is finite and the same for all residuals, we say we have
homoscedasticity. If instead the variance can change, we say we have
heteroscedasticity.</p>
<p><strong>Sufficient statistics</strong>: A statistic is sufficient with respect to a
model parameter if no other statistic computed from the same sample
provides any additional information about that sample. In other words,
that statistic is <em>sufficient</em> to summarize your samples without losing
information. For example, given a sample of independent values from a
normal distribution with expected value <span class="math notranslate nohighlight">\(\mu\)</span> and known finite variance
the sample mean is sufficient statistics for <span class="math notranslate nohighlight">\(\mu\)</span>. Notice that the mean
says nothing about the dispersion, thus it is only sufficient with
respect to the parameter <span class="math notranslate nohighlight">\(\mu\)</span>. It is known that for iid data the only
distributions with a sufficient statistic with dimension equal to the
dimension of <span class="math notranslate nohighlight">\(\theta\)</span> are the distributions from the exponential family.
For other distribution, the dimension of the sufficient statistic
increases with the sample size.</p>
<p><strong>Synthetic data</strong>: Also known as fake data it refers to data generated
from a model instead of being gathered from experimentation or
observation. Samples from the posterior/prior predictive distributions
are examples of synthetic data.</p>
<p><strong>Timestamp</strong>: A timestamp is an encoded information to identify when a
certain event happens. Usually a timestamp is written in the format of
date and time of day, with more accurate fraction of a second when
necessary.</p>
<p><strong>Turing-complete</strong> In colloquial usage, is used to mean that any
real-world general-purpose computer or computer language can
approximately simulate the computational aspects of any other real-world
general-purpose computer or computer language.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="chp_11.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">11. </span>Appendiceal Topics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="references.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>References</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Martin, Kumar, Lao<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>