
<!DOCTYPE html>

<html data-content_root="" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/><meta content="Docutils 0.18.1: http://docutils.sourceforge.net/" name="generator"/>
<title>6. Time Series — Bayesian Modeling and Computation in Python</title>
<script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
<!-- Loaded before other Sphinx assets -->
<link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet"/>
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet"/>
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet"/>
<link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" rel="preload" type="font/woff2"/>
<link href="../_static/pygments.css" rel="stylesheet" type="text/css"/>
<link href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" rel="stylesheet" type="text/css"/>
<link href="../_static/togglebutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/copybutton.css" rel="stylesheet" type="text/css"/>
<link href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../_static/sphinx-codeautolink.css" rel="stylesheet" type="text/css"/>
<link href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" rel="preload"/>
<link as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" rel="preload"/>
<script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
<script src="../_static/doctools.js"></script>
<script src="../_static/clipboard.min.js"></script>
<script src="../_static/copybutton.js"></script>
<script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
<script>let toggleHintShow = 'Click to show';</script>
<script>let toggleHintHide = 'Click to hide';</script>
<script>let toggleOpenOnPrint = 'true';</script>
<script src="../_static/togglebutton.js"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
<script src="../_static/design-tabs.js"></script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../_static/sphinx-thebe.js"></script>
<script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
<script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>DOCUMENTATION_OPTIONS.pagename = 'markdown/chp_06';</script>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="chp_07.html" rel="next" title="7. Bayesian Additive Regression Trees"/>
<link href="chp_05.html" rel="prev" title="5. Splines"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="en" name="docsearch:language"/>
</head>
<body data-bs-root-margin="0px 0px -60%" data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-default-mode="" data-offset="180">
<a class="skip-link" href="#main-content">Skip to main content</a>
<div id="pst-scroll-pixel-helper"></div>
<button class="btn rounded-pill" id="pst-back-to-top" type="button">
<i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>
<input class="sidebar-toggle" id="__primary" name="__primary" type="checkbox"/>
<label class="overlay overlay-primary" for="__primary"></label>
<input class="sidebar-toggle" id="__secondary" name="__secondary" type="checkbox"/>
<label class="overlay overlay-secondary" for="__secondary"></label>
<div class="search-button__wrapper">
<div class="search-button__overlay"></div>
<div class="search-button__search-container">
<form action="../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="fa-solid fa-magnifying-glass"></i>
<input aria-label="Search this book..." autocapitalize="off" autocomplete="off" autocorrect="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." spellcheck="false" type="search"/>
<span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
</div>
<nav class="bd-header navbar navbar-expand-lg bd-navbar">
</nav>
<div class="bd-container">
<div class="bd-container__inner bd-page-width">
<div class="bd-sidebar-primary bd-sidebar">
<div class="sidebar-header-items sidebar-primary__section">
</div>
<div class="sidebar-primary-items__start sidebar-primary__section">
<div class="sidebar-primary-item">
<a class="navbar-brand logo" href="../welcome.html">
<p class="title logo__title">Bayesian Modeling and Computation in Python</p>
</a></div>
<div class="sidebar-primary-item"><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item navbar-nav active">
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 0</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dedication.html">Dedication</a></li>
<li class="toctree-l1"><a class="reference internal" href="foreword.html">Foreword</a></li>
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="symbollist.html">Symbols</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chp_01.html">1. Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_02.html">2. Exploratory Analysis of Bayesian Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_03.html">3. Linear Models and Probabilistic Programming Languages</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_04.html">4. Extending Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_05.html">5. Splines</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Time Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_07.html">7. Bayesian Additive Regression Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_08.html">8. Approximate Bayesian Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_09.html">9. End to End Bayesian Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_10.html">10. Probabilistic Programming Languages</a></li>
<li class="toctree-l1"><a class="reference internal" href="chp_11.html">11. Appendiceal Topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">12. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">13. References</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_01.html">Code 1: Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_02.html">Code 2: Exploratory Analysis of Bayesian Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_03.html">Code 3: Linear Models and Probabilistic Programming Languages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_04.html">Code 4: Extending Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_05.html">Code 5: Splines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_06.html">Code 6: Time Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_07.html">Code 7: Bayesian Additive Regression Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_08.html">Code 8: Approximate Bayesian Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_09.html">Code 9: End to End Bayesian Workflows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_10.html">Code 10: Probabilistic Programming Languages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/chp_11.html">Code 11: Appendiceal Topics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../solutions/chp_01.html">Solutions 1: Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../solutions/chp_02.html">Solutions 2: Exploratory Analysis of Bayesian models</a></li>
</ul>
</div>
</nav></div>
</div>
<div class="sidebar-primary-items__end sidebar-primary__section">
</div>
<div id="rtd-footer-container"></div>
</div>
<main class="bd-main" id="main-content">
<div class="sbt-scroll-pixel-helper"></div>
<div class="bd-content">
<div class="bd-article-container">
<div class="bd-header-article">
<div class="header-article-items header-article__inner">
<div class="header-article-items__start">
<div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__primary" title="Toggle primary sidebar">
<span class="fa-solid fa-bars"></span>
</label></div>
</div>
<div class="header-article-items__end">
<div class="header-article-item">
<div class="article-header-buttons">
<div class="dropdown dropdown-source-buttons">
<button aria-expanded="false" aria-label="Source repositories" class="btn dropdown-toggle" data-bs-toggle="dropdown" type="button">
<i class="fab fa-github"></i>
</button>
<ul class="dropdown-menu">
<li><a class="btn btn-sm btn-source-repository-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1" target="_blank" title="Source repository">
<span class="btn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="btn__text-container">Repository</span>
</a>
</li>
<li><a class="btn btn-sm btn-source-issues-button dropdown-item" data-bs-placement="left" data-bs-toggle="tooltip" href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/chp_06.html&amp;body=Your%20issue%20content%20here." target="_blank" title="Open an issue">
<span class="btn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
</ul>
</div>
<button class="btn btn-sm btn-fullscreen-button" data-bs-placement="bottom" data-bs-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="btn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" data-bs-placement="bottom" data-bs-toggle="tooltip" for="__secondary" title="Toggle secondary sidebar">
<span class="fa-solid fa-list"></span>
</label>
</div></div>
</div>
</div>
</div>
<div class="onlyprint" id="jb-print-docs-body">
<h1>Time Series</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-overview-of-time-series-problems">6.1. An Overview of Time Series Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-analysis-as-a-regression-problem">6.2. Time Series Analysis as a Regression Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#design-matrices-for-time-series">6.2.1. Design Matrices for Time Series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-functions-and-generalized-additive-model">6.2.2. Basis Functions and Generalized Additive Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-models">6.3. Autoregressive Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-ar-process-and-smoothing">6.3.1. Latent AR Process and Smoothing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#s-ar-i-ma-x">6.3.2. (S)AR(I)MA(X)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space-models">6.4. State Space Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-gaussian-state-space-models-and-kalman-filter">6.4.1. Linear Gaussian State Space Models and Kalman filter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arima-expressed-as-a-state-space-model">6.4.2. ARIMA, Expressed as a State Space Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-structural-time-series">6.4.3. Bayesian Structural Time Series</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-time-series-models">6.5. Other Time Series Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-criticism-and-choosing-priors">6.6. Model Criticism and Choosing Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#priors-for-time-series-models">6.6.1. Priors for Time Series Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">6.7. Exercises</a></li>
</ul>
</nav>
</div>
</div>
</div>
<div id="searchbox"></div>
<article class="bd-article" role="main">
<section class="tex2jax_ignore mathjax_ignore" id="time-series">
<span id="chap4"></span><h1><span class="section-number">6. </span>Time Series<a class="headerlink" href="#time-series" title="Permalink to this heading">#</a></h1>
<p>“It is difficult to make predictions, especially about the future”.
This is true when dutch politician Karl Kristian Steincke allegedly said
this sometime in the 1940s <a class="footnote-reference brackets" href="#id52" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, and it is still true today especially
if you are working on time series and forecasting problems. There are
many applications of time series analysis, from making predictions with
forecasting, to understanding what were the underlying latent factors in
the historical trend. In this chapter we will discuss some Bayesian
approaches to this problem. We will start by considering time series
modeling as a regression problem, with the design matrices parsed from
the timestamp information. We will then explore the approaches to model
temporal correlation using autoregressive components. These models
extend into a wider (more general) class of State Space Model and
Bayesian Structural Time Series model (BSTS), and we will introduce a
specialized inference method in the linear Gaussian cases: Kalman
Filter. The remainder of the chapter will give a brief summary of model
comparison as well as considerations to be made when choosing prior
distributions for time series models.</p>
<section id="an-overview-of-time-series-problems">
<span id="id2"></span><h2><span class="section-number">6.1. </span>An Overview of Time Series Problems<a class="headerlink" href="#an-overview-of-time-series-problems" title="Permalink to this heading">#</a></h2>
<p>In many real life applications we observe data sequentially in time,
generating timestamps along the way each time we make an observation. In
addition to the observation itself, the timestamp information can be
quite informative when:</p>
<ul class="simple">
<li><p>There is a temporal <strong>trend</strong>, for example, regional population,
global GDP, annual CO₂ emissions in the US. Usually this is an
overall pattern which we intuitively label as “growth” or
“decline”.</p></li>
<li><p>There is some recurrent pattern correlated to time, called
<strong>seasonality</strong> <a class="footnote-reference brackets" href="#id53" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. For example, changes in monthly temperature
(higher in the summer and lower in the winter), monthly rainfall
amounts (in many regions of the world this is lower in winter and
higher during summer), daily coffee consumption in a given office
building (higher in the weekdays and lower in the weekends), hourly
number of bike rentals(higher in the day than during the night),
like we saw in Chapter <a class="reference internal" href="chp_05.html#chap3-5"><span class="std std-ref">5</span></a>.</p></li>
<li><p>The current data point informs the next data point in some way. In
other words where noise or <strong>residuals</strong> are correlated <a class="footnote-reference brackets" href="#id54" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>. For
example, the daily number of cases resolved at a help desk, stock
price, hourly temperature, hourly rainfall amounts.</p></li>
</ul>
<p>It is thus quite natural and useful to consider the decomposition of a
time series into:</p>
<div class="math notranslate nohighlight" id="equation-eq-generic-time-series">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-eq-generic-time-series" title="Permalink to this equation">#</a></span>\[    y_t = \text{Trend}_t + \text{Seasonality}_t + \text{Residuals}_t\]</div>
<p>Most of the classical time series models are based on this
decomposition. In this chapter, we will discuss modeling approaches on
time series that display some level of temporal trend and seasonality,
and explore methods to capture these regular patterns, as well as the
less-regular patterns (e.g., residuals correlated in time).</p>
</section>
<section id="time-series-analysis-as-a-regression-problem">
<span id="id5"></span><h2><span class="section-number">6.2. </span>Time Series Analysis as a Regression Problem<a class="headerlink" href="#time-series-analysis-as-a-regression-problem" title="Permalink to this heading">#</a></h2>
<p>We will start with modeling a time series with a linear regression model
on a widely used demo data set that appears in many tutorials (e.g.,
PyMC3, TensorFlow Probability) and it was used as an example in the
Gaussian Processes for Machine Learning book by
<span id="id6">Rasmussen and Williams [<a class="reference internal" href="references.html#id91" title="Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, Cambridge, Mass, 2005. ISBN 978-0-262-18253-9.">52</a>]</span>. Atmospheric CO₂ measurements have been taken
regularly at the Mauna Loa observatory in Hawaii since the late 1950s at
hourly intervals. In many examples the observations are aggregated into
monthly average as shown in <a class="reference internal" href="#fig-fig1-co2-by-month"><span class="std std-numref">Fig. 6.1</span></a>. We load the
data into Python with Code Block
<a class="reference internal" href="#load-co2-data"><span class="std std-ref">load_co2_data</span></a>, and also split the data
set into training and testing set. We will fit the model using the
training set only, and evaluate the forecast against the testing set.</p>
<figure class="align-default" id="fig-fig1-co2-by-month">
<a class="reference internal image-reference" href="../_images/fig1_co2_by_month.png"><img alt="../_images/fig1_co2_by_month.png" src="../_images/fig1_co2_by_month.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.1 </span><span class="caption-text">Monthly CO₂ measurements in Mauna Loa from 1966 January to 2019
February, split into training (shown in black) and testing (shown in
blue) set. We can see a strong upward trend and seasonality pattern in
the data.</span><a class="headerlink" href="#fig-fig1-co2-by-month" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="literal-block-wrapper docutils container" id="load-co2-data">
<div class="code-block-caption"><span class="caption-number">Listing 6.1 </span><span class="caption-text">load_co2_data</span><a class="headerlink" href="#load-co2-data" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">co2_by_month</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"../data/monthly_mauna_loa_co2.csv"</span><span class="p">)</span>
<span class="n">co2_by_month</span><span class="p">[</span><span class="s2">"date_month"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">co2_by_month</span><span class="p">[</span><span class="s2">"date_month"</span><span class="p">])</span>
<span class="n">co2_by_month</span><span class="p">[</span><span class="s2">"CO2"</span><span class="p">]</span> <span class="o">=</span> <span class="n">co2_by_month</span><span class="p">[</span><span class="s2">"CO2"</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
<span class="n">co2_by_month</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">"date_month"</span><span class="p">,</span> <span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">num_forecast_steps</span> <span class="o">=</span> <span class="mi">12</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># Forecast the final ten years, given previous data</span>
<span class="n">co2_by_month_training_data</span> <span class="o">=</span> <span class="n">co2_by_month</span><span class="p">[:</span><span class="o">-</span><span class="n">num_forecast_steps</span><span class="p">]</span>
<span class="n">co2_by_month_testing_data</span> <span class="o">=</span> <span class="n">co2_by_month</span><span class="p">[</span><span class="o">-</span><span class="n">num_forecast_steps</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<p>Here we have a vector of observations of monthly atmospheric CO₂
concentrations <span class="math notranslate nohighlight">\(y_t\)</span> with <span class="math notranslate nohighlight">\(t = [0, \dots, 636]\)</span>; each element associated
with a timestamp. The month of the year could be nicely parsed into a
vector of <span class="math notranslate nohighlight">\([1, 2, 3,\dots, 12, 1, 2,\dots]\)</span>. Recall that for linear
regression we can state the likelihood as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-regression-model">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-eq-regression-model" title="Permalink to this equation">#</a></span>\[    Y \sim \mathcal{N}(\mathbf{X} \beta, \sigma)\]</div>
<p>Considering the seasonality effect, we can use the month of the year
predictor directly to index a vector of regression coefficient. Here
using Code Block
<a class="reference internal" href="#generate-design-matrix"><span class="std std-ref">generate_design_matrix</span></a>, we dummy
code the predictor into a design matrix with <code class="docutils literal notranslate"><span class="pre">shape</span> <span class="pre">=</span> <span class="pre">(637,</span> <span class="pre">12)</span></code>. Adding
a linear predictor to the design matrix to capture the upward increasing
trend we see in the data, we get the design matrix for the time series.
You can see a subset of the design matrix in
<a class="reference internal" href="#fig-fig2-sparse-design-matrix"><span class="std std-numref">Fig. 6.2</span></a>.</p>
<figure class="align-default" id="fig-fig2-sparse-design-matrix">
<a class="reference internal image-reference" href="../_images/fig2_sparse_design_matrix.png"><img alt="../_images/fig2_sparse_design_matrix.png" src="../_images/fig2_sparse_design_matrix.png" style="width: 5.2in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.2 </span><span class="caption-text">Design matrix with a linear component and month of the year component
for a simple regression model for time series. The design matrix is
transposed into <span class="math notranslate nohighlight">\(feature * timestamps\)</span> so it is easier to visualize. In
the figure, the first row (index 0) contains continuous values between 0
and 1 representing the time and the linear growth. The rest of the rows
(index 1 - 12) are dummy coding of month information. The color coding
goes from black for 1 to light gray for 0.</span><a class="headerlink" href="#fig-fig2-sparse-design-matrix" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="literal-block-wrapper docutils container" id="generate-design-matrix">
<div class="code-block-caption"><span class="caption-number">Listing 6.2 </span><span class="caption-text">generate_design_matrix</span><a class="headerlink" href="#generate-design-matrix" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">trend_all</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">co2_by_month</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">trend_all</span> <span class="o">=</span> <span class="n">trend_all</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
<span class="n">trend</span> <span class="o">=</span> <span class="n">trend_all</span><span class="p">[:</span><span class="o">-</span><span class="n">num_forecast_steps</span><span class="p">,</span> <span class="p">:]</span>

<span class="n">seasonality_all</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span>
   <span class="n">co2_by_month</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">month</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
<span class="n">seasonality</span> <span class="o">=</span> <span class="n">seasonality_all</span><span class="p">[:</span><span class="o">-</span><span class="n">num_forecast_steps</span><span class="p">,</span> <span class="p">:]</span>

<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">X_subset</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html#numpy.concatenate" title="numpy.concatenate"><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span></a><span class="p">([</span><span class="n">trend</span><span class="p">,</span> <span class="n">seasonality</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">50</span><span class="p">:]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_subset</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition-parsing-timestamps-to-a-design-matrix admonition">
<p class="admonition-title">Parsing timestamps to a design matrix</p>
<p>Treatment of timestamps could be tedious and error prone, especially when time zone is involved. Typical
cyclical information we could parse from timestamp are, in order of
resolution:</p>
<ul class="simple">
<li><p>Second of the hour (1, 2, …, 60)</p></li>
<li><p>Hour of the day (1, 2, …, 24)</p></li>
<li><p>Day of the week (Monday, Tuesday, …, Sunday)</p></li>
<li><p>Day of the month (1, 2, …, 31)</p></li>
<li><p>Holiday effect (New year’s day, Easter holiday, International
Workers’ Day, Christmas day, etc)</p></li>
<li><p>Month of the year (1, 2, …, 12)</p></li>
</ul>
<p>All of which could be parsed into a design matrix with dummy coding.
Effects like day of the week and day of the month usually are closely
related to human activities. For example, passenger numbers of public
transportation usually show a strong week day effect; consumer spending
might be higher after a payday, which is usually around the end of the
month. In this chapter we mostly consider timestamps recorded at regular
intervals.</p>
</div>
<p>We can now write down our first time series model as a regression
problem, using <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code>, using the same
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> API and TFP Bayesian modeling methods
we introduced in Chapter <a class="reference internal" href="chp_03.html#chap2"><span class="std std-ref">3</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="regression-model-for-timeseries">
<div class="code-block-caption"><span class="caption-number">Listing 6.3 </span><span class="caption-text">regression_model_for_timeseries</span><a class="headerlink" href="#regression-model-for-timeseries" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tfd</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">distributions</span>
<span class="n">root</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span><span class="o">.</span><span class="n">Root</span>

<span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">ts_regression_model</span><span class="p">():</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">100.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"intercept"</span><span class="p">))</span>
    <span class="n">trend_coeff</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"trend_coeff"</span><span class="p">))</span>
    <span class="n">seasonality_coeff</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span>
                   <span class="n">sample_shape</span><span class="o">=</span><span class="n">seasonality</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                   <span class="n">name</span><span class="o">=</span><span class="s2">"seasonality_coeff"</span><span class="p">))</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"noise_sigma"</span><span class="p">))</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="p">(</span><span class="n">intercept</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...-&gt;...i"</span><span class="p">,</span> <span class="n">trend</span><span class="p">,</span> <span class="n">trend_coeff</span><span class="p">)</span> <span class="o">+</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">seasonality</span><span class="p">,</span> <span class="n">seasonality_coeff</span><span class="p">))</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">noise</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
        <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">"observed"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>As we mentioned in earlier chapters, TFP offers a lower level API
compared to PyMC3. While it is more flexible to interact with low level
modules and component (e.g., customized composable inference
approaches), we usually end up with a bit more boilerplate code, and
additional shape handling in the model using <code class="docutils literal notranslate"><span class="pre">tfp</span></code> compared to other
PPLs. For example, in Code Block
<a class="reference internal" href="#regression-model-for-timeseries"><span class="std std-ref">regression_model_for_timeseries</span></a>
we use <code class="docutils literal notranslate"><span class="pre">einsum</span></code> instead of <code class="docutils literal notranslate"><span class="pre">matmul</span></code> with Python Ellipsis so it can
handle arbitrary <em>batch shape</em> (see Section <a class="reference internal" href="chp_10.html#shape-ppl"><span class="std std-ref">Shape Handling in PPLs</span></a>)
for more details).</p>
<p>Running the Code Block
<a class="reference internal" href="#regression-model-for-timeseries"><span class="std std-ref">regression_model_for_timeseries</span></a>
gives us a regression model <code class="docutils literal notranslate"><span class="pre">ts_regression_model</span></code>. It has similar
functionality to <code class="docutils literal notranslate"><span class="pre">tfd.Distribution</span></code> which we can utilize in our Bayesian
workflow. To draw prior and prior predictive samples, we can call the
<code class="docutils literal notranslate"><span class="pre">.sample(.)</span></code> method (see Code Block
<a class="reference internal" href="#prior-predictive"><span class="std std-ref">prior_predictive</span></a>, with the result
shown in <a class="reference internal" href="#fig-fig3-prior-predictive1"><span class="std std-numref">Fig. 6.3</span></a>).</p>
<div class="literal-block-wrapper docutils container" id="prior-predictive">
<div class="code-block-caption"><span class="caption-number">Listing 6.4 </span><span class="caption-text">prior_predictive</span><a class="headerlink" href="#prior-predictive" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Draw 100 prior and prior predictive samples</span>
<span class="n">prior_samples</span> <span class="o">=</span> <span class="n">ts_regression_model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>  
<span class="n">prior_predictive_timeseries</span> <span class="o">=</span> <span class="n">prior_samples</span><span class="o">.</span><span class="n">observed</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">co2_by_month</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="o">-</span><span class="n">num_forecast_steps</span><span class="p">],</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">prior_predictive_timeseries</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"Year"</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">autofmt_xdate</span><span class="p">()</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig3-prior-predictive1">
<a class="reference internal image-reference" href="../_images/fig3_prior_predictive1.png"><img alt="../_images/fig3_prior_predictive1.png" src="../_images/fig3_prior_predictive1.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.3 </span><span class="caption-text">Prior predictive samples from a simple regression model for modeling the
Monthly CO₂ measurements in Mauna Loa time series. Each line plot is
one simulated time series. Since we use an uninformative prior the prior
prediction has a pretty wide range.</span><a class="headerlink" href="#fig-fig3-prior-predictive1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We can now run inference of the regression model and format the result
into an <code class="docutils literal notranslate"><span class="pre">az.InferenceData</span></code> object in Code Block
<a class="reference internal" href="#inference-of-regression-model"><span class="std std-ref">inference_of_regression_model</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="inference-of-regression-model">
<div class="code-block-caption"><span class="caption-number">Listing 6.5 </span><span class="caption-text">inference_of_regression_model</span><a class="headerlink" href="#inference-of-regression-model" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">run_mcmc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
    <span class="n">tfp</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">mcmc</span><span class="o">.</span><span class="n">windowed_adaptive_nuts</span><span class="p">,</span>
    <span class="n">autograph</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">jit_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">mcmc_samples</span><span class="p">,</span> <span class="n">sampler_stats</span> <span class="o">=</span> <span class="n">run_mcmc</span><span class="p">(</span>
    <span class="mi">1000</span><span class="p">,</span> <span class="n">ts_regression_model</span><span class="p">,</span> <span class="n">n_chains</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_adaptation_steps</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">observed</span><span class="o">=</span><span class="n">co2_by_month_training_data</span><span class="p">[</span><span class="s2">"CO2"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>

<span class="n">regression_idata</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://python.arviz.org/en/stable/api/generated/arviz.from_dict.html#arviz.from_dict" title="arviz.from_dict"><span class="n">az</span><span class="o">.</span><span class="n">from_dict</span></a><span class="p">(</span>
    <span class="n">posterior</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># TFP mcmc returns (num_samples, num_chains, ...), we swap</span>
        <span class="c1"># the first and second axis below for each RV so the shape</span>
        <span class="c1"># is what ArviZ expects.</span>
        <span class="n">k</span><span class="p">:</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.swapaxes.html#numpy.swapaxes" title="numpy.swapaxes"><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span></a><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span>
    <span class="n">sample_stats</span><span class="o">=</span><span class="p">{</span>
        <span class="n">k</span><span class="p">:</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.swapaxes.html#numpy.swapaxes" title="numpy.swapaxes"><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span></a><span class="p">(</span><span class="n">sampler_stats</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">"target_log_prob"</span><span class="p">,</span> <span class="s2">"diverging"</span><span class="p">,</span> <span class="s2">"accept_ratio"</span><span class="p">,</span> <span class="s2">"n_steps"</span><span class="p">]})</span>
</pre></div>
</div>
</div>
<p>To draw posterior predictive samples conditioned on the inference
result, we can use the <code class="docutils literal notranslate"><span class="pre">.sample_distributions</span></code> method and condition on
the posterior samples. In this case, since we would like to also plot
the posterior predictive sample for the trend and seasonality components
in the time series, while conditioning on both the training and testing
data set. To visualize the forecasting ability of the model we construct
the posterior predictive distributions in Code Block
<a class="reference internal" href="#posterior-predictive-with-component"><span class="std std-ref">posterior_predictive_with_component</span></a>,
with the result displayed in
<a class="reference internal" href="#fig-fig4-posterior-predictive-components1"><span class="std std-numref">Fig. 6.4</span></a> for the trend and
seasonality components and in <a class="reference internal" href="#fig-fig5-posterior-predictive1"><span class="std std-numref">Fig. 6.5</span></a>
for the overall model fit and forecast.</p>
<div class="literal-block-wrapper docutils container" id="posterior-predictive-with-component">
<div class="code-block-caption"><span class="caption-number">Listing 6.6 </span><span class="caption-text">posterior_predictive_with_component</span><a class="headerlink" href="#posterior-predictive-with-component" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We can draw posterior predictive sample with jd.sample_distributions()</span>
<span class="c1"># But since we want to also plot the posterior predictive distribution for </span>
<span class="c1"># each components, conditioned on both training and testing data, we</span>
<span class="c1"># construct the posterior predictive distribution as below:</span>
<span class="n">nchains</span> <span class="o">=</span> <span class="n">regression_idata</span><span class="o">.</span><span class="n">posterior</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="s2">"chain"</span><span class="p">]</span>

<span class="n">trend_posterior</span> <span class="o">=</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">intercept</span> <span class="o">+</span> \
    <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...-&gt;i..."</span><span class="p">,</span> <span class="n">trend_all</span><span class="p">,</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">trend_coeff</span><span class="p">)</span>
<span class="n">seasonality_posterior</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
    <span class="s2">"ij,...j-&gt;i..."</span><span class="p">,</span> <span class="n">seasonality_all</span><span class="p">,</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">seasonality_coeff</span><span class="p">)</span>

<span class="n">y_hat</span> <span class="o">=</span> <span class="n">trend_posterior</span> <span class="o">+</span> <span class="n">seasonality_posterior</span>
<span class="n">posterior_predictive_dist</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">mcmc_samples</span><span class="o">.</span><span class="n">noise_sigma</span><span class="p">)</span>
<span class="n">posterior_predictive_samples</span> <span class="o">=</span> <span class="n">posterior_predictive_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig4-posterior-predictive-components1">
<a class="reference internal image-reference" href="../_images/fig4_posterior_predictive_components1.png"><img alt="../_images/fig4_posterior_predictive_components1.png" src="../_images/fig4_posterior_predictive_components1.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.4 </span><span class="caption-text">Posterior predictive samples of the trend component and seasonality
component of a regression model for time series.</span><a class="headerlink" href="#fig-fig4-posterior-predictive-components1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="fig-fig5-posterior-predictive1">
<a class="reference internal image-reference" href="../_images/fig5_posterior_predictive1.png"><img alt="../_images/fig5_posterior_predictive1.png" src="../_images/fig5_posterior_predictive1.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.5 </span><span class="caption-text">Posterior predictive samples from a simple regression model for time
series in gray, with the actual data plotted in black and blue. While
the overall fit is reasonable for the training set (plotted in black),
the forecast (out of sample prediction) is poor as the underlying trend
is accelerating more than linearly.</span><a class="headerlink" href="#fig-fig5-posterior-predictive1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Looking at the out of sample prediction in
<a class="reference internal" href="#fig-fig5-posterior-predictive1"><span class="std std-numref">Fig. 6.5</span></a>, we notice that:</p>
<ol class="arabic simple">
<li><p>The linear trend does not perform well when we forecast further into
the future and gives forecast consistently lower than the actual
observed. Specifically the atmospheric CO₂ does not increase
linearly with a constant slope over the years <a class="footnote-reference brackets" href="#id55" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p></li>
<li><p>The range of uncertainty is almost constant (sometimes also referred
to as the forecast cone), where intuitively we expect the
uncertainty to increase when we forecast farther into the future.</p></li>
</ol>
<section id="design-matrices-for-time-series">
<span id="id8"></span><h3><span class="section-number">6.2.1. </span>Design Matrices for Time Series<a class="headerlink" href="#design-matrices-for-time-series" title="Permalink to this heading">#</a></h3>
<p>In the regression model above, a rather simplistic design matrix was
used. We can get a better model to capture our knowledge of the observed
time series by adding additional information to our design matrix.</p>
<p>Generally, a better trend component is the most important aspect for
improving forecast performance: seasonality components are <em>usually</em>
stationary <a class="footnote-reference brackets" href="#id56" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> with easy to estimate parameters. Restated, there is a
repeated pattern that forms a kind of a repeated measure. Thus most time
series modeling involves designing a latent process that realistically
captures the non-stationarity in the trend.</p>
<p>One approach that has been quite successful is using a local linear
process for the trend component. Basically, it is a smooth trend that is
linear within some range, with an intercept and coefficient that
changes, or drifts, slowly over the observed time span. A prime example
of such an application is Facebook Prophet <a class="footnote-reference brackets" href="#id57" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>, where a semi-smooth
step linear function is used to model the trend <span id="id11">[<a class="reference internal" href="references.html#id125" title="Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45, 2018.">53</a>]</span>. By
allowing the slope to change at some specific breakpoints, we can
generate a trend line that could capture the long-term trend much better
than a straight line. This is similar to the idea of indicator functions
we discussed in Section <a class="reference internal" href="chp_05.html#expanding-feature-space"><span class="std std-ref">Expanding the Feature Space</span></a>. In a time series
context we specify this idea mathematically in Equation
<a class="reference internal" href="#equation-eq-step-linear-function">(6.3)</a></p>
<div class="math notranslate nohighlight" id="equation-eq-step-linear-function">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-eq-step-linear-function" title="Permalink to this equation">#</a></span>\[    g(t) = (k + \mathbf{A}\delta) t + (m + \mathbf{A} \gamma)\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the (global) growth rate, <span class="math notranslate nohighlight">\(\delta\)</span> is a vector of rate
adjustments at each change point, <span class="math notranslate nohighlight">\(m\)</span> is the (global) intercept.
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a matrix with <code class="docutils literal notranslate"><span class="pre">shape=(n_t,</span> <span class="pre">n_s)</span></code> with <span class="math notranslate nohighlight">\(n_s\)</span> being the
number of change points. At time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> accumulates the drift
effect <span class="math notranslate nohighlight">\(\delta\)</span> of the slope. <span class="math notranslate nohighlight">\(\gamma\)</span> is set to <span class="math notranslate nohighlight">\(-s_j \times \delta_j\)</span>
(where <span class="math notranslate nohighlight">\(s_j\)</span> is the time location of the <span class="math notranslate nohighlight">\(n_s\)</span> change points) to make
the trend line continuous. A regularized prior, like <span class="math notranslate nohighlight">\(\text{Laplace}\)</span>,
is usually chosen for <span class="math notranslate nohighlight">\(\delta\)</span> to express that we don’t expect to see
sudden or large change in the slope. You can see in Code Block
<a class="reference internal" href="#step-linear-function-for-trend"><span class="std std-ref">step_linear_function_for_trend</span></a>
for an example of a randomly generated step linear function and its
breakdown in <a class="reference internal" href="#fig-fig6-step-linear-function"><span class="std std-numref">Fig. 6.6</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="step-linear-function-for-trend">
<div class="code-block-caption"><span class="caption-number">Listing 6.7 </span><span class="caption-text">step_linear_function_for_trend</span><a class="headerlink" href="#step-linear-function-for-trend" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_changepoints</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">n_tp</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">t</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_tp</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_changepoints</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">s</span><span class="p">)</span>

<span class="n">k</span><span class="p">,</span> <span class="n">m</span> <span class="o">=</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">40</span>
<span class="n">delta</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.laplace.html#numpy.random.laplace" title="numpy.random.laplace"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">laplace</span></a><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n_changepoints</span><span class="p">)</span>
<span class="n">growth</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">+</span> <span class="n">A</span> <span class="o">@</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span>
<span class="n">offset</span> <span class="o">=</span> <span class="n">m</span> <span class="o">+</span> <span class="n">A</span> <span class="o">@</span> <span class="p">(</span><span class="o">-</span><span class="n">s</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
<span class="n">trend</span> <span class="o">=</span> <span class="n">growth</span> <span class="o">+</span> <span class="n">offset</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig6-step-linear-function">
<a class="reference internal image-reference" href="../_images/fig6_step_linear_function.png"><img alt="../_images/fig6_step_linear_function.png" src="../_images/fig6_step_linear_function.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.6 </span><span class="caption-text">A step linear function as trend component for a time series model,
generated with Code Block
<a class="reference internal" href="#step-linear-function-for-trend"><span class="std std-ref">step_linear_function_for_trend</span></a>.
The first panel is the design matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, with the same color
coding that black for 1 and light gray for 0. The last panel is the
resulting function <span class="math notranslate nohighlight">\(g(t)\)</span> in Equation <a class="reference internal" href="#equation-eq-step-linear-function">(6.3)</a> that
we could use as trend in a time series model. The two middle panels are
the breakdown of the two components in Equation
<a class="reference internal" href="#equation-eq-step-linear-function">(6.3)</a>. Note how combining the two makes the
resulting trend continuous.</span><a class="headerlink" href="#fig-fig6-step-linear-function" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In practice, we usually specify a priori how many change points there
are so <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> can be generated statically. One common approach is
to specify more change points than you believe the time series actually
displays, and place a more sparse prior on <span class="math notranslate nohighlight">\(\delta\)</span> to regulate the
posterior towards 0. Automatic change point detection is also possible
<span id="id12">[<a class="reference internal" href="references.html#id126" title="Ryan Prescott Adams and David JC MacKay. Bayesian online changepoint detection. arXiv preprint arXiv:0710.3742, 2007.">54</a>]</span>.</p>
</section>
<section id="basis-functions-and-generalized-additive-model">
<span id="chp4-gam"></span><h3><span class="section-number">6.2.2. </span>Basis Functions and Generalized Additive Model<a class="headerlink" href="#basis-functions-and-generalized-additive-model" title="Permalink to this heading">#</a></h3>
<p>In our regression model defined in Code Block
<a class="reference internal" href="#regression-model-for-timeseries"><span class="std std-ref">regression_model_for_timeseries</span></a>,
we model the seasonality component with a sparse, index, matrix. An
alternative is to use basis functions like B-spline (see Chapter
<a class="reference internal" href="chp_05.html#chap3-5"><span class="std std-ref">5</span></a>), or Fourier basis function as in the Facebook
Prophet model. Basis function as a design matrix might provide some nice
properties like orthogonality (see Box <strong>Mathematical properties of
design matrix</strong>), which makes numerically solving the linear equation
more stable <span id="id13">[<a class="reference internal" href="references.html#id160" title="G. Strang. Introduction to Linear Algebra. Wellesley-Cambridge Press, 2009. ISBN 9780980232714.">55</a>]</span>.</p>
<p>Fourier basis functions are a collection of sine and cosine functions
that can be used for approximating arbitrary smooth seasonal effects
<span id="id14">[<a class="reference internal" href="references.html#id127" title="Andrew C Harvey and Neil Shephard. Structural time series models. Handbook of Statistics,(edited by GS Maddala, CR Rao and HD Vinod), 11:261–302, 1993.">56</a>]</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-fourier-basis-functions">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-eq-fourier-basis-functions" title="Permalink to this equation">#</a></span>\[    s(t) = \sum^N_{n=1} \left[a_n \text{cos}\left(\frac{2 \pi nt}{P} \right) + b_n \text{sin}\left(\frac{2 \pi nt}{P}\right) \right]\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is the regular period the time series has (e.g. <span class="math notranslate nohighlight">\(P = 365.25\)</span>
for yearly data or <span class="math notranslate nohighlight">\(P = 7\)</span> for weekly data, when the time variable is
scaled in days). We can generate them statically with formulation as
shown in Code Block
<a class="reference internal" href="#fourier-basis-as-seasonality"><span class="std std-ref">fourier_basis_as_seasonality</span></a>,
and visualize it in <a class="reference internal" href="#fig-fig7-fourier-basis"><span class="std std-numref">Fig. 6.7</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="fourier-basis-as-seasonality">
<div class="code-block-caption"><span class="caption-number">Listing 6.8 </span><span class="caption-text">fourier_basis_as_seasonality</span><a class="headerlink" href="#fourier-basis-as-seasonality" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span> <span class="nf">gen_fourier_basis</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">365.25</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/constants.html#numpy.pi" title="numpy.pi"><span class="n">np</span><span class="o">.</span><span class="n">pi</span></a> <span class="o">*</span> <span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">t</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">/</span> <span class="n">p</span>
    <span class="k">return</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html#numpy.concatenate" title="numpy.concatenate"><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span></a><span class="p">((</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.cos.html#numpy.cos" title="numpy.cos"><span class="n">np</span><span class="o">.</span><span class="n">cos</span></a><span class="p">(</span><span class="n">x</span><span class="p">),</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.sin.html#numpy.sin" title="numpy.sin"><span class="n">np</span><span class="o">.</span><span class="n">sin</span></a><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">n_tp</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">t_monthly</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.asarray.html#numpy.asarray" title="numpy.asarray"><span class="n">np</span><span class="o">.</span><span class="n">asarray</span></a><span class="p">([</span><span class="n">i</span> <span class="o">%</span> <span class="n">p</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_tp</span><span class="p">)])</span>
<span class="n">monthly_X</span> <span class="o">=</span> <span class="n">gen_fourier_basis</span><span class="p">(</span><span class="n">t_monthly</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig7-fourier-basis">
<a class="reference internal image-reference" href="../_images/fig7_fourier_basis.png"><img alt="../_images/fig7_fourier_basis.png" src="../_images/fig7_fourier_basis.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.7 </span><span class="caption-text">Fourier basis function with n=3. There are in total 6 predictors, where
we highlighted the first one by setting the rest semi-transparent.</span><a class="headerlink" href="#fig-fig7-fourier-basis" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Fitting the seasonality using a design matrix generated from Fourier
basis function as above requires estimating 2N parameters
<span class="math notranslate nohighlight">\(\beta = [a_1, b_1, \dots , a_N , b_N]\)</span>.</p>
<p>Regression models like Facebook Prophet are also referred to as a (GAM),
as their response variable <span class="math notranslate nohighlight">\(Y_t\)</span> depends linearly on unknown smooth
basis functions <a class="footnote-reference brackets" href="#id58" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>. We also discussed other GAMs previously in Chapter
<a class="reference internal" href="chp_05.html#chap3-5"><span class="std std-ref">5</span></a>.</p>
<div class="admonition-mathematical-properties-of-design-matrix admonition">
<p class="admonition-title">Mathematical properties of design matrix</p>
<p>Mathematical properties of design matrices are studied quite extensively in the linear least
squares problem setting, where we want to solve
<span class="math notranslate nohighlight">\(min \mid Y - \mathbf{X} \beta \mid ^{2}\)</span> for <span class="math notranslate nohighlight">\(\beta\)</span>. We can often get
a sense how stable the solution of <span class="math notranslate nohighlight">\(\beta\)</span> will be, or even possible to
get a solution at all, by inspecting the property of matrix
<span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span>. One such property is the condition number,
which is an indication of whether the solution of <span class="math notranslate nohighlight">\(\beta\)</span> may be prone
to large numerical errors. For example, if the design matrix contains
columns that are highly correlated (multicollinearity), the conditioned
number will be large and the matrix <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> is
ill-conditioned. Similar principle also applies in Bayesian modeling. An
in-depth exploratory data analysis in your analyses workflow is useful
no matter what formal modeling approach you are taking. Basis functions
as a design matrix usually are well-conditioned.</p>
</div>
<p>A Facebook Prophet-like GAM for the monthly CO₂ measurements is
expressed in Code Block <a class="reference internal" href="#gam"><span class="std std-ref">gam</span></a>. We assign weakly
informative prior to <code class="docutils literal notranslate"><span class="pre">k</span></code> and <code class="docutils literal notranslate"><span class="pre">m</span></code> to express our knowledge that monthly
measure is trending upward in general. This gives prior predictive
samples in a similar range of what is actually being observed (see
<a class="reference internal" href="#fig-fig8-prior-predictive2"><span class="std std-numref">Fig. 6.8</span></a>).</p>
<div class="literal-block-wrapper docutils container" id="gam">
<div class="code-block-caption"><span class="caption-number">Listing 6.9 </span><span class="caption-text">gam</span><a class="headerlink" href="#gam" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate trend design matrix</span>
<span class="n">n_changepoints</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">n_tp</span> <span class="o">=</span> <span class="n">seasonality_all</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">t</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_tp</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">n_changepoints</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)[</span><span class="mi">1</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">A</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.float32" title="numpy.float32"><span class="n">np</span><span class="o">.</span><span class="n">float32</span></a><span class="p">)</span>
<span class="c1"># Generate seasonality design matrix</span>
<span class="c1"># Set n=6 here so that there are 12 columns (same as `seasonality_all`)</span>
<span class="n">X_pred</span> <span class="o">=</span> <span class="n">gen_fourier_basis</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.where.html#numpy.where" title="numpy.where"><span class="n">np</span><span class="o">.</span><span class="n">where</span></a><span class="p">(</span><span class="n">seasonality_all</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">p</span><span class="o">=</span><span class="n">seasonality_all</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                           <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">n_pred</span> <span class="o">=</span> <span class="n">X_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">gam</span><span class="p">():</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">n_pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"beta"</span><span class="p">))</span>
    <span class="n">seasonality</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">X_pred</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="n">k</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"k"</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">co2_by_month_training_data</span><span class="p">[</span><span class="s2">"CO2"</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"m"</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"tau"</span><span class="p">))</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Laplace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">tau</span><span class="p">),</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">n_changepoints</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"delta"</span><span class="p">)</span>

    <span class="n">growth_rate</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="o">-</span><span class="n">s</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">trend</span> <span class="o">=</span> <span class="n">growth_rate</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">offset</span>

    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">seasonality</span> <span class="o">+</span> <span class="n">trend</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">co2_by_month_training_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

    <span class="n">noise_sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"noise_sigma"</span><span class="p">))</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
        <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s2">"observed"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig8-prior-predictive2">
<a class="reference internal image-reference" href="../_images/fig8_prior_predictive2.png"><img alt="../_images/fig8_prior_predictive2.png" src="../_images/fig8_prior_predictive2.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.8 </span><span class="caption-text">Prior predictive samples from a Facebook Prophet-like GAM with a weakly
informative prior on trend related parameters generated from Code Block
<a class="reference internal" href="#gam"><span class="std std-ref">gam</span></a>. Each line plot is one simulated time series.
The predictive samples are now in a similar range to what actually being
observed, particularly when comparing this figure to
<a class="reference internal" href="#fig-fig3-prior-predictive1"><span class="std std-numref">Fig. 6.3</span></a>.</span><a class="headerlink" href="#fig-fig8-prior-predictive2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>After inference, we can generate posterior predictive samples. As you
can see in <a class="reference internal" href="#fig-fig9-posterior-predictive2"><span class="std std-numref">Fig. 6.9</span></a>, the forecast
performance is better than the simple regression model in
<a class="reference internal" href="#fig-fig5-posterior-predictive1"><span class="std std-numref">Fig. 6.5</span></a>. Note that in <span id="id16">Taylor and Letham [<a class="reference internal" href="references.html#id125" title="Sean J Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72(1):37–45, 2018.">53</a>]</span>,
the generative process for forecast is not
identical to the generative model, as the step linear function is evenly
spaced with the change point predetermined. It is recommended that for
forecasting, at each time point we first determine whether that time
point would be a change point, with a probability proportional to the
number of predefined change points divided by the total number of
observations, and then generate a new delta from the posterior
distribution <span class="math notranslate nohighlight">\(\delta_{new} \sim \text{Laplace}(0, \tau)\)</span>. Here however,
to simplify the generative process we simply use the linear trend from
the last period.</p>
<figure class="align-default" id="fig-fig9-posterior-predictive2">
<a class="reference internal image-reference" href="../_images/fig9_posterior_predictive2.png"><img alt="../_images/fig9_posterior_predictive2.png" src="../_images/fig9_posterior_predictive2.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.9 </span><span class="caption-text">Posterior predictive samples from a Facebook Prophet-like from Code
Block <a class="reference internal" href="#gam"><span class="std std-ref">gam</span></a> in gray, with the actual data plotted in
black and blue.</span><a class="headerlink" href="#fig-fig9-posterior-predictive2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="autoregressive-models">
<span id="chap4-ar"></span><h2><span class="section-number">6.3. </span>Autoregressive Models<a class="headerlink" href="#autoregressive-models" title="Permalink to this heading">#</a></h2>
<p>One characteristic of time series is the sequential dependency of the
observations. This usually introduces structured errors that are
correlated temporally on previous observation(s) or error(s). A typical
example is autoregressive-ness. In an autoregressive model, the
distribution of output at time <span class="math notranslate nohighlight">\(t\)</span> is parameterized by a linear function
of previous observations. Consider a first-order autoregressive model
(usually we write that as AR(1) with a Gaussian likelihood:</p>
<div class="math notranslate nohighlight" id="equation-eq-ar1">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-eq-ar1" title="Permalink to this equation">#</a></span>\[    y_t \sim \mathcal{N}(\alpha + \rho y_{t-1}, \sigma)\]</div>
<p>The distribution of <span class="math notranslate nohighlight">\(y_t\)</span> follows a Normal distribution with the
location being a linear function of <span class="math notranslate nohighlight">\(y_{t-1}\)</span>. In Python, we can write
down such a model with a for loop that explicitly builds out the
autoregressive process. For example, in Code Block
<a class="reference internal" href="#ar1-with-forloop"><span class="std std-ref">ar1_with_forloop</span></a> we create an AR(1)
process using <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> with <span class="math notranslate nohighlight">\(\alpha = 0\)</span>, and
draw random samples from it by conditioned on <span class="math notranslate nohighlight">\(\sigma = 1\)</span> and different
values of <span class="math notranslate nohighlight">\(\rho\)</span>. The result is shown in
<a class="reference internal" href="#fig-fig10-ar1-process"><span class="std std-numref">Fig. 6.10</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="ar1-with-forloop">
<div class="code-block-caption"><span class="caption-number">Listing 6.10 </span><span class="caption-text">ar1_with_forloop</span><a class="headerlink" href="#ar1-with-forloop" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_t</span> <span class="o">=</span> <span class="mi">200</span>

<span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">ar1_with_forloop</span><span class="p">():</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">))</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_t</span><span class="p">):</span>
        <span class="n">x_i</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rho</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_i</span><span class="p">)</span>

<span class="n">nplot</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span><span class="n">nplot</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">rho</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html#numpy.linspace" title="numpy.linspace"><span class="n">np</span><span class="o">.</span><span class="n">linspace</span></a><span class="p">(</span><span class="o">-</span><span class="mf">1.01</span><span class="p">,</span> <span class="mf">1.01</span><span class="p">,</span> <span class="n">nplot</span><span class="p">)):</span>
    <span class="n">test_samples</span> <span class="o">=</span> <span class="n">ar1_with_forloop</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">rho</span><span class="p">))</span>
    <span class="n">ar1_samples</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">test_samples</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ar1_samples</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">"$\rho$=</span><span class="si">%.2f</span><span class="s2">"</span> <span class="o">%</span> <span class="n">rho</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">loc</span><span class="o">=</span><span class="s2">"upper left"</span><span class="p">,</span>
              <span class="n">borderaxespad</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig10-ar1-process">
<a class="reference internal image-reference" href="../_images/fig10_ar1_process.png"><img alt="../_images/fig10_ar1_process.png" src="../_images/fig10_ar1_process.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.10 </span><span class="caption-text">Random sample of an AR(1) process with <span class="math notranslate nohighlight">\(\sigma = 1\)</span> and different
<span class="math notranslate nohighlight">\(\rho\)</span>. Note that the AR(1) process is not stationary when
<span class="math notranslate nohighlight">\(\mid \rho \mid &gt; 1\)</span>.</span><a class="headerlink" href="#fig-fig10-ar1-process" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Using a for-loop to generate the time series random variable is pretty
straightforward, but now each time point is a random variable, which
makes working with it quite difficult (e.g., it does not scale well with
more time points). When possible, we prefer writing models that use
vectorized operations. The model above can be rewritten without using
for-loop by using the Autoregressive distribution <code class="docutils literal notranslate"><span class="pre">tfd.Autoregressive</span></code>
in TFP, which takes a <code class="docutils literal notranslate"><span class="pre">distribution_fn</span></code> that represents Equation
<a class="reference internal" href="#equation-eq-ar1">(6.5)</a>, a function that takes <span class="math notranslate nohighlight">\(y_{t-1}\)</span> as input and returns the
distribution of <span class="math notranslate nohighlight">\(y_t\)</span>. However, the Autoregressive distribution in TFP
only retains the end state of the process, the distribution representing
the random variable <span class="math notranslate nohighlight">\(y_t\)</span> after the initial value <span class="math notranslate nohighlight">\(y_0\)</span> iterates for <span class="math notranslate nohighlight">\(t\)</span>
steps. To get all the time steps of a AR process, we need to express
Equation <a class="reference internal" href="#equation-eq-ar1">(6.5)</a> a bit differently using a backshift operator, also
called (Lag operator) <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> that shifts the time series
<span class="math notranslate nohighlight">\(\mathbf{B} y_t = y_{t-1}\)</span> for all <span class="math notranslate nohighlight">\(t &gt; 0\)</span>. Re-expressing Equation
<a class="reference internal" href="#equation-eq-ar1">(6.5)</a> with a backshift operator <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> we have
<span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\rho \mathbf{B} Y, \sigma)\)</span>. Conceptually, you can
think of it as evaluating a vectorized likelihood
<code class="docutils literal notranslate"><span class="pre">Normal(ρ</span> <span class="pre">*</span> <span class="pre">y[:-1],</span> <span class="pre">σ).log_prob(y[1:])</span></code>. In Code Block
<a class="reference internal" href="#ar1-without-forloop"><span class="std std-ref">ar1_without_forloop</span></a> we construct
the same generative AR(1) model for <code class="docutils literal notranslate"><span class="pre">n_t</span></code> steps with the
<code class="docutils literal notranslate"><span class="pre">tfd.Autoregressive</span></code> API. Note that we did not construct the backshift
operator <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> explicitly by just generating the outcome
<span class="math notranslate nohighlight">\(y_{t-1}\)</span> directly shown in Code Block
<a class="reference internal" href="#ar1-without-forloop"><span class="std std-ref">ar1_without_forloop</span></a>, where a Python
function <code class="docutils literal notranslate"><span class="pre">ar1_fun</span></code> applies the backshift operation and generates the
distribution for the next step.</p>
<div class="literal-block-wrapper docutils container" id="ar1-without-forloop">
<div class="code-block-caption"><span class="caption-number">Listing 6.11 </span><span class="caption-text">ar1_without_forloop</span><a class="headerlink" href="#ar1-without-forloop" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">ar1_without_forloop</span><span class="p">():</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">ar1_fun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="c1"># We apply the backshift operation here</span>
        <span class="n">x_tm1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">x_tm1</span> <span class="o">*</span> <span class="n">rho</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
                               <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">dist</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Autoregressive</span><span class="p">(</span>
        <span class="n">distribution_fn</span><span class="o">=</span><span class="n">ar1_fun</span><span class="p">,</span>
        <span class="n">sample0</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">n_t</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">rho</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">num_steps</span><span class="o">=</span><span class="n">n_t</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We are now ready to extend the Facebook Prophet -like GAM above with
AR(1) process as likelihood. But before we do that let us rewrite the
GAM in Code Block <a class="reference internal" href="#gam"><span class="std std-ref">gam</span></a> slightly differently into
Code Block <a class="reference internal" href="#gam-alternative"><span class="std std-ref">gam_alternative</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="gam-alternative">
<div class="code-block-caption"><span class="caption-number">Listing 6.12 </span><span class="caption-text">gam_alternative</span><a class="headerlink" href="#gam-alternative" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gam_trend_seasonality</span><span class="p">():</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">n_pred</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"beta"</span><span class="p">))</span>
    <span class="n">seasonality</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">X_pred</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="n">k</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"k"</span><span class="p">))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span>
        <span class="n">co2_by_month_training_data</span><span class="p">[</span><span class="s2">"CO2"</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scale</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"m"</span><span class="p">))</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"tau"</span><span class="p">))</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Laplace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">tau</span><span class="p">),</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">n_changepoints</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"delta"</span><span class="p">)</span>

    <span class="n">growth_rate</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"ij,...j-&gt;...i"</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="o">-</span><span class="n">s</span> <span class="o">*</span> <span class="n">delta</span><span class="p">)</span>
    <span class="n">trend</span> <span class="o">=</span> <span class="n">growth_rate</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">offset</span>
    <span class="n">noise_sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">5.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"noise_sigma"</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">seasonality</span><span class="p">,</span> <span class="n">trend</span><span class="p">,</span> <span class="n">noise_sigma</span>

<span class="k">def</span> <span class="nf">generate_gam</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

    <span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
    <span class="k">def</span> <span class="nf">gam</span><span class="p">():</span>
        <span class="n">seasonality</span><span class="p">,</span> <span class="n">trend</span><span class="p">,</span> <span class="n">noise_sigma</span> <span class="o">=</span> <span class="k">yield from</span> <span class="n">gam_trend_seasonality</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">seasonality</span> <span class="o">+</span> <span class="n">trend</span>
        <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">co2_by_month_training_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="c1"># likelihood</span>
        <span class="n">observed</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
            <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"observed"</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">gam</span>

<span class="n">gam</span> <span class="o">=</span> <span class="n">generate_gam</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Comparing Code Block <a class="reference internal" href="#gam-alternative"><span class="std std-ref">gam_alternative</span></a>
with Code Block <a class="reference internal" href="#gam"><span class="std std-ref">gam</span></a>, we see two major differences:</p>
<ol class="arabic simple">
<li><p>We split out the construction of the trend and seasonality
components (with their priors) into a separate function, and in the
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> model block we use a <code class="docutils literal notranslate"><span class="pre">yield</span> <span class="pre">from</span></code>
statement so we get the identical <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code>
model in both Code Blocks;</p></li>
<li><p>We wrap the <code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> in another Python
function so it is easier to condition on both the training and
testing set.</p></li>
</ol>
<p>Code Block <a class="reference internal" href="#gam-alternative"><span class="std std-ref">gam_alternative</span></a> is a much
more modular approach. We can write down a GAM with an AR(1) likelihood
by just changing the likelihood part. This is what we do in Code Block
<a class="reference internal" href="#gam-with-ar-likelihood"><span class="std std-ref">gam_with_ar_likelihood</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="gam-with-ar-likelihood">
<div class="code-block-caption"><span class="caption-number">Listing 6.13 </span><span class="caption-text">gam_with_ar_likelihood</span><a class="headerlink" href="#gam-with-ar-likelihood" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_gam_ar_likelihood</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

    <span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
    <span class="k">def</span> <span class="nf">gam_with_ar_likelihood</span><span class="p">():</span>
        <span class="n">seasonality</span><span class="p">,</span> <span class="n">trend</span><span class="p">,</span> <span class="n">noise_sigma</span> <span class="o">=</span> <span class="k">yield from</span> <span class="n">gam_trend_seasonality</span><span class="p">()</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">seasonality</span> <span class="o">+</span> <span class="n">trend</span>
        <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">co2_by_month_training_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="c1"># Likelihood</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"rho"</span><span class="p">))</span>
        <span class="k">def</span> <span class="nf">ar_fun</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
            <span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
                            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">rho</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">y_hat</span>
            <span class="k">return</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
                <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">noise_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
                <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">observed</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Autoregressive</span><span class="p">(</span>
            <span class="n">distribution_fn</span><span class="o">=</span><span class="n">ar_fun</span><span class="p">,</span>
            <span class="n">sample0</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span>
            <span class="n">num_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"observed"</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">gam_with_ar_likelihood</span>

<span class="n">gam_with_ar_likelihood</span> <span class="o">=</span> <span class="n">generate_gam_ar_likelihood</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Another way to think about AR(1) model here is as extending our linear
regression notion to include an observation dependent column in the
design matrix, and setting the element of this column <span class="math notranslate nohighlight">\(x_i\)</span> being
<span class="math notranslate nohighlight">\(y_{i-1}\)</span>. The autoregressive coefficient <span class="math notranslate nohighlight">\(\rho\)</span> is then no different to
any other regression coefficient, which is just telling us what is the
linear contribution of the previous observation to the expectation of
the current observation <a class="footnote-reference brackets" href="#id59" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>. In this model, we found that the effect is
almost negligible by inspecting the posterior distribution of <span class="math notranslate nohighlight">\(\rho\)</span>
(see <a class="reference internal" href="#fig-fig11-ar1-likelihood-rho"><span class="std std-numref">Fig. 6.11</span></a>):</p>
<figure class="align-default" id="fig-fig11-ar1-likelihood-rho">
<a class="reference internal image-reference" href="../_images/fig11_ar1_likelihood_rho.png"><img alt="../_images/fig11_ar1_likelihood_rho.png" src="../_images/fig11_ar1_likelihood_rho.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.11 </span><span class="caption-text">Posterior distribution of the parameters in the likelihood for the
Facebook Prophet -like GAM defined in Code Block
<a class="reference internal" href="#gam-with-ar-likelihood"><span class="std std-ref">gam_with_ar_likelihood</span></a>. Leftmost
panel is the <span class="math notranslate nohighlight">\(\sigma\)</span> in the model with a Normal likelihood, middle and
rightmost panels are <span class="math notranslate nohighlight">\(\sigma\)</span> and <span class="math notranslate nohighlight">\(\rho\)</span> in the model with an AR(1)
likelihood. Both models return a similar estimation of <span class="math notranslate nohighlight">\(\sigma\)</span>, with
the <span class="math notranslate nohighlight">\(\rho\)</span> estimated centered around 0.</span><a class="headerlink" href="#fig-fig11-ar1-likelihood-rho" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Instead of using an AR(k) likelihood, we can also include AR in a time
series model by adding a latent AR component to the linear prediction.
This is the <code class="docutils literal notranslate"><span class="pre">gam_with_latent_ar</span></code> model in Code Block
<a class="reference internal" href="#gam-with-latent-ar"><span class="std std-ref">gam_with_latent_ar</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="gam-with-latent-ar">
<div class="code-block-caption"><span class="caption-number">Listing 6.14 </span><span class="caption-text">gam_with_latent_ar</span><a class="headerlink" href="#gam-with-latent-ar" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_gam_ar_latent</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

    <span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
    <span class="k">def</span> <span class="nf">gam_with_latent_ar</span><span class="p">():</span>
        <span class="n">seasonality</span><span class="p">,</span> <span class="n">trend</span><span class="p">,</span> <span class="n">noise_sigma</span> <span class="o">=</span> <span class="k">yield from</span> <span class="n">gam_trend_seasonality</span><span class="p">()</span>
        
        <span class="c1"># Latent AR(1)</span>
        <span class="n">ar_sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"ar_sigma"</span><span class="p">))</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"rho"</span><span class="p">))</span>
        <span class="k">def</span> <span class="nf">ar_fun</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
            <span class="n">loc</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">y</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span>
                            <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">rho</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
                <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">ar_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
                <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">temporal_error</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Autoregressive</span><span class="p">(</span>
            <span class="n">distribution_fn</span><span class="o">=</span><span class="n">ar_fun</span><span class="p">,</span>
            <span class="n">sample0</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">trend</span><span class="p">),</span>
            <span class="n">num_steps</span><span class="o">=</span><span class="n">trend</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"temporal_error"</span><span class="p">)</span>

        <span class="c1"># Linear prediction</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">seasonality</span> <span class="o">+</span> <span class="n">trend</span> <span class="o">+</span> <span class="n">temporal_error</span>
        <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
            <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="n">co2_by_month_training_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

        <span class="c1"># Likelihood</span>
        <span class="n">observed</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
            <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
            <span class="n">reinterpreted_batch_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"observed"</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">gam_with_latent_ar</span>

<span class="n">gam_with_latent_ar</span> <span class="o">=</span> <span class="n">generate_gam_ar_latent</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>With the explicit latent AR process, we are adding a random variable
with the same size as the observed data to the model. Since it is now an
explicit component added to the linear prediction <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, we can
interpret the AR process to be complementary to, or even part of, the
trend component. We can visualize the latent AR component after
inference similar to the trend and seasonality components of a time
series model (see <a class="reference internal" href="#fig-fig12-posterior-predictive-ar1"><span class="std std-numref">Fig. 6.12</span></a>).</p>
<figure class="align-default" id="fig-fig12-posterior-predictive-ar1">
<a class="reference internal image-reference" href="../_images/fig12_posterior_predictive_ar1.png"><img alt="../_images/fig12_posterior_predictive_ar1.png" src="../_images/fig12_posterior_predictive_ar1.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.12 </span><span class="caption-text">Posterior predictive samples of the trend, seasonality, and AR(1)
components of the GAM based time series model <code class="docutils literal notranslate"><span class="pre">gam_with_latent_ar</span></code>
specified in Code Block
<a class="reference internal" href="#gam-with-latent-ar"><span class="std std-ref">gam_with_latent_ar</span></a>.</span><a class="headerlink" href="#fig-fig12-posterior-predictive-ar1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Another way to interpret the explicit latent AR process is that it
captures the temporally correlated <em>residuals</em>, so we expect the
posterior estimation of the <span class="math notranslate nohighlight">\(\sigma_{noise}\)</span> will be smaller compared to
the model without this component. In
<a class="reference internal" href="#fig-fig13-ar1-likelihood-rho2"><span class="std std-numref">Fig. 6.13</span></a> we display the posterior
distribution of <span class="math notranslate nohighlight">\(\sigma_{noise}\)</span>, <span class="math notranslate nohighlight">\(\sigma_{AR}\)</span>, and <span class="math notranslate nohighlight">\(\rho\)</span> for model
<code class="docutils literal notranslate"><span class="pre">gam_with_latent_ar</span></code>. In comparison to model <code class="docutils literal notranslate"><span class="pre">gam_with_ar_likelihood</span></code>,
we indeed get a lower estimation of <span class="math notranslate nohighlight">\(\sigma_{noise}\)</span>, with a much higher
estimation of <span class="math notranslate nohighlight">\(\rho\)</span>.</p>
<figure class="align-default" id="fig-fig13-ar1-likelihood-rho2">
<a class="reference internal image-reference" href="../_images/fig13_ar1_likelihood_rho2.png"><img alt="../_images/fig13_ar1_likelihood_rho2.png" src="../_images/fig13_ar1_likelihood_rho2.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.13 </span><span class="caption-text">Posterior distribution of <span class="math notranslate nohighlight">\(\sigma_{noise}\)</span>, <span class="math notranslate nohighlight">\(\sigma_{AR}\)</span>, and <span class="math notranslate nohighlight">\(\rho\)</span> of
the AR(1) latent component for <code class="docutils literal notranslate"><span class="pre">gam_with_latent_ar</span></code> specified in Code
Block <a class="reference internal" href="#gam-with-latent-ar"><span class="std std-ref">gam_with_latent_ar</span></a>. Note not
to be confused with <a class="reference internal" href="#fig-fig11-ar1-likelihood-rho"><span class="std std-numref">Fig. 6.11</span></a> where we
displays posterior distribution of parameters from 2 different GAMs.</span><a class="headerlink" href="#fig-fig13-ar1-likelihood-rho2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="latent-ar-process-and-smoothing">
<span id="id18"></span><h3><span class="section-number">6.3.1. </span>Latent AR Process and Smoothing<a class="headerlink" href="#latent-ar-process-and-smoothing" title="Permalink to this heading">#</a></h3>
<p>A latent process is quite powerful at capturing the subtle trends in the
observed time series. It can even approximate some arbitrary functions.
To see that let us consider modeling a toy problem with a time series
model that contains a latent (GRW) component, as formulated in Equation
<a class="reference internal" href="#equation-eq-gw-formulation1">(6.6)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-gw-formulation1">
<span class="eqno">(6.6)<a class="headerlink" href="#equation-eq-gw-formulation1" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
z_i &amp; \sim \mathcal{N}(z_{i-1}, \sigma_{z}^2) \: \text{ for } i=1,\dots,N \\
y_i &amp; \sim \mathcal{N}(z_i,  \sigma_{y}^2)
\end{split}\end{split}\]</div>
<p>The GRW here is the same as an AR(1) process with <span class="math notranslate nohighlight">\(\rho = 1\)</span>. By placing
different prior on <span class="math notranslate nohighlight">\(\sigma_{z}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{y}\)</span> in Equation
<a class="reference internal" href="#equation-eq-gw-formulation1">(6.6)</a>, we can emphasize how much of the variance in
the observed data should be accounted for in the GRW, and how much is
iid <em>noise</em>. We can also compute the ratio
<span class="math notranslate nohighlight">\(\alpha = \frac{\sigma_{y}^2}{\sigma_{z}^2 + \sigma_{y}^2}\)</span>, where
<span class="math notranslate nohighlight">\(\alpha\)</span> is in the range <span class="math notranslate nohighlight">\([0, 1]\)</span> that can be interpret as the degree of
smoothing. Thus we can express the model in Equation
<a class="reference internal" href="#equation-eq-gw-formulation1">(6.6)</a> equivalently as Equation
<a class="reference internal" href="#equation-eq-gw-formulation2">(6.7)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-gw-formulation2">
<span class="eqno">(6.7)<a class="headerlink" href="#equation-eq-gw-formulation2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
z_i &amp; \sim \mathcal{N}(z_{i-1}, (1 - \alpha) \sigma^2) \: \text{ for } i=1,\dots,N \\
y_i &amp; \sim \mathcal{N}(z_i,   \alpha \sigma^2)
\end{split}\end{split}\]</div>
<p>Our latent GRW model in Equation <a class="reference internal" href="#equation-eq-gw-formulation2">(6.7)</a> could be
written in TFP in Code Block <a class="reference internal" href="#gw-tfp"><span class="std std-ref">gw_tfp</span></a>. By placing
informative prior on <span class="math notranslate nohighlight">\(\alpha\)</span> we can control how much “smoothing” we
would like to see in the latent GRW (larger <span class="math notranslate nohighlight">\(\alpha\)</span> gives smoother
approximation). Let us fit the model <code class="docutils literal notranslate"><span class="pre">smoothing_grw</span></code> with some noisy
observations simulated from an arbitrary function. The data is shown as
black solid dots in <a class="reference internal" href="#fig-fig14-smoothing-with-gw"><span class="std std-numref">Fig. 6.14</span></a>, with the
fitted latent Random Walk displayed in the same Figure. As you can see
we can approximate the underlying function pretty well.</p>
<div class="literal-block-wrapper docutils container" id="gw-tfp">
<div class="code-block-caption"><span class="caption-number">Listing 6.15 </span><span class="caption-text">gw_tfp</span><a class="headerlink" href="#gw-tfp" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">smoothing_grw</span><span class="p">():</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.</span><span class="p">))</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="mf">10.</span><span class="p">))</span>
    <span class="n">sigma0</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">sigma1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">sigma0</span><span class="p">),</span> <span class="n">num_steps</span><span class="p">)</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Independent</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">sigma1</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig14-smoothing-with-gw">
<a class="reference internal image-reference" href="../_images/fig14_smoothing_with_gw.png"><img alt="../_images/fig14_smoothing_with_gw.png" src="../_images/fig14_smoothing_with_gw.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.14 </span><span class="caption-text">Simulated observations from <span class="math notranslate nohighlight">\(y \sim \text{Normal}(f(x), 1)\)</span> with
<span class="math notranslate nohighlight">\(f(x) = e^{1 + x^{0.5} - e^{\frac{x}{15}}}\)</span>, and the inferred latent
Gaussian Random Walk. The gray semi-transparent region is the posterior
94% HDI interval of the latent Gaussian Random Walk <span class="math notranslate nohighlight">\(z\)</span>, with the
posterior mean plot in dash blue line.</span><a class="headerlink" href="#fig-fig14-smoothing-with-gw" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>There are a few other interesting properties of the AR process, with
connection to the Gaussian Process <span id="id19">[<a class="reference internal" href="references.html#id91" title="Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press, Cambridge, Mass, 2005. ISBN 978-0-262-18253-9.">52</a>]</span>. For example, you
might find that the Autoregressive model <em>alone</em> is useless to capture
the long-term trend. Even though the model seems to fit well the
observation, during forecast you will observe the forecast value regress
to the mean of the last few time steps very quickly. Same as what you
will observe using the Gaussian Process with a constant mean function
<a class="footnote-reference brackets" href="#id60" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>.</p>
<p>An autoregressive component as an additional trend component could place
some challenges to model inference. For example, scaling could be an
issue as we are adding a random variable with the same shape as the
observed time series. We might have an unidentifiable model when both
the trend component and the AR process are flexible, as the AR process
alone already has the ability to approximate the underlying trend, a
smoothed function, of the observed data as we have seen here.</p>
</section>
<section id="s-ar-i-ma-x">
<span id="sarimax"></span><h3><span class="section-number">6.3.2. </span>(S)AR(I)MA(X)<a class="headerlink" href="#s-ar-i-ma-x" title="Permalink to this heading">#</a></h3>
<p>Many classical time series models share a similar autoregressive like
pattern, where you have some latent parameter at time <span class="math notranslate nohighlight">\(t\)</span> that is
dependent on the value of itself or another parameter at <span class="math notranslate nohighlight">\(t-k\)</span>. Two
examples of these models are</p>
<ul class="simple">
<li><p>Autoregressive conditional heteroscedasticity (ARCH) model, where
the scale of the residuals vary over time;</p></li>
<li><p>Moving average (MA) model, which is a linear combination of previous
residuals are added to the mean of the series.</p></li>
</ul>
<p>Some of these classical time series models could be combined into more
complex models, one of such extensions is the Seasonal AutoRegressive
Integrated Moving Average with eXogenous regressors model (SARIMAX).
While the naming might look intimidating, the basic concept is largely a
straightforward combination of the AR and MA model. Extending the AR
model with MA we get:</p>
<div class="math notranslate nohighlight" id="equation-eq-arma">
<span class="eqno">(6.8)<a class="headerlink" href="#equation-eq-arma" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
y_t &amp; = \alpha + \sum_{i=1}^{p}\phi_i y_{t-i} + \sum_{j=1}^{q}\theta_j \epsilon_{t-j} + \epsilon_t \\
\epsilon_t &amp; \sim \mathcal{N}(0, \sigma^2)
\end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the order of the autoregressive model and <span class="math notranslate nohighlight">\(q\)</span> is the order
of the moving average model. Conventionally, we write models as such
being ARMA(p, q). Similarly, for seasonal ARMA we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-sarma">
<span class="eqno">(6.9)<a class="headerlink" href="#equation-eq-sarma" title="Permalink to this equation">#</a></span>\[\begin{split}
y_t = \alpha + \sum_{i=1}^{p}\phi_i y_{t-period*i} + \sum_{j=1}^{q}\theta_j \epsilon_{t-period*j} + \epsilon_t
\end{split}\]</div>
<p>The integrated part of an ARIMA model refers to the summary statistics
of a time series: order of integration. Denoted as <span class="math notranslate nohighlight">\(I(d)\)</span>, a time series
is integrated to order <span class="math notranslate nohighlight">\(d\)</span> if taking repeated differences <span class="math notranslate nohighlight">\(d\)</span> times
yields a stationary series. Following <span id="id21">Box <em>et al.</em> [<a class="reference internal" href="references.html#id130" title="G.E.P. Box, G.M. Jenkins, and G.C. Reinsel. Time Series Analysis: Forecasting and Control. Wiley Series in Probability and Statistics. Wiley, 2008. ISBN 9780470272848.">57</a>]</span>,
we repeatedly take difference of the observed time series as a
preprocessing step to account for the <span class="math notranslate nohighlight">\(I(d)\)</span> part of an ARIMA(p,d,q)
model, and model the resulting differenced series as a stationary
process with ARMA(p,q). The operation itself is also quite standard in
Python. We can use <code class="docutils literal notranslate"><span class="pre">numpy.diff</span></code> where the first difference computed is
<code class="docutils literal notranslate"><span class="pre">delta_y[i]</span> <span class="pre">=</span> <span class="pre">y[i]</span> <span class="pre">-</span> <span class="pre">y[i-1]</span></code> along a given axis, and higher differences
are calculated by repeating the same operation recursively on the
resulting array.</p>
<p>If we have an additional regressor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, in the model above
<span class="math notranslate nohighlight">\(\alpha\)</span> is replaced with the linear prediction <span class="math notranslate nohighlight">\(\mathbf{X} \beta\)</span>. We
will apply the same differencing operation on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> if <span class="math notranslate nohighlight">\(d &gt; 0\)</span>.
Note that we can have either seasonal (SARIMA) or exogenous regressors
(ARIMAX) but not both.</p>
<div class="admonition-notation-for-s-ar-i-ma-x admonition">
<p class="admonition-title">Notation for (S)AR(I)MA(X)</p>
<p>Typically, ARIMA models are denoted as ARIMA(p,d,q), which is to say we have a model containing order <span class="math notranslate nohighlight">\(p\)</span> of
AR, <span class="math notranslate nohighlight">\(d\)</span> degree of I, and order <span class="math notranslate nohighlight">\(q\)</span> of MA. For example, ARIMA(1,0,0) is
just a AR(1). We denote seasonal ARIMA models as
<span class="math notranslate nohighlight">\(\text{SARIMA}(p,d,q)(P,D,Q)_{s}\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> refers to the number of
periods in each season, and the uppercase <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(D\)</span>, <span class="math notranslate nohighlight">\(Q\)</span> are the seasonal
counter part of the ARIMA model <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(q\)</span>. Sometimes seasonal ARIMA
are denoted also as <span class="math notranslate nohighlight">\(\text{SARIMA}(p,d,q)(P,D,Q,s)\)</span>. If there are
exogenous regressors, we write <span class="math notranslate nohighlight">\(\text{ARIMAX}(p,d,q)\mathbf{X}[k]\)</span> with
<span class="math notranslate nohighlight">\(\mathbf{X}[k]\)</span> indicating we have a <span class="math notranslate nohighlight">\(k\)</span> columns design matrix
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
</div>
<p>As the second example in this chapter, we will use different ARIMA to
model the time series of the monthly live births in the United States
from 1948 to 1979 <span id="id22">[<a class="reference internal" href="references.html#id132" title="R. Shumway and D. Stoffer. Time Series: A Data Analysis Approach Using R. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2019. ISBN 9781000001563.">58</a>]</span>. The data is shown in
<a class="reference internal" href="#fig-fig15-birth-by-month"><span class="std std-numref">Fig. 6.15</span></a>.</p>
<figure class="align-default" id="fig-fig15-birth-by-month">
<a class="reference internal image-reference" href="../_images/fig15_birth_by_month.png"><img alt="../_images/fig15_birth_by_month.png" src="../_images/fig15_birth_by_month.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.15 </span><span class="caption-text">Monthly live births in the United States (1948-1979). Y-axis shows the
number of births in thousands.</span><a class="headerlink" href="#fig-fig15-birth-by-month" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We will start with a <span class="math notranslate nohighlight">\(\text{SARIMA}(1, 1, 1)(1, 1, 1)_{12}\)</span> model. First
we load and pre-process the observed time series in Code Block
<a class="reference internal" href="#sarima-preprocess"><span class="std std-ref">sarima_preprocess</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="sarima-preprocess">
<div class="code-block-caption"><span class="caption-number">Listing 6.16 </span><span class="caption-text">sarima_preprocess</span><a class="headerlink" href="#sarima-preprocess" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">us_monthly_birth</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">"../data/monthly_birth_usa.csv"</span><span class="p">)</span>
<span class="n">us_monthly_birth</span><span class="p">[</span><span class="s2">"date_month"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">us_monthly_birth</span><span class="p">[</span><span class="s2">"date_month"</span><span class="p">])</span>
<span class="n">us_monthly_birth</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">"date_month"</span><span class="p">,</span> <span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># y ~ Sarima(1,1,1)(1,1,1)[12]</span>
<span class="n">p</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">P</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">period</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="c1"># Time series data: us_monthly_birth.shape = (372,)</span>
<span class="n">observed</span> <span class="o">=</span> <span class="n">us_monthly_birth</span><span class="p">[</span><span class="s2">"birth_in_thousands"</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="c1"># Integrated to seasonal order $D$</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
    <span class="n">observed</span> <span class="o">=</span> <span class="n">observed</span><span class="p">[</span><span class="n">period</span><span class="p">:]</span> <span class="o">-</span> <span class="n">observed</span><span class="p">[:</span><span class="o">-</span><span class="n">period</span><span class="p">]</span>
<span class="c1"># Integrated to order $d$</span>
<span class="n">observed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.diff.html#numpy.diff" title="numpy.diff"><span class="n">np</span><span class="o">.</span><span class="n">diff</span></a><span class="p">(</span><span class="n">observed</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">d</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>At time of writing TFP does not have a dedicated implementation of an
ARMA distribution. To run inference of our SARIMA model, TFP requires a
Python <code class="docutils literal notranslate"><span class="pre">callable</span></code> representing the log posterior density function (up to
some constant <span id="id23">[<a class="reference internal" href="references.html#id133" title="Junpeng Lao, Christopher Suter, Ian Langmore, Cyril Chimisov, Ashish Saxena, Pavel Sountsov, Dave Moore, Rif A Saurous, Matthew D Hoffman, and Joshua V. Dillon. Tfp.mcmc: modern markov chain monte carlo tools built for modern hardware. arXiv preprint arXiv:2002.01184, 2020.">30</a>]</span>. In this case, we can archive that by
implementing the likelihood function of <span class="math notranslate nohighlight">\(\text{SARMA}(1, 1)(1, 1)_{12}\)</span>
(since the <span class="math notranslate nohighlight">\(\text{I}\)</span> part is already dealt with via differencing). We
do that in Code Block
<a class="reference internal" href="#sarima-likelihood"><span class="std std-ref">sarima_likelihood</span></a> using a
<code class="docutils literal notranslate"><span class="pre">tf.while_loop</span></code> to construct the residual time series <span class="math notranslate nohighlight">\(\epsilon_t\)</span> and
evaluated on a <span class="math notranslate nohighlight">\(\text{Normal}\)</span> distribution <a class="footnote-reference brackets" href="#id61" id="id24" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. From the programming
point of view, the biggest challenge here is to make sure the shape is
correct when we index to the time series. To avoid additional control
flow to check whether some of the indexes are valid (e.g, we cannot
index to <span class="math notranslate nohighlight">\(t-1\)</span> and <span class="math notranslate nohighlight">\(t-period-1\)</span> when <span class="math notranslate nohighlight">\(t=0\)</span>), we pad the time series with
zeros.</p>
<div class="literal-block-wrapper docutils container" id="sarima-likelihood">
<div class="code-block-caption"><span class="caption-number">Listing 6.17 </span><span class="caption-text">sarima_likelihood</span><a class="headerlink" href="#sarima-likelihood" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">likelihood</span><span class="p">(</span><span class="n">mu0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">sphi</span><span class="p">,</span> <span class="n">stheta</span><span class="p">):</span>
    <span class="n">batch_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">mu0</span><span class="p">)</span>
    <span class="n">y_extended</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([[</span><span class="n">r</span><span class="p">],</span> <span class="n">batch_shape</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mu0</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"...,j-&gt;j..."</span><span class="p">,</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mu0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">observed</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                  <span class="n">observed</span><span class="p">)],</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">eps_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_extended</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">observed</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">arma_onestep</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">eps_t</span><span class="p">):</span>
        <span class="n">t_shift</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">r</span>
        <span class="c1"># AR</span>
        <span class="n">y_past</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y_extended</span><span class="p">,</span> <span class="n">t_shift</span> <span class="o">-</span> <span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">ar</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"...p,p...-&gt;..."</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">y_past</span><span class="p">)</span>
        <span class="c1"># MA</span>
        <span class="n">eps_past</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">eps_t</span><span class="p">,</span> <span class="n">t_shift</span> <span class="o">-</span> <span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">ma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"...q,q...-&gt;..."</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">eps_past</span><span class="p">)</span>
        <span class="c1"># Seasonal AR</span>
        <span class="n">sy_past</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y_extended</span><span class="p">,</span> <span class="n">t_shift</span> <span class="o">-</span> <span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">period</span><span class="p">)</span>
        <span class="n">sar</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"...p,p...-&gt;..."</span><span class="p">,</span> <span class="n">sphi</span><span class="p">,</span> <span class="n">sy_past</span><span class="p">)</span>
        <span class="c1"># Seasonal MA</span>
        <span class="n">seps_past</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">eps_t</span><span class="p">,</span> <span class="n">t_shift</span> <span class="o">-</span> <span class="p">(</span><a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="n">Q</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">period</span><span class="p">)</span>
        <span class="n">sma</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"...q,q...-&gt;..."</span><span class="p">,</span> <span class="n">stheta</span><span class="p">,</span> <span class="n">seps_past</span><span class="p">)</span>

        <span class="n">mu_at_t</span> <span class="o">=</span> <span class="n">ar</span> <span class="o">+</span> <span class="n">ma</span> <span class="o">+</span> <span class="n">sar</span> <span class="o">+</span> <span class="n">sma</span> <span class="o">+</span> <span class="n">mu0</span>
        <span class="n">eps_update</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">y_extended</span><span class="p">,</span> <span class="n">t_shift</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu_at_t</span>
        <span class="n">epsilon_t_next</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">tensor_scatter_nd_update</span><span class="p">(</span>
            <span class="n">eps_t</span><span class="p">,</span> <span class="p">[[</span><span class="n">t_shift</span><span class="p">]],</span> <span class="n">eps_update</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon_t_next</span>

    <span class="n">t</span><span class="p">,</span> <span class="n">eps_output_</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">t</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">:</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">observed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">arma_onestep</span><span class="p">,</span>
        <span class="n">loop_vars</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps_t</span><span class="p">),</span>
        <span class="n">maximum_iterations</span><span class="o">=</span><span class="n">observed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">eps_output</span> <span class="o">=</span> <span class="n">eps_output_</span><span class="p">[</span><span class="n">r</span><span class="p">:]</span>
    <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
        <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">eps_output</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Adding the prior to the unknown parameters (in this case, <code class="docutils literal notranslate"><span class="pre">mu0</span></code>,
<code class="docutils literal notranslate"><span class="pre">sigma</span></code>, <code class="docutils literal notranslate"><span class="pre">phi</span></code>, <code class="docutils literal notranslate"><span class="pre">theta</span></code>, <code class="docutils literal notranslate"><span class="pre">sphi</span></code>, and <code class="docutils literal notranslate"><span class="pre">stheta</span></code>), we can generate the
posterior density function for inference. This is shown in Code Block
<a class="reference internal" href="#sarima-posterior"><span class="std std-ref">sarima_posterior</span></a>, with a resulting
<code class="docutils literal notranslate"><span class="pre">target_log_prob_fn</span></code> that we sample from in Code Block
<a class="reference internal" href="#sarima-posterior"><span class="std std-ref">sarima_posterior</span></a> <a class="footnote-reference brackets" href="#id62" id="id25" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="sarima-posterior">
<div class="code-block-caption"><span class="caption-number">Listing 6.18 </span><span class="caption-text">sarima_posterior</span><a class="headerlink" href="#sarima-posterior" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">sarima_priors</span><span class="p">():</span>
    <span class="n">mu0</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'mu0'</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'sigma'</span><span class="p">))</span>

    <span class="n">phi</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">p</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'phi'</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">q</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'theta'</span><span class="p">))</span>
    <span class="n">sphi</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">P</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'sphi'</span><span class="p">))</span>
    <span class="n">stheta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="n">Q</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">'stheta'</span><span class="p">))</span>

<span class="n">target_log_prob_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">sarima_priors</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">likelihood</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The preprocessing of the time series to account for the <em>integrated</em>
part in Code Block <a class="reference internal" href="#sarima-preprocess"><span class="std std-ref">sarima_preprocess</span></a>
and the likelihood implementation in Code Block
<a class="reference internal" href="#sarima-likelihood"><span class="std std-ref">sarima_likelihood</span></a> could be refactored
into a helper Python <code class="docutils literal notranslate"><span class="pre">Class</span></code> that flexibility generate different SARIMA
likelihood. For example, <a class="reference internal" href="#tab-loo-sarima"><span class="std std-numref">Table 6.1</span></a> shows the model
comparison between the <span class="math notranslate nohighlight">\(\text{SARIMA}(1,1,1)(1,1,1)_{12}\)</span> model from
Code Block <a class="reference internal" href="#sarima-posterior"><span class="std std-ref">sarima_posterior</span></a> and a
similar <span class="math notranslate nohighlight">\(\text{SARIMA}(0,1,2)(1,1,1)_{12}\)</span> model.</p>
<table class="table" id="tab-loo-sarima">
<caption><span class="caption-number">Table 6.1 </span><span class="caption-text">Summary of model comparison using LOO (log scale) for different SARIMA models.</span><a class="headerlink" href="#tab-loo-sarima" title="Permalink to this table">#</a></caption>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>rank</strong></p></td>
<td><p><strong>loo</strong></p></td>
<td><p><strong>p_loo</strong></p></td>
<td><p><strong>d_loo</strong></p></td>
<td><p><strong>weight</strong></p></td>
<td><p><strong>se</strong></p></td>
<td><p><strong>dse</strong></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\text{SARIMA}(0,1,2)(1,1,1)_{12}\)</span></p></td>
<td><p>0</p></td>
<td><p>-1235.60</p></td>
<td><p>7.51</p></td>
<td><p>0.00</p></td>
<td><p>0.5</p></td>
<td><p>15.41</p></td>
<td><p>0.00</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\text{SARIMA}(1,1,1)(1,1,1)_{12}\)</span></p></td>
<td><p>1</p></td>
<td><p>-1235.97</p></td>
<td><p>8.30</p></td>
<td><p>0.37</p></td>
<td><p>0.5</p></td>
<td><p>15.47</p></td>
<td><p>6.29</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="state-space-models">
<span id="id26"></span><h2><span class="section-number">6.4. </span>State Space Models<a class="headerlink" href="#state-space-models" title="Permalink to this heading">#</a></h2>
<p>In the implementation of the ARMA log-likelihood function above (Code
Block <a class="reference internal" href="#sarima-likelihood"><span class="std std-ref">sarima_likelihood</span></a>), we iterate
through time steps to condition on the observations and construct some
latent variables for that time slice. Indeed, unless the models are of a
very specific and simple variety (e.g. the Markov dependencies between
each two consecutive time steps make it possible to reduce the
generative process into vectorized operations), this recursive pattern
is a very natural way to express time series models. A powerful, general
formulation of this pattern is the State Space model, a discrete-time
process where we assume at each time step some latent states <span class="math notranslate nohighlight">\(X_t\)</span>
evolves from previous step <span class="math notranslate nohighlight">\(X_{t-1}\)</span> (a Markov Sequence), and we
observed <span class="math notranslate nohighlight">\(Y_t\)</span> that is some projection from the latent states <span class="math notranslate nohighlight">\(X_t\)</span> to
the observable space <a class="footnote-reference brackets" href="#id63" id="id27" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-state-space-model">
<span class="eqno">(6.10)<a class="headerlink" href="#equation-eq-state-space-model" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
X_0 &amp; \sim p(X_0) \\
\text{for t in 0...T:} \\
    Y_t &amp; \sim p^{\psi}(Y_t \mid X_t) \\
    X_{t+1} &amp; \sim p^{\theta}(X_{t+1} \mid X_{t})
\end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p(X_0)\)</span> is the prior distribution of the latent states at time
step 0, <span class="math notranslate nohighlight">\(p^{\theta}(X_{t+1} \mid X_t)\)</span> is the transition probability
distribution parameterized by a vector of parameter <span class="math notranslate nohighlight">\(\theta\)</span> that
describes the system dynamics, and <span class="math notranslate nohighlight">\(p^{\psi}(Y_t \mid X_t)\)</span> being the
observation distribution parameterized by <span class="math notranslate nohighlight">\(\psi\)</span> that describes the
measurement at time <span class="math notranslate nohighlight">\(t\)</span> conditioned on the latent states.</p>
<div class="admonition-implementation-of-state-space-model-for-efficient-computation admonition">
<p class="admonition-title">Implementation of state space model for efficient computation</p>
<p>There is a harmony between mathematical formulation and computation implementation
of a State Space model with API like <code class="docutils literal notranslate"><span class="pre">tf.while_loop</span></code> or <code class="docutils literal notranslate"><span class="pre">tf.scan</span></code>.
Unlike using a Python <code class="docutils literal notranslate"><span class="pre">for</span></code> loop or <code class="docutils literal notranslate"><span class="pre">while</span></code> loop, they require compiling
the loop body into a function that takes the same structure of tensors
as input and outputs. This functional style of implementation is useful
to make explicit how the latent states are being transitioned at each
time step and how the measurement, from latent state to observed, should
be outputted. It is worth noting that implementation of state space
model and its associated inference algorithm like Kalman filter also
involved design decisions about where to place some of the initial
computation. In the formulation above, we place a prior on the initial
latent condition, and the first observation is a measure of the initial
state directly. However, it is equally valid to make a transition on the
latent state at step 0, then make the first observation with
modification to the prior distribution the two approaches are
equivalent.</p>
<p>There is however a subtle trickiness in dealing with shape when
implementing filters for time series problems. The main challenge is
where to place the time dimension. An obvious choice is to place it at
axis 0, as it becomes nature to do <code class="docutils literal notranslate"><span class="pre">time_series[t]</span></code> with <code class="docutils literal notranslate"><span class="pre">t</span></code> being some
time index. Moreover, loop construction using <code class="docutils literal notranslate"><span class="pre">tf.scan</span></code> or <code class="docutils literal notranslate"><span class="pre">theano.scan</span></code>
to loop over a time series automatically places the time dimension on
axis 0. However, it conflicts with the batch dimensions, which are
usually the leading axis. For example, if we want to vectorize over N
batch of k dimension time series, each with T total time stamps, the
array will have a shape of <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">T,</span> <span class="pre">...]</span></code> but the output of <code class="docutils literal notranslate"><span class="pre">tf.scan</span></code>
will have a shape of <code class="docutils literal notranslate"><span class="pre">[T,</span> <span class="pre">N,</span> <span class="pre">...]</span></code>. Currently, it seems unavoidable that
modelers need to perform some transpose on a scan output so that it
matches the semantic of the batch and time dimension as the input.</p>
</div>
<p>Once we have the state space representation of a time series problem, we
are in a sequential analysis framework that typically includes tasks
like filtering and smoothing:</p>
<ul class="simple">
<li><p>Filtering: computing the marginal distribution of the latent state
<span class="math notranslate nohighlight">\(X_k\)</span>, conditioned on observations up to that time step <span class="math notranslate nohighlight">\(k\)</span>:
<span class="math notranslate nohighlight">\(p(X_k \mid y_{0:k}), k = 0,...,T\)</span>; <span class="math notranslate nohighlight">\(\circ\)</span> Prediction: a forecast
distribution of the latent state, extending the filtering
distribution into the future for <span class="math notranslate nohighlight">\(n\)</span> steps:
<span class="math notranslate nohighlight">\(p(X_k+n \mid y_{0:k}), k = 0,...,T, n=1, 2,...\)</span></p></li>
<li><p>Smoothing: similar to filtering where we try to compute the marginal
distribution of the latent state at each time step <span class="math notranslate nohighlight">\(X_k\)</span>, but
conditioned on all observations: :
<span class="math notranslate nohighlight">\(p(X_k \mid y_{0:T}), k = 0,...,T\)</span>.</p></li>
</ul>
<p>notice how the subscript of <span class="math notranslate nohighlight">\(y_{0:\dots}\)</span> is different in filtering and
smoothing: for filtering it is conditioned on <span class="math notranslate nohighlight">\(y_{0:k}\)</span> and for
smoothing it is conditioned on <span class="math notranslate nohighlight">\(y_{0:T}\)</span>.</p>
<p>Indeed, there is a strong tradition of considering time series modeling
problems from a filtering and smoothing perspective. For example, the
way we compute log likelihood of an ARMA process above could be seen as
a filtering problem where the observed data is deconstructed into some
latent unobserved states.</p>
<section id="linear-gaussian-state-space-models-and-kalman-filter">
<span id="lgssm-time-series"></span><h3><span class="section-number">6.4.1. </span>Linear Gaussian State Space Models and Kalman filter<a class="headerlink" href="#linear-gaussian-state-space-models-and-kalman-filter" title="Permalink to this heading">#</a></h3>
<p>Perhaps one of the most notable State Space models is Linear Gaussian
State Space Model, where we have latent states <span class="math notranslate nohighlight">\(X_t\)</span> and the observation
model <span class="math notranslate nohighlight">\(Y_t\)</span> distributed as (multivariate) Gaussian, with the transition
and measurement both being linear functions:</p>
<div class="math notranslate nohighlight" id="equation-eq-lgssm">
<span class="eqno">(6.11)<a class="headerlink" href="#equation-eq-lgssm" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
Y_t &amp; = \mathbf{H}_t X_t + \epsilon_t \\
X_t &amp; = \mathbf{F}_t X_{t-1} + \eta_t
\end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_t \sim \mathcal{N}(0, \mathbf{R}_t)\)</span> and
<span class="math notranslate nohighlight">\(\eta_t \sim \mathcal{N}(0, \mathbf{Q}_t)\)</span> are the noise components.
Variables (<span class="math notranslate nohighlight">\(\mathbf{H}_t\)</span>, <span class="math notranslate nohighlight">\(\mathbf{F}_t\)</span>) are matrices describing the
linear transformation (Linear Operators) usually <span class="math notranslate nohighlight">\(\mathbf{F}_t\)</span> is a
square matrix and <span class="math notranslate nohighlight">\(\mathbf{H}_t\)</span> has a lower rank than <span class="math notranslate nohighlight">\(\mathbf{F}_t\)</span>
that “push-forward” the states from latent space to measurement space.
<span class="math notranslate nohighlight">\(\mathbf{R}_t\)</span>, <span class="math notranslate nohighlight">\(\mathbf{Q}_t\)</span> are covariance matrices (positive
semidefinite matrices). You can also find some intuitive examples of
transition matrix in Section <a class="reference internal" href="chp_11.html#markov-chains"><span class="std std-ref">Markov Chains</span></a>.</p>
<p>Since <span class="math notranslate nohighlight">\(\epsilon_t\)</span> and <span class="math notranslate nohighlight">\(\eta_t\)</span> are random variables following Gaussian
distribution, the linear function above performs affine transformation
of the Gaussian random variables, resulting in <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(Y_t\)</span> also
distributed as Gaussian. The property of the prior (state at <span class="math notranslate nohighlight">\(t-1\)</span>) and
posterior (state at <span class="math notranslate nohighlight">\(t\)</span>) being conjugate make it possible to derive a
closed form solution to the Bayesian filtering equations: the Kalman
filter (Kalman, 1960). Arguably the most important application of a
conjugate Bayesian model, the Kalman filter helped humans land on the
moon and is still widely used in many areas.</p>
<p>To gain an intuitive understanding of Kalman filter, we first look at
the generative process from time <span class="math notranslate nohighlight">\(t-1\)</span> to <span class="math notranslate nohighlight">\(t\)</span> of the Linear Gaussian
State Space Model:</p>
<div class="math notranslate nohighlight" id="equation-eq-lgssm-generative">
<span class="eqno">(6.12)<a class="headerlink" href="#equation-eq-lgssm-generative" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
X_t \sim p(X_t \mid X_{t-1}) &amp; \equiv \mathcal{N}(\mathbf{F}_{t} X_{t-1}, \mathbf{Q}_{t}) \\
    Y_t \sim p(Y_t \mid X_t) &amp; \equiv \mathcal{N}(\mathbf{H}_t X_t, \mathbf{R}_t)
\end{split}\end{split}\]</div>
<p>where the conditioned distribution of <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(Y_t\)</span> are denoted as
<span class="math notranslate nohighlight">\(p(.)\)</span> (we use <span class="math notranslate nohighlight">\(\equiv\)</span> to indicate that the conditional distribution is
a Multivariate Gaussian). Note that <span class="math notranslate nohighlight">\(X_t\)</span> only depends on the state from
the last time step <span class="math notranslate nohighlight">\(X_{t-1}\)</span> but not the past observation(s). This means
that the generative process could very well be done by first generating
the latent time series <span class="math notranslate nohighlight">\(X_t\)</span> for <span class="math notranslate nohighlight">\(t = 0...T\)</span> and then project the whole
latent time series to the measurement space. In the Bayesian filtering
context, <span class="math notranslate nohighlight">\(Y_t\)</span> is observed (partly if there is missing data) and thus to
be used to update the state <span class="math notranslate nohighlight">\(X_t\)</span>, similar to how we update the prior
using the observed likelihood in a static model:</p>
<div class="math notranslate nohighlight" id="equation-eq-kalman-fitler">
<span class="eqno">(6.13)<a class="headerlink" href="#equation-eq-kalman-fitler" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
X_0 \sim p(X_0 \mid m_0, \mathbf{P}_0) &amp; \equiv \mathcal{N}(m_0, \mathbf{P}_0) \\
X_{t \mid t-1} \sim p(X_{t \mid t-1} \mid Y_{0:t-1}) &amp; \equiv \mathcal{N}(m_{t \mid t-1}, \mathbf{P}_{t \mid t-1}) \\
X_{t \mid t} \sim p(X_{t \mid t} \mid Y_{0:t}) &amp; \equiv \mathcal{N}(m_{t \mid t}, \mathbf{P}_{t \mid t}) \\
Y_t \sim p(Y_t \mid Y_{0:t-1}) &amp; \equiv \mathcal{N}(\mathbf{H}_t m_{t \mid t-1}, \mathbf{S}_t)
\end{split}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(m_t\)</span> and <span class="math notranslate nohighlight">\(\mathbf{P}_t\)</span> represent the mean and covariance matrix
of the latent state <span class="math notranslate nohighlight">\(X_t\)</span> at each time step. <span class="math notranslate nohighlight">\(X_{t \mid t-1}\)</span> is the
predicted latent state with associated parameter <span class="math notranslate nohighlight">\(m_{t \mid t-1}\)</span>
(predicted mean) and <span class="math notranslate nohighlight">\(\mathbf{P}_{t \mid t-1}\)</span> (predicted covariance),
whereas <span class="math notranslate nohighlight">\(X_{t \mid t}\)</span> is the filtered latent state with associated
parameter <span class="math notranslate nohighlight">\(m_{t \mid t}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{P}_{t \mid t}\)</span>. The subscripts in
Equation <a class="reference internal" href="#equation-eq-kalman-fitler">(6.13)</a> might get confusing, a good high-level
view to keep in mind is that from the previous time step we have a
filtered state <span class="math notranslate nohighlight">\(X_{t-1 \mid t-1}\)</span>, which after applying the transition
matrix <span class="math notranslate nohighlight">\(\mathbf{F}_{t}\)</span> we get a predicted state <span class="math notranslate nohighlight">\(X_{t \mid t-1}\)</span>, and
upon incorporating the observation of the current time step we get the
filtered state for the next time step <span class="math notranslate nohighlight">\(X_{t \mid t}\)</span>.</p>
<p>The parameters of the distributions above in Equation
<a class="reference internal" href="#equation-eq-kalman-fitler">(6.13)</a> are computed using the Kalman filter prediction
and update steps:</p>
<ul>
<li><p>Prediction</p>
<div class="math notranslate nohighlight" id="equation-eq-kalman-fitler-preddict-step">
<span class="eqno">(6.14)<a class="headerlink" href="#equation-eq-kalman-fitler-preddict-step" title="Permalink to this equation">#</a></span>\[\begin{split}    \begin{split}
    m_{t \mid t-1} &amp; = \mathbf{F}_{t} m_{t-1 \mid t-1} \\
    \mathbf{P}_{t \mid t-1} &amp; = \mathbf{F}_{t} \mathbf{P}_{t-1 \mid t-1} \mathbf{F}_{t}^T + \mathbf{Q}_{t}
    \end{split}\end{split}\]</div>
</li>
<li><p>Update</p>
<div class="math notranslate nohighlight" id="equation-eq-kalman-fitler-update-step">
<span class="eqno">(6.15)<a class="headerlink" href="#equation-eq-kalman-fitler-update-step" title="Permalink to this equation">#</a></span>\[\begin{split}    \begin{split}
    z_t &amp; = Y_t - \mathbf{H}_t m_{t \mid t-1} \\
    \mathbf{S}_t &amp; = \mathbf{H}_t \mathbf{P}_{t \mid t-1} \mathbf{H}_t^T + \mathbf{R}_t \\
    \mathbf{K}_t &amp; = \mathbf{P}_{t \mid t-1} \mathbf{H}_t^T \mathbf{S}_t^{-1} \\
    m_{t \mid t} &amp; = m_{t \mid t-1} + \mathbf{K}_t z_t \\
    \mathbf{P}_{t \mid t} &amp; = \mathbf{P}_{t \mid t-1} - \mathbf{K}_t \mathbf{S}_t \mathbf{K}_t^T
    \end{split}\end{split}\]</div>
</li>
</ul>
<p>The proof of deriving the Kalman filter equations is an application of
the joint multivariate Gaussian distribution. In practice, there are
some tricks in implementation to make sure the computation is
numerically stable (e.g., avoid inverting matrix <span class="math notranslate nohighlight">\(\mathbf{S}_t\)</span>, using a
Jordan form update in computing <span class="math notranslate nohighlight">\(\mathbf{P}_{t \mid t}\)</span> to ensure the
result is a positive definite matrix <span id="id28">[<a class="reference internal" href="references.html#id134" title="M. West and J. Harrison. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York, 2013. ISBN 9781475793659.">59</a>]</span>. In TFP, the
linear Gaussian state space model and related Kalman filter is
conveniently implemented as a distribution
<code class="docutils literal notranslate"><span class="pre">tfd.LinearGaussianStateSpaceModel</span></code>.</p>
<p>One of the practical challenges in using Linear Gaussian State Space
Model for time series modeling is expressing the unknown parameters as
Gaussian latent state. We will demonstrate with a simple linear growth
time series as the first example (see Chapter 3 of Bayesian Filtering
and Smoothing <span id="id29">[<a class="reference internal" href="references.html#id129" title="S. Särkkä. Bayesian Filtering and Smoothing. Bayesian Filtering and Smoothing. Cambridge University Press, 2013. ISBN 9781107030657.">60</a>]</span>:</p>
<div class="literal-block-wrapper docutils container" id="linear-growth-model">
<div class="code-block-caption"><span class="caption-number">Listing 6.19 </span><span class="caption-text">linear_growth_model</span><a class="headerlink" href="#linear-growth-model" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.6</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">time_stamp</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">theta1</span> <span class="o">*</span> <span class="n">time_stamp</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>You might recognize Code Block
<a class="reference internal" href="#linear-growth-model"><span class="std std-ref">linear_growth_model</span></a> as a simple
linear regression. To solve it as a filtering problem using Kalman
filter, we need to assume that the measurement noise <span class="math notranslate nohighlight">\(\sigma\)</span> is known,
and the unknown parameters <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span> follow a Gaussian
prior distribution.</p>
<p>In a state space form, we have the latent states:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-growth-state">
<span class="eqno">(6.16)<a class="headerlink" href="#equation-eq-linear-growth-state" title="Permalink to this equation">#</a></span>\[\begin{split}X_t = \left[\begin{array}{ccc}
  \theta_0 \\
  \theta_1 \\
\end{array}\right]\end{split}\]</div>
<p>Since the latent state does not change over time, the transition
operator <span class="math notranslate nohighlight">\(F_t\)</span> is an identity matrix with no transition noise. The
observation operator describes the “push-forward” from latent to
measurement space, which is a matrix form of the linear function <a class="footnote-reference brackets" href="#id64" id="id30" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-growth-observed-state">
<span class="eqno">(6.17)<a class="headerlink" href="#equation-eq-linear-growth-observed-state" title="Permalink to this equation">#</a></span>\[\begin{split}y_t = \theta_0 + \theta_1 * t = \left[\begin{array}{ccc}
  1, t \\
\end{array}\right]\left[\begin{array}{ccc}
  \theta_0 \\
  \theta_1 \\
\end{array}\right]\end{split}\]</div>
<p>Expressed with the <code class="docutils literal notranslate"><span class="pre">tfd.LinearGaussianStateSpaceModel</span></code> API, we have:</p>
<div class="literal-block-wrapper docutils container" id="tfd-lgssm-linear-growth">
<div class="code-block-caption"><span class="caption-number">Listing 6.20 </span><span class="caption-text">tfd_lgssm_linear_growth</span><a class="headerlink" href="#tfd-lgssm-linear-growth" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># X_0</span>
<span class="n">initial_state_prior</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">scale_diag</span><span class="o">=</span><span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">])</span>
<span class="c1"># F_t</span>
<span class="n">transition_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperatorIdentity</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># eta_t ~ Normal(0, Q_t)</span>
<span class="n">transition_noise</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">scale_diag</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
<span class="c1"># H_t</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">time_stamp</span><span class="p">),</span> <span class="n">time_stamp</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">observation_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperatorFullMatrix</span><span class="p">(</span>
    <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">t</span><span class="p">)])</span>
<span class="c1"># epsilon_t ~ Normal(0, R_t)</span>
<span class="n">observation_noise</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">],</span> <span class="n">scale_diag</span><span class="o">=</span><span class="p">[</span><span class="n">sigma</span><span class="p">])</span>

<span class="n">linear_growth_model</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">LinearGaussianStateSpaceModel</span><span class="p">(</span>
    <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num_timesteps</span><span class="p">,</span>
    <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">,</span>
    <span class="n">transition_noise</span><span class="o">=</span><span class="n">transition_noise</span><span class="p">,</span>
    <span class="n">observation_matrix</span><span class="o">=</span><span class="n">observation_matrix</span><span class="p">,</span>
    <span class="n">observation_noise</span><span class="o">=</span><span class="n">observation_noise</span><span class="p">,</span>
    <span class="n">initial_state_prior</span><span class="o">=</span><span class="n">initial_state_prior</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>we can apply the Kalman filter to obtain the posterior distribution of
<span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span>:</p>
<div class="literal-block-wrapper docutils container" id="tfd-lgssm-linear-growth-filter">
<div class="code-block-caption"><span class="caption-number">Listing 6.21 </span><span class="caption-text">tfd_lgssm_linear_growth_filter</span><a class="headerlink" href="#tfd-lgssm-linear-growth-filter" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run the Kalman filter</span>
<span class="p">(</span>
    <span class="n">log_likelihoods</span><span class="p">,</span>
    <span class="n">mt_filtered</span><span class="p">,</span> <span class="n">Pt_filtered</span><span class="p">,</span>
    <span class="n">mt_predicted</span><span class="p">,</span> <span class="n">Pt_predicted</span><span class="p">,</span>
    <span class="n">observation_means</span><span class="p">,</span> <span class="n">observation_cov</span>  <span class="c1"># observation_cov is S_t</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">linear_growth_model</span><span class="o">.</span><span class="n">forward_filter</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can compare the result from the Kalman filter (i.e., iteratively
observing each time steps) with the analytic result (i.e., observing the
full time series) in <a class="reference internal" href="#fig-fig16-linear-growth-lgssm"><span class="std std-numref">Fig. 6.16</span></a>.</p>
<figure class="align-default" id="fig-fig16-linear-growth-lgssm">
<a class="reference internal image-reference" href="../_images/fig16_linear_growth_lgssm.png"><img alt="../_images/fig16_linear_growth_lgssm.png" src="../_images/fig16_linear_growth_lgssm.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.16 </span><span class="caption-text">Linear Growth time series model, inference using a Kalman filter. In the
first panel we show the observed data (gray dot connected by dash line)
and the one-step prediction from the Kalman filter (<span class="math notranslate nohighlight">\(H_t m_{t \mid t-1}\)</span>
in solid black line). The posterior distribution of the latent state
<span class="math notranslate nohighlight">\(X_t\)</span> after observing each time step is compared with the closed form
solution using all data (black solid line) in the middle and rightmost
panel.</span><a class="headerlink" href="#fig-fig16-linear-growth-lgssm" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="arima-expressed-as-a-state-space-model">
<span id="id31"></span><h3><span class="section-number">6.4.2. </span>ARIMA, Expressed as a State Space Model<a class="headerlink" href="#arima-expressed-as-a-state-space-model" title="Permalink to this heading">#</a></h3>
<p>State space models are a unified methodology that generalized many
classical time series models. However, it might not always be obvious
how we can express a model in state space format. In this section we
will look at how to express a more complex linear Gaussian state space
model: ARMA and ARIMA. Recall the ARMA(p,q) Equation <a class="reference internal" href="#equation-eq-arma">(6.8)</a> from
above, we have the AR coefficient parameters <span class="math notranslate nohighlight">\(\phi_i\)</span>, the MA
coefficient <span class="math notranslate nohighlight">\(\theta_j\)</span>, and noise parameter <span class="math notranslate nohighlight">\(\sigma\)</span>. It is tempting to
use <span class="math notranslate nohighlight">\(\sigma\)</span> to parameterize the observation noise distribution <span class="math notranslate nohighlight">\(R_t\)</span>.
However, the moving average of the noise from the previous steps in the
ARMA(p,q) Equation <a class="reference internal" href="#equation-eq-arma">(6.8)</a> requires us to “record” the current
noise. The only solution is to formulate it into the transition noise so
it becomes part of the latent state <span class="math notranslate nohighlight">\(X_t\)</span>. First, we reformulate
ARMA(p,q) Equation <a class="reference internal" href="#equation-eq-arma">(6.8)</a> into:</p>
<div class="math notranslate nohighlight" id="equation-eq-arma-pre-lgssm">
<span class="eqno">(6.18)<a class="headerlink" href="#equation-eq-arma-pre-lgssm" title="Permalink to this equation">#</a></span>\[y_t = \sum_{i=1}^{r}\phi_i y_{t-i} + \sum_{i=1}^{r-1}\theta_i \epsilon_{t-i} + \epsilon_t\]</div>
<p>where the constant term <span class="math notranslate nohighlight">\(\alpha\)</span> from Equation <a class="reference internal" href="#equation-eq-arma">(6.8)</a> is omitted,
and <span class="math notranslate nohighlight">\(r = max(p, q+1)\)</span>. We pad zeros to coefficient parameters <span class="math notranslate nohighlight">\(\phi\)</span> and
<span class="math notranslate nohighlight">\(\theta\)</span> when needed so that they have the same size <span class="math notranslate nohighlight">\(r\)</span>. The component
of the state equation for <span class="math notranslate nohighlight">\(X_t\)</span> is thus:</p>
<div class="math notranslate nohighlight" id="equation-eq-arma-lgssm-state-fn">
<span class="eqno">(6.19)<a class="headerlink" href="#equation-eq-arma-lgssm-state-fn" title="Permalink to this equation">#</a></span>\[\begin{split}\mathbf{F}_t = \mathbf{F} = \left[\begin{array}{cccc}
\phi_1 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
\phi_{r-1} &amp; 0 &amp; \cdots &amp; 1  \\
\phi_r &amp; 0 &amp; \cdots &amp; 0 
\end{array}\right], \\
\mathbf{A} = \left[\begin{array}{c}
1\\
\theta_1 \\
\vdots \\
\theta_{r-1} \\
\end{array}\right],
\eta'_{t+1} \sim \mathcal{N}(0, \sigma^2), \eta_t = \mathbf{A} \eta'_{t+1}\end{split}\]</div>
<p>With the latent state being:</p>
<div class="math notranslate nohighlight" id="equation-eq-arma-lgssm-state">
<span class="eqno">(6.20)<a class="headerlink" href="#equation-eq-arma-lgssm-state" title="Permalink to this equation">#</a></span>\[\begin{split}X_t = \left[\begin{array}{ccc}
y_t \\
\phi_2 y_{t-1} + \dots + \phi_r y_{t-r+1} + \theta_1 \eta'_t + \dots + \theta_{r-1} \eta'_{t-r+2} \\
\phi_3 y_{t-1} + \dots + \phi_r y_{t-r+2} + \theta_2 \eta'_t + \dots + \theta_{r-1} \eta'_{t-r+3} \\
\vdots \\
\phi_r y_{t-1} + \theta_{r-1} \eta'_t
\end{array}\right]\end{split}\]</div>
<p>The observation operator is thus simply an indexing matrix
<span class="math notranslate nohighlight">\(\mathbf{H}_t = [1, 0, 0, \dots, 0]\)</span> with the observation equation being
<span class="math notranslate nohighlight">\(y_t = \mathbf{H}_t X_t\)</span> <a class="footnote-reference brackets" href="#id65" id="id32" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>.</p>
<p>For example, an ARMA(2,1) model in state space representation is:</p>
<div class="math notranslate nohighlight" id="equation-eq-arma-lgssm-state-full">
<span class="eqno">(6.21)<a class="headerlink" href="#equation-eq-arma-lgssm-state-full" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
\left[\begin{array}{ccc}
y_{t+1}\\
\phi_2 y_t + \theta_1 \eta'_{t+1}\\
\end{array}\right] &amp; =  
\left[\begin{array}{ccc}
\phi_1 &amp; 1\\
\phi_2 &amp; 0\\
\end{array}\right]
\left[\begin{array}{ccc}
y_t\\
\phi_2 y_{t-1} + \theta_1 \eta'_t\\
\end{array}\right] + \left[\begin{array}{ccc}
1\\
\theta_1\\
\end{array}\right] \eta'_{t+1}\\
\eta'_{t+1} &amp; \sim \mathcal{N}(0, \sigma^2)
\end{split}\end{split}\]</div>
<p>You might notice that the state transition is slightly different than
what we defined above, as the transition noise is not drawn from a
Multivariate Gaussian distribution. The covariance matrix of <span class="math notranslate nohighlight">\(\eta\)</span> is
<span class="math notranslate nohighlight">\(\mathbf{Q}_t = \mathbf{A} \sigma^2 \mathbf{A}^T\)</span>, which in this case
results in a singular random variable <span class="math notranslate nohighlight">\(\eta\)</span>. Nonetheless, we can define
the model in TFP. For example, in Code Block
<a class="reference internal" href="#tfd-lgssm-arma-simulate"><span class="std std-ref">tfd_lgssm_arma_simulate</span></a> we
defines a ARMA(2,1) model with <span class="math notranslate nohighlight">\(\phi = [-0.1, 0.5]\)</span>, <span class="math notranslate nohighlight">\(\theta = -0.25\)</span>,
and <span class="math notranslate nohighlight">\(\sigma = 1.25\)</span>, and draw one random time series.</p>
<div class="literal-block-wrapper docutils container" id="tfd-lgssm-arma-simulate">
<div class="code-block-caption"><span class="caption-number">Listing 6.22 </span><span class="caption-text">tfd_lgssm_arma_simulate</span><a class="headerlink" href="#tfd-lgssm-arma-simulate" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_timesteps</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">phi1</span> <span class="o">=</span> <span class="o">-</span><span class="mf">.1</span>
<span class="n">phi2</span> <span class="o">=</span> <span class="mf">.5</span>
<span class="n">theta1</span> <span class="o">=</span> <span class="o">-</span><span class="mf">.25</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.25</span>

<span class="c1"># X_0</span>
<span class="n">initial_state_prior</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
   <span class="n">scale_diag</span><span class="o">=</span><span class="p">[</span><span class="n">sigma</span><span class="p">,</span> <span class="n">sigma</span><span class="p">])</span>
<span class="c1"># F_t</span>
<span class="n">transition_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperatorFullMatrix</span><span class="p">(</span>
   <span class="p">[[</span><span class="n">phi1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">phi2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="c1"># eta_t ~ Normal(0, Q_t)</span>
<span class="n">R_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([[</span><span class="n">sigma</span><span class="p">],</span> <span class="p">[</span><span class="n">sigma</span><span class="o">*</span><span class="n">theta1</span><span class="p">]])</span>
<span class="n">Q_t_tril</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">R_t</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">R_t</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">transition_noise</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span>
   <span class="n">scale_tril</span><span class="o">=</span><span class="n">Q_t_tril</span><span class="p">)</span>
<span class="c1"># H_t</span>
<span class="n">observation_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperatorFullMatrix</span><span class="p">(</span>
   <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="c1"># epsilon_t ~ Normal(0, 0)</span>
<span class="n">observation_noise</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
   <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">],</span> <span class="n">scale_diag</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">])</span>

<span class="n">arma</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">LinearGaussianStateSpaceModel</span><span class="p">(</span>
   <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num_timesteps</span><span class="p">,</span>
   <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">,</span>
   <span class="n">transition_noise</span><span class="o">=</span><span class="n">transition_noise</span><span class="p">,</span>
   <span class="n">observation_matrix</span><span class="o">=</span><span class="n">observation_matrix</span><span class="p">,</span>
   <span class="n">observation_noise</span><span class="o">=</span><span class="n">observation_noise</span><span class="p">,</span>
   <span class="n">initial_state_prior</span><span class="o">=</span><span class="n">initial_state_prior</span>
   <span class="p">)</span>

<span class="n">sim_ts</span> <span class="o">=</span> <span class="n">arma</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Simulate from the model</span>
</pre></div>
</div>
</div>
<p>Adding the appropriate prior and some small rewrite to handle the shape
a bit better, we can get a full generative ARMA(2,1) model in Code Block
<a class="reference internal" href="#tfd-lgssm-arma-with-prior"><span class="std std-ref">tfd_lgssm_arma_with_prior</span></a>.
Conditioning on the (simulated) data <code class="docutils literal notranslate"><span class="pre">sim_ts</span></code> and running inference are
straightforward since we are working with a
<code class="docutils literal notranslate"><span class="pre">tfd.JointDistributionCoroutine</span></code> model. Note that the unknown parameters
are not part of the latent state <span class="math notranslate nohighlight">\(X_t\)</span>, thus instead of a Bayesian
filter like Kalman filter, inference is done using standard MCMC method.
We show the resulting trace plot of the posterior samples in
<a class="reference internal" href="#fig-fig17-arma-lgssm-inference-result"><span class="std std-numref">Fig. 6.17</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="tfd-lgssm-arma-with-prior">
<div class="code-block-caption"><span class="caption-number">Listing 6.23 </span><span class="caption-text">tfd_lgssm_arma_with_prior</span><a class="headerlink" href="#tfd-lgssm-arma-with-prior" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tfd</span><span class="o">.</span><span class="n">JointDistributionCoroutine</span>
<span class="k">def</span> <span class="nf">arma_lgssm</span><span class="p">():</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">HalfStudentT</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"sigma"</span><span class="p">))</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"phi"</span><span class="p">))</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">root</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span><span class="n">tfd</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">"theta"</span><span class="p">))</span>
    <span class="c1"># Prior for initial state</span>
    <span class="n">init_scale_diag</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">initial_state_prior</span> <span class="o">=</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
        <span class="n">scale_diag</span><span class="o">=</span><span class="n">init_scale_diag</span><span class="p">)</span>
    
    <span class="n">F_t</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">phi</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span>
                     <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">]),</span>
                                <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">phi</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])],</span>
                               <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span>
                    <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">transition_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperatorFullMatrix</span><span class="p">(</span><span class="n">F_t</span><span class="p">)</span>
    
    <span class="n">transition_scale_tril</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">scale_tril</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">transition_scale_tril</span><span class="p">,</span>
         <span class="n">tf</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">transition_scale_tril</span><span class="p">)],</span>
        <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">transition_noise</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalTriL</span><span class="p">(</span>
        <span class="n">scale_tril</span><span class="o">=</span><span class="n">scale_tril</span><span class="p">)</span>
    
    <span class="n">observation_matrix</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinearOperatorFullMatrix</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
    <span class="n">observation_noise</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tfd</span><span class="o">.</span><span class="n">MultivariateNormalDiag</span><span class="p">(</span>
        <span class="n">loc</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale_diag</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">])</span>

    <span class="n">arma</span> <span class="o">=</span> <span class="k">yield</span> <span class="n">tfd</span><span class="o">.</span><span class="n">LinearGaussianStateSpaceModel</span><span class="p">(</span>
            <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num_timesteps</span><span class="p">,</span>
            <span class="n">transition_matrix</span><span class="o">=</span><span class="n">transition_matrix</span><span class="p">,</span>
            <span class="n">transition_noise</span><span class="o">=</span><span class="n">transition_noise</span><span class="p">,</span>
            <span class="n">observation_matrix</span><span class="o">=</span><span class="n">observation_matrix</span><span class="p">,</span>
            <span class="n">observation_noise</span><span class="o">=</span><span class="n">observation_noise</span><span class="p">,</span>
            <span class="n">initial_state_prior</span><span class="o">=</span><span class="n">initial_state_prior</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">"arma"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig17-arma-lgssm-inference-result">
<a class="reference internal image-reference" href="../_images/fig17_arma_lgssm_inference_result.png"><img alt="../_images/fig17_arma_lgssm_inference_result.png" src="../_images/fig17_arma_lgssm_inference_result.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.17 </span><span class="caption-text">MCMC sampling result from the ARMA(2,1) model <code class="docutils literal notranslate"><span class="pre">arma_lgssm</span></code> defined in
Code Block
<a class="reference internal" href="#tfd-lgssm-arma-with-prior"><span class="std std-ref">tfd_lgssm_arma_with_prior</span></a>,
conditioned on the simulated data <code class="docutils literal notranslate"><span class="pre">sim_ts</span></code> generated in Code Block
<a class="reference internal" href="#tfd-lgssm-arma-simulate"><span class="std std-ref">tfd_lgssm_arma_simulate</span></a>. The
true values of the parameters are plotted as vertical lines in the
posterior density plot and horizontal lines in the trace plot.</span><a class="headerlink" href="#fig-fig17-arma-lgssm-inference-result" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We can already use this formulation for ARIMA modeling with <span class="math notranslate nohighlight">\(d&gt;0\)</span> by
preprocessing the observed time series to account for the integrated
part. However, state space model representation gives us an advantage
where we can write down the generative process directly and more
intuitively without taking the repeated differences <span class="math notranslate nohighlight">\(d\)</span> times on the
observation in the data preprocessing step.</p>
<p>For example, consider extending the ARMA(2,1) model above with <span class="math notranslate nohighlight">\(d=1\)</span>, we
have <span class="math notranslate nohighlight">\(\Delta y_t = y_t - y_{t-1}\)</span>, which means
<span class="math notranslate nohighlight">\(y_t = y_{t-1} + \Delta y_t\)</span> and we can define observation operator as
<span class="math notranslate nohighlight">\(\mathbf{H}_t = [1, 1, 0]\)</span>, with the latent state <span class="math notranslate nohighlight">\(X_t\)</span> and state
transition being:</p>
<div class="math notranslate nohighlight" id="equation-eq-arima-lgssm-state-transition">
<span class="eqno">(6.22)<a class="headerlink" href="#equation-eq-arima-lgssm-state-transition" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
\left[\begin{array}{ccc}
y_{t-1} + \Delta y_t \\
\phi_1 \Delta y_t + \phi_2 \Delta y_{t-1} + \eta'_{t+1} + \theta_1 \eta'_t\\
\phi_2 \Delta y_t + \theta_1 \eta'_{t+1}\\
\end{array}\right] &amp; =  
\left[\begin{array}{ccc}
1 &amp; 1 &amp; 0 \\
0 &amp; \phi_1 &amp; 1\\
0 &amp; \phi_2 &amp; 0\\
\end{array}\right]
\left[\begin{array}{ccc}
y_{t-1}\\
\Delta y_t \\
\phi_2 \Delta y_{t-1} + \theta_1 \eta'_t\\
\end{array}\right] + \left[\begin{array}{ccc}
0 \\
1 \\
\theta_1\\
\end{array}\right] \eta'_{t+1}
\end{split}\end{split}\]</div>
<p>As you can see, while the parameterization results in a larger size
latent state vector <span class="math notranslate nohighlight">\(X_t\)</span>, the number of parameters stays the same.
Moreover, the model is generative in <span class="math notranslate nohighlight">\(y_t\)</span> instead of <span class="math notranslate nohighlight">\(\Delta y_t\)</span>.
However, challenges may arise when specifying the distribution of the
initial state <span class="math notranslate nohighlight">\(X_0\)</span>, as the first elements (<span class="math notranslate nohighlight">\(y_0\)</span>) are now
non-stationary. In practice, we can assign an informative prior around
the initial value of the time series after centering (subtracting the
mean). More discussion around this topic and an in depth introduction to
state space models for time series problems could be found in <span id="id33">Durbin and Koopman [<a class="reference internal" href="references.html#id131" title="James Durbin and Siem Jan Koopman. Time series analysis by state space methods. Oxford university press, 2012.">61</a>]</span>.</p>
</section>
<section id="bayesian-structural-time-series">
<span id="id34"></span><h3><span class="section-number">6.4.3. </span>Bayesian Structural Time Series<a class="headerlink" href="#bayesian-structural-time-series" title="Permalink to this heading">#</a></h3>
<p>A linear Gaussian state space representation of a time series model has
another advantage that it is easily extendable, especially with other
linear Gaussian state space models. To combine two models, we follow the
same idea of concatenating two normal random variables in the latent
space. We generate a block diagonal matrix using the 2 covariance
matrix, concatenating the mean on the event axis. In the measurement
space the operation is equivalent to summing two normal random
variables. More concretely, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-combining-lgssm">
<span class="eqno">(6.23)<a class="headerlink" href="#equation-eq-combining-lgssm" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
\mathbf{F}_t &amp; = \left[\begin{array}{ccc}
\mathbf{F}_{\mathbf{1}, t} &amp; 0 \\
0 &amp; \mathbf{F}_{\mathbf{2}, t}\\
\end{array}\right], 
\mathbf{Q}_t = \left[\begin{array}{ccc}
\mathbf{Q}_{\mathbf{1}, t} &amp; 0 \\
0 &amp; \mathbf{Q}_{\mathbf{2}, t}\\
\end{array}\right],
X_t = \left[\begin{array}{ccc}
X_{1,t} \\
X_{2,t}\\
\end{array}\right] \\
\mathbf{H}_t &amp; = \left[\begin{array}{ccc}
\mathbf{H}_{\mathbf{1}, t} &amp; \mathbf{H}_{\mathbf{2}, t} \\
\end{array}\right],
\mathbf{R}_t = \mathbf{R}_{\mathbf{1}, t} + \mathbf{R}_{\mathbf{2}, t}\\
\end{split}\end{split}\]</div>
<p>If we have a time series model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> that is not linear
Gaussian. We can also incorporate it into a state space model. To do
that, we treat the prediction <span class="math notranslate nohighlight">\(\hat{\psi}_t\)</span> from <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> at each
time step as a static “known” value and add to the observation noise
distribution <span class="math notranslate nohighlight">\(\epsilon_t \sim N(\hat{\mu}_t + \hat{\psi}_t, R_t)\)</span>.
Conceptually we can understand it as subtracting the prediction of
<span class="math notranslate nohighlight">\(\mathcal{M}\)</span> from <span class="math notranslate nohighlight">\(Y_t\)</span> and modeling the result, so that the Kalman
filter and other linear Gaussian state space model properties still
hold.</p>
<p>This <em>composability</em> feature makes it easy to build a time series model
that is constructed from multiple smaller linear Gaussian state space
model components. We can have individual state space representations for
the trend, seasonal, and error terms, and combine them into what is
usually referred to as a <em>structural time series</em> model or dynamic
linear model. TFP provides a very convenient way to build Bayesian
structural time series with the <code class="docutils literal notranslate"><span class="pre">tfp.sts</span></code> module, along with helper
functions to deconstruct the components, make forecasts, inference, and
other diagnostics.</p>
<p>For example, we can model the monthly birth data using a structural time
series with a local linear trend component and a seasonal component to
account for the monthly pattern in Code Block
<a class="reference internal" href="#tfp-sts-example2"><span class="std std-ref">tfp_sts_example2</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="tfp-sts-example2">
<div class="code-block-caption"><span class="caption-number">Listing 6.24 </span><span class="caption-text">tfp_sts_example2</span><a class="headerlink" href="#tfp-sts-example2" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_bsts_model</span><span class="p">(</span><span class="n">observed</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Args:</span>
<span class="sd">        observed: Observed time series, tfp.sts use it to generate prior.</span>
<span class="sd">    """</span>
    <span class="c1"># Trend</span>
    <span class="n">trend</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">sts</span><span class="o">.</span><span class="n">LocalLinearTrend</span><span class="p">(</span><span class="n">observed_time_series</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>
    <span class="c1"># Seasonal</span>
    <span class="n">seasonal</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">sts</span><span class="o">.</span><span class="n">Seasonal</span><span class="p">(</span><span class="n">num_seasons</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">observed_time_series</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>
    <span class="c1"># Full model</span>
    <span class="k">return</span> <span class="n">tfp</span><span class="o">.</span><span class="n">sts</span><span class="o">.</span><span class="n">Sum</span><span class="p">([</span><span class="n">trend</span><span class="p">,</span> <span class="n">seasonal</span><span class="p">],</span> <span class="n">observed_time_series</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>

<span class="n">observed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">us_monthly_birth</span><span class="p">[</span><span class="s2">"birth_in_thousands"</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">birth_model</span> <span class="o">=</span> <span class="n">generate_bsts_model</span><span class="p">(</span><span class="n">observed</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>

<span class="c1"># Generate the posterior distribution conditioned on the observed</span>
<span class="n">target_log_prob_fn</span> <span class="o">=</span> <span class="n">birth_model</span><span class="o">.</span><span class="n">joint_log_prob</span><span class="p">(</span><span class="n">observed_time_series</span><span class="o">=</span><span class="n">observed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can inspect each component in <code class="docutils literal notranslate"><span class="pre">birth_model</span></code>:</p>
<div class="literal-block-wrapper docutils container" id="tfp-sts-model">
<div class="code-block-caption"><span class="caption-number">Listing 6.25 </span><span class="caption-text">tfp_sts_model</span><a class="headerlink" href="#tfp-sts-model" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">birth_model</span><span class="o">.</span><span class="n">components</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[&lt;tensorflow_probability.python.sts.local_linear_trend.LocalLinearTrend at ...&gt;,
 &lt;tensorflow_probability.python.sts.seasonal.Seasonal at ...&gt;]
</pre></div>
</div>
<p>Each of the components is parameterized by some hyperparameters, which
are the unknown parameters that we want to do inference on. They are not
part of the latent state <span class="math notranslate nohighlight">\(X_t\)</span>, but might parameterize the prior that
generates <span class="math notranslate nohighlight">\(X_t\)</span>. For example, we can check the parameters of the
seasonal component:</p>
<div class="literal-block-wrapper docutils container" id="tfp-sts-model-component">
<div class="code-block-caption"><span class="caption-number">Listing 6.26 </span><span class="caption-text">tfp_sts_model_component</span><a class="headerlink" href="#tfp-sts-model-component" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">birth_model</span><span class="o">.</span><span class="n">components</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[Parameter(name='drift_scale', prior=&lt;tfp.distributions.LogNormal 
'Seasonal_LogNormal' batch_shape=[] event_shape=[] dtype=float32&gt;,
bijector=&lt;tensorflow_probability.python.bijectors.chain.Chain object at ...&gt;)]
</pre></div>
</div>
<p>Here the seasonal component of the STS model contains 12 latent states
(one for each month), but the component only contains 1 parameter (the
hyperparameter that parameterized the latent states). You might have
already noticed from examples in the previous session how unknown
parameters are treated differently. In the linear growth model, unknown
parameters are part of the latent state <span class="math notranslate nohighlight">\(X_t\)</span>, in the ARIMA model, the
unknown parameters parameterized <span class="math notranslate nohighlight">\(\mathbf{F}_t\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Q}_t\)</span>. For
the latter case, we cannot use Kalman filter to infer those parameters.
Instead, the latent state is effectively marginalized out but we can
nonetheless recover them after inference by running the Kalman filter
conditioned on the posterior distribution (represented as Monte Carlo
Samples). A conceptual description of the parameterization could be
found in the <a class="reference internal" href="#fig-fig18-bsts-lgssm"><span class="std std-numref">Fig. 6.18</span></a>:</p>
<figure class="align-default" id="fig-fig18-bsts-lgssm">
<a class="reference internal image-reference" href="../_images/fig18_bsts_lgssm.png"><img alt="../_images/fig18_bsts_lgssm.png" src="../_images/fig18_bsts_lgssm.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.18 </span><span class="caption-text">Relationship between Bayesian Structural Time Series (blue box) and
Linear Gaussian State Space Model (red box). The Linear Gaussian State
Space Model shown here is an example containing a local linear trend
component, a seasonal component, and an Autoregressive component.</span><a class="headerlink" href="#fig-fig18-bsts-lgssm" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Thus running inference on a structural time series model could
conceptually be understood as generating a linear Gaussian state space
model from the parameters to be inferred, running the Kalman filter to
obtain the data likelihood, and combining with the prior log-likelihood
conditioned on the current value of the parameters. Unfortunately, the
operation of iterating through each data point is quite computationally
costly (even though Kalman filter is already an extremely efficient
algorithm), thus fitting structural time series may not scale very well
when running long time series.</p>
<p>After running inference on a structural time series model there are some
helpful utility functions from <code class="docutils literal notranslate"><span class="pre">tfp.sts</span></code> we can use to make forecast and
inspect each inferred component with Code Block
<a class="reference internal" href="#tfp-sts-example2-result"><span class="std std-ref">tfp_sts_example2_result</span></a>. The
result is shown in <a class="reference internal" href="#fig-fig19-bsts-lgssm-result"><span class="std std-numref">Fig. 6.19</span></a>.</p>
<div class="literal-block-wrapper docutils container" id="tfp-sts-example2-result">
<div class="code-block-caption"><span class="caption-number">Listing 6.27 </span><span class="caption-text">tfp_sts_example2_result</span><a class="headerlink" href="#tfp-sts-example2-result" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using a subset of posterior samples.</span>
<span class="n">parameter_samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mcmc_samples</span><span class="p">]</span>

<span class="c1"># Get structual compoenent.</span>
<span class="n">component_dists</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">sts</span><span class="o">.</span><span class="n">decompose_by_component</span><span class="p">(</span>
    <span class="n">birth_model</span><span class="p">,</span>
    <span class="n">observed_time_series</span><span class="o">=</span><span class="n">observed</span><span class="p">,</span>
    <span class="n">parameter_samples</span><span class="o">=</span><span class="n">parameter_samples</span><span class="p">)</span>

<span class="c1"># Get forecast for n_steps.</span>
<span class="n">n_steps</span> <span class="o">=</span> <span class="mi">36</span>
<span class="n">forecast_dist</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">sts</span><span class="o">.</span><span class="n">forecast</span><span class="p">(</span>
    <span class="n">birth_model</span><span class="p">,</span>
    <span class="n">observed_time_series</span><span class="o">=</span><span class="n">observed</span><span class="p">,</span>
    <span class="n">parameter_samples</span><span class="o">=</span><span class="n">parameter_samples</span><span class="p">,</span>
    <span class="n">num_steps_forecast</span><span class="o">=</span><span class="n">n_steps</span><span class="p">)</span>
<span class="n">birth_dates</span> <span class="o">=</span> <span class="n">us_monthly_birth</span><span class="o">.</span><span class="n">index</span>
<span class="n">forecast_date</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span>
    <span class="n">start</span><span class="o">=</span><span class="n">birth_dates</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.timedelta64" title="numpy.timedelta64"><span class="n">np</span><span class="o">.</span><span class="n">timedelta64</span></a><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">"M"</span><span class="p">),</span>
    <span class="n">end</span><span class="o">=</span><span class="n">birth_dates</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.timedelta64" title="numpy.timedelta64"><span class="n">np</span><span class="o">.</span><span class="n">timedelta64</span></a><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">n_steps</span><span class="p">,</span> <span class="s2">"M"</span><span class="p">),</span>
    <span class="n">freq</span><span class="o">=</span><span class="s2">"M"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots" title="matplotlib.pyplot.subplots"><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span></a><span class="p">(</span>
    <span class="mi">1</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">component_dists</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">us_monthly_birth</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"observed"</span><span class="p">)</span>

<span class="n">forecast_mean</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html#numpy.squeeze" title="numpy.squeeze"><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">forecast_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">forecast_date</span><span class="p">,</span> <span class="n">forecast_mean</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="s2">"forecast mean"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"C4"</span><span class="p">)</span>

<span class="n">forecast_std</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html#numpy.squeeze" title="numpy.squeeze"><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">forecast_dist</span><span class="o">.</span><span class="n">stddev</span><span class="p">())</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">forecast_date</span><span class="p">,</span>
                <span class="n">forecast_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">forecast_std</span><span class="p">,</span>
                <span class="n">forecast_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">forecast_std</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_color</span><span class="p">(),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax_</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">dist</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">component_dists</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">comp_mean</span><span class="p">,</span> <span class="n">comp_std</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html#numpy.squeeze" title="numpy.squeeze"><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()),</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html#numpy.squeeze" title="numpy.squeeze"><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">dist</span><span class="o">.</span><span class="n">stddev</span><span class="p">())</span>
    <span class="n">line</span> <span class="o">=</span> <span class="n">ax_</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">birth_dates</span><span class="p">,</span> <span class="n">dist</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">ax_</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">birth_dates</span><span class="p">,</span>
                     <span class="n">comp_mean</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">comp_std</span><span class="p">,</span>
                     <span class="n">comp_mean</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">comp_std</span><span class="p">,</span>
                     <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax_</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">key</span><span class="o">.</span><span class="n">name</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<figure class="align-default" id="fig-fig19-bsts-lgssm-result">
<a class="reference internal image-reference" href="../_images/fig19_bsts_lgssm_result.png"><img alt="../_images/fig19_bsts_lgssm_result.png" src="../_images/fig19_bsts_lgssm_result.png" style="width: 8.00in;"/></a>
<figcaption>
<p><span class="caption-number">Fig. 6.19 </span><span class="caption-text">Inference result and forecast of monthly live births in the United
States (1948-1979) using the <code class="docutils literal notranslate"><span class="pre">tfp.sts</span></code> API with Code Block
<a class="reference internal" href="#tfp-sts-example2-result"><span class="std std-ref">tfp_sts_example2_result</span></a>. Top
panel: 36 months forecast; bottom 2 panels: decomposition of the
structural time series.</span><a class="headerlink" href="#fig-fig19-bsts-lgssm-result" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="other-time-series-models">
<span id="id35"></span><h2><span class="section-number">6.5. </span>Other Time Series Models<a class="headerlink" href="#other-time-series-models" title="Permalink to this heading">#</a></h2>
<p>While structural time series and linear Gaussian state space models are
powerful and expressive classes of time series models, they certainly do
not cover all our needs. For example, some interesting extensions
include nonlinear Gaussian state space models, where the transition
function and measurement function are differentiable nonlinear
functions. Extended Kalman filter could be used for inference of <span class="math notranslate nohighlight">\(X_t\)</span>
for these models <span id="id36">[<a class="reference internal" href="references.html#id161" title="Mohinder S Grewal and Angus P Andrews. Kalman filtering: Theory and Practice with MATLAB. John Wiley &amp; Sons, 2014.">62</a>]</span>. There is the Unscented Kalman
filter for inference of non-Gaussian nonlinear models
<span id="id37">[<a class="reference internal" href="references.html#id161" title="Mohinder S Grewal and Angus P Andrews. Kalman filtering: Theory and Practice with MATLAB. John Wiley &amp; Sons, 2014.">62</a>]</span>, and Particle filter as a general filtering approach
for state space models <span id="id38">[<a class="reference internal" href="references.html#id38" title="N. Chopin and O. Papaspiliopoulos. An Introduction to Sequential Monte Carlo. Springer Series in Statistics. Springer International Publishing, 2020. ISBN 9783030478445.">63</a>]</span>.</p>
<p>Another class of widely used time series models is the Hidden Markov
model, which is a state space model with discrete state space. There are
also specialized algorithms for doing inference of these models, for
example, the forward-backward algorithm for computing the marginal
posterior likelihood, and the Viterbi algorithm for computing the
posterior mode.</p>
<p>In addition there are ordinary differential equations (ODE) and
stochastic differential equations (SDE) that are continuous time models.
In <a class="reference internal" href="#table-ts-model-type"><span class="std std-numref">Table 6.2</span></a> we divide the space of models by
their treatment of stochasticity and time. While we are not going into
details of these models, they are well studied subjects with easy to use
implementations in the Python computing ecosystem.</p>
<table class="table" id="table-ts-model-type">
<caption><span class="caption-number">Table 6.2 </span><span class="caption-text">Various time series models categorized by treatment of stochasticity and time</span><a class="headerlink" href="#table-ts-model-type" title="Permalink to this table">#</a></caption>
<tbody>
<tr class="row-odd"><td></td>
<td><p><strong>Deterministic dynamics</strong></p></td>
<td><p><strong>Stochastic dynamics</strong></p></td>
</tr>
<tr class="row-even"><td><p><strong>Discrete time</strong></p></td>
<td><p>automata / discretized ODEs</p></td>
<td><p>state space models</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Continuous time</strong></p></td>
<td><p>ODEs</p></td>
<td><p>SDEs</p></td>
</tr>
</tbody>
</table>
</section>
<section id="model-criticism-and-choosing-priors">
<span id="id39"></span><h2><span class="section-number">6.6. </span>Model Criticism and Choosing Priors<a class="headerlink" href="#model-criticism-and-choosing-priors" title="Permalink to this heading">#</a></h2>
<p>In the seminal time series book by  <span id="id40">Box <em>et al.</em> [<a class="reference internal" href="references.html#id130" title="G.E.P. Box, G.M. Jenkins, and G.C. Reinsel. Time Series Analysis: Forecasting and Control. Wiley Series in Probability and Statistics. Wiley, 2008. ISBN 9780470272848.">57</a>]</span>
<a class="footnote-reference brackets" href="#id66" id="id41" role="doc-noteref"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></a>, they outlined five important practical problems for time series
modeling:</p>
<ul class="simple">
<li><p>Forecasting</p></li>
<li><p>Estimation of Transfer Functions</p></li>
<li><p>Analysis of Effects of Unusual Intervention Events to a System</p></li>
<li><p>Analysis of Multivariate Time Series</p></li>
<li><p>Discrete Control Systems</p></li>
</ul>
<p>In practice, most time series problems aim at performing some sort of
forecasting (or nowcasting where you try to infer at instantaneous time
<span class="math notranslate nohighlight">\(t\)</span> some observed quantity that are not yet available due to delay in
getting the measurements), which sets up a natural model criticism
criteria in time series analysis problems. While we do not have specific
treatment around Bayesian decision theory in this chapter, It is worth
quoting from <span id="id42">West and Harrison [<a class="reference internal" href="references.html#id134" title="M. West and J. Harrison. Bayesian Forecasting and Dynamic Models. Springer Series in Statistics. Springer New York, 2013. ISBN 9781475793659.">59</a>]</span>:</p>
<blockquote>
<div><p>Good modeling demands hard thinking, and good forecasting requires an integrated view of the role of forecasting within decision systems.</p>
</div></blockquote>
<p>In practice, criticism of time series model inference and evaluation of
forecasting should be closely integrated with the decision making
process, especially how uncertainty should be incorporated into
decisions. Nonetheless, forecast performance could be evaluated alone.
Usually this is done by collecting new data or keeping some hold out
dataset as we did in this Chapter for the CO₂ example, and compare the
observation with the forecast using standard metrics. One of a popular
choice is Mean Absolute Percentage Error (MAPE), which simply compute:</p>
<div class="math notranslate nohighlight" id="equation-eq-mape">
<span class="eqno">(6.24)<a class="headerlink" href="#equation-eq-mape" title="Permalink to this equation">#</a></span>\[MAPE = \frac{1}{n} \sum_{i=1}^{n} \frac{|\text{forecast}_i - \text{observed}_i|}{\text{observed}_i}\]</div>
<p>However, there are some known biases of MAPE, for example, large errors
during low value observation periods will significantly impact MAPE.
Also, it is difficult to compare MAPE across multiple time series when
the range of observation differs greatly.</p>
<p>Cross-validation based model evaluation approaches still apply and are
recommended for time series models. However, using LOO for a single time
series will be problematic if the goal is to estimate the predictive
performance for future time points. Simply leaving out one observation
at a time does not respect the temporal structure of the data (or
model). For example, if you remove one point <span class="math notranslate nohighlight">\(t\)</span> and use the rest of the
points for predictions you will be using the points
<span class="math notranslate nohighlight">\(t_{-1}, t_{-2}, ...\)</span> which may be fine as previous observations (up to
some point) inform future ones, but you will be also using points
<span class="math notranslate nohighlight">\(t_{+1}, t_{+2}, ...\)</span>, that is you will be using the future to predict
the past. Thus, we can compute LOO, but the interpretation of the number
will get will be nonsensical and thus misleading. Instead of leaving one
(or some) time points out, we need some form of leave-future-out
cross-validation (LFO-CV, see e.g. <span id="id43">Bürkner <em>et al.</em> [<a class="reference internal" href="references.html#id123" title="Paul-Christian Bürkner, Jonah Gabry, and Aki Vehtari. Approximate leave-future-out cross-validation for bayesian time series models. Journal of Statistical Computation and Simulation, 90(14):2499–2523, 2020.">64</a>]</span>. As a rough sketch,
after initial model inference, to approximate 1-step-ahead predictions
we would iterate over the hold out time series or future observations
and evaluate on the log predictive density, and refit the model
including a specific time point when the Pareto <span class="math notranslate nohighlight">\(k\)</span> estimate exceeds
some threshold <a class="footnote-reference brackets" href="#id67" id="id44" role="doc-noteref"><span class="fn-bracket">[</span>16<span class="fn-bracket">]</span></a>. Thus, LFO-CV does not refer to one particular
prediction task but rather to various possible cross validation
approaches that all involve some form of prediction of future time
points.</p>
<section id="priors-for-time-series-models">
<span id="id45"></span><h3><span class="section-number">6.6.1. </span>Priors for Time Series Models<a class="headerlink" href="#priors-for-time-series-models" title="Permalink to this heading">#</a></h3>
<p>In Section <a class="reference internal" href="#chp4-gam"><span class="std std-ref">Basis Functions and Generalized Additive Model</span></a> we used a regularizing prior, the Laplace prior, for
the slope of the step linear function. As we mentioned this is to
express our prior knowledge that the change in slope is usually small
and close to zero, so that the resulting latent trend is smoother.
Another common use of regularizing priors or sparse priors, is for
modeling holiday or special days effect. Usually each holiday has its
own coefficients, and we want to express a prior that indicates some
holidays could have huge effect on the time series, but most holidays
are just like any other ordinary day. We can formalize this intuition
with a horseshoe prior <span id="id46">[<a class="reference internal" href="references.html#id166" title="Carlos M. Carvalho, Nicholas G. Polson, and James G. Scott. The horseshoe estimator for sparse signals. Biometrika, 97(2):465–480, 2010.">65</a>, <a class="reference internal" href="references.html#id168" title="Juho Piironen, Aki Vehtari, and others. Sparsity information and regularization in the horseshoe and other shrinkage priors. Electronic Journal of Statistics, 11(2):5018–5051, 2017.">66</a>]</span>
as shown in Equation <a class="reference internal" href="#equation-eq-horse-shoe">(6.25)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-horse-shoe">
<span class="eqno">(6.25)<a class="headerlink" href="#equation-eq-horse-shoe" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{split}
\lambda_t^2 \sim&amp; \mathcal{H}\text{C}(1.) \\
\beta_t \sim&amp; \mathcal{N}(0, \lambda_t^2 \tau^2)
\end{split}\end{split}\]</div>
<p>The global parameter <span class="math notranslate nohighlight">\(\tau\)</span> in the horseshoe prior pulls the
coefficients of the holiday effect globally towards zero. Meanwhile, the
heavy tail from the local scales <span class="math notranslate nohighlight">\(\lambda_t\)</span> let some effect break out
from the shrinkage. We can accommodate different levels of sparsity by
changing the value of <span class="math notranslate nohighlight">\(\tau\)</span>: the closer <span class="math notranslate nohighlight">\(\tau\)</span> is to zero the more
shrinkage of the holiday effect <span class="math notranslate nohighlight">\(\beta_t\)</span> to tends to zero, whereas with
a larger <span class="math notranslate nohighlight">\(\tau\)</span> we have a more diffuse prior <span id="id47">[<a class="reference internal" href="references.html#id167" title="Juho Piironen and Aki Vehtari. On the hyperprior choice for the global shrinkage parameter in the horseshoe prior. In Artificial Intelligence and Statistics, 905–913. PMLR, 2017.">67</a>]</span>
<a class="footnote-reference brackets" href="#id68" id="id48" role="doc-noteref"><span class="fn-bracket">[</span>17<span class="fn-bracket">]</span></a>. For example, in Case Study 2 of
<span id="id49">Riutort-Mayol <em>et al.</em> [<a class="reference internal" href="references.html#id169" title="Gabriel Riutort-Mayol, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. Practical hilbert space approximate bayesian gaussian processes for probabilistic programming. arXiv preprint arXiv:2004.11408, 2020.">68</a>]</span> they included a special day effect for each
individual day of a year (366 as the Leap Day is included) and use a
horseshoe prior to regularize it.</p>
<p>Another important consideration of prior for time series model is the
prior for the observation noise. Most time series data are by nature are
non-repeated measures. We simply cannot go back in time and make another
observation under the exact condition (i.e., we cannot quantify the
<strong>aleatoric</strong> uncertainty). This means our model needs information from
the prior to “decide” whether the noise is from measurement or from
latent process (i.e., the <strong>epistemic</strong> uncertainty). For example, in a
time series model with a latent autoregressive component or a local
linear trend model, we can place more informative prior on the
observation noise to regulate it towards a smaller value. This will
“push” the trend or autoregressive component to overfits the underlying
drift pattern and we might have a nicer forecast on the trend (higher
forecast accuracy in the short term). The risk is that we are
overconfident about the underlying trend, which will likely result in a
poor forecast in the long run. In a real world application where time
series are most likely non-stationary, we should be ready to adjust the
prior accordingly.</p>
</section>
</section>
<section id="exercises">
<span id="exercises6"></span><h2><span class="section-number">6.7. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<p><strong>6E1.</strong> As we explained in Box <em>Parsing timestamp to design
matrix</em> above, date information could be formatted into a design matrix
for regression model to account for the periodic pattern in a time
series. Try generating the following design matrix for the year 2021.
Hint: use Code Block <a class="reference internal" href="#timerange-2021"><span class="std std-ref">timerange_2021</span></a> to
generate all time stamps for 2021:</p>
<div class="literal-block-wrapper docutils container" id="timerange-2021">
<div class="code-block-caption"><span class="caption-number">Listing 6.28 </span><span class="caption-text">timerange_2021</span><a class="headerlink" href="#timerange-2021" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">datetime_index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s2">"2021-01-01"</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">"2021-12-31"</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">'D'</span><span class="p">)</span>
</pre></div>
</div>
</div>
<ul class="simple">
<li><p>A design matrix for day of the month effect.</p></li>
<li><p>A design matrix for weekday vs weekend effect.</p></li>
<li><p>Company G pay their employee on the 25th of every month, and if the
25th falls on a weekend, the payday is moved up to the Friday
before. Try to create a design matrix to encode the pay day of 2021.</p></li>
<li><p>A design matrix for the US Federal holiday effect <a class="footnote-reference brackets" href="#id69" id="id50" role="doc-noteref"><span class="fn-bracket">[</span>18<span class="fn-bracket">]</span></a> in 2021.
Create the design matrix so that each holiday has their individual
coefficient.</p></li>
</ul>
<p><strong>6E2.</strong> In the previous exercise , the design matrix for
holiday effect treat each holiday separately. What if we consider all
holiday effects to be the same? What is the shape of the design matrix
if we do so? Reason about how does it affects the fit of the regression
time series model.</p>
<p><strong>6E3.</strong> Fit a linear regression to the
<code class="docutils literal notranslate"><span class="pre">"monthly_mauna_loa_co2.csv"</span></code> dataset:</p>
<ul class="simple">
<li><p>A plain regression with an intercept and slope, using linear time as
predictor.</p></li>
<li><p>A covariate adjusted regression like the square root predictor in
the baby example in Chapter <a class="reference internal" href="chp_04.html#chap3"><span class="std std-ref">4</span></a> Code Block
<a class="reference internal" href="chp_04.html#babies-transformed"><span class="std std-ref">babies_transformed</span></a>.</p></li>
</ul>
<p>Explain what these models are missing compared to Code Block
<a class="reference internal" href="#regression-model-for-timeseries"><span class="std std-ref">regression_model_for_timeseries</span></a>.</p>
<p><strong>6E4.</strong> Explain in your own words the difference between
regression, autoregressive and state space architectures. In which
situation would each be particularly useful.</p>
<p><strong>6M5.</strong> Does using basis function as design matrix actually
have better condition number than sparse matrix? Compare the condition
number of the following design matrix of the same rank using
<code class="docutils literal notranslate"><span class="pre">numpy.linalg.cond</span></code>:</p>
<ul class="simple">
<li><p>Dummy coded design matrix <code class="docutils literal notranslate"><span class="pre">seasonality_all</span></code> from Code Block
<a class="reference internal" href="#generate-design-matrix"><span class="std std-ref">generate_design_matrix</span></a>.</p></li>
<li><p>Fourier basis function design matrix <code class="docutils literal notranslate"><span class="pre">X_pred</span></code> from Code Block
<a class="reference internal" href="#gam"><span class="std std-ref">gam</span></a>.</p></li>
<li><p>An array of the same shape as <code class="docutils literal notranslate"><span class="pre">seasonality_all</span></code> with values drawn
from a Normal distribution.</p></li>
<li><p>An array of the same shape as <code class="docutils literal notranslate"><span class="pre">seasonality_all</span></code> with values drawn
from a Normal distribution <em>and</em> one of the column being identical
to another.</p></li>
</ul>
<p><strong>6M6.</strong> The <code class="docutils literal notranslate"><span class="pre">gen_fourier_basis</span></code> function from Code Block
<a class="reference internal" href="#fourier-basis-as-seasonality"><span class="std std-ref">fourier_basis_as_seasonality</span></a>
takes a time index <code class="docutils literal notranslate"><span class="pre">t</span></code> as the first input. There are a few different
ways to represent the time index, for example, if we are observing some
data monthly from 2019 January for 36 months, we can code the time index
in 2 equivalent ways as shown below in Code Block
<a class="reference internal" href="#exercise-chap4-e6"><span class="std std-ref">exercise_chap4_e6</span></a>:</p>
<div class="literal-block-wrapper docutils container" id="exercise-chap4-e6">
<div class="code-block-caption"><span class="caption-number">Listing 6.29 </span><span class="caption-text">exercise_chap4_e6</span><a class="headerlink" href="#exercise-chap4-e6" title="Permalink to this code">#</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nmonths</span> <span class="o">=</span> <span class="mi">36</span>
<span class="n">day0</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="p">(</span><span class="s1">'2019-01-01'</span><span class="p">)</span>
<span class="n">time_index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span>
    <span class="n">start</span><span class="o">=</span><span class="n">day0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">day0</span> <span class="o">+</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.timedelta64" title="numpy.timedelta64"><span class="n">np</span><span class="o">.</span><span class="n">timedelta64</span></a><span class="p">(</span><span class="n">nmonths</span><span class="p">,</span> <span class="s1">'M'</span><span class="p">),</span> 
    <span class="n">freq</span><span class="o">=</span><span class="s1">'M'</span><span class="p">)</span>

<span class="n">t0</span> <span class="o">=</span> <a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.arange.html#numpy.arange" title="numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">time_index</span><span class="p">))</span>
<span class="n">design_matrix0</span> <span class="o">=</span> <span class="n">gen_fourier_basis</span><span class="p">(</span><span class="n">t0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="n">t1</span> <span class="o">=</span> <span class="n">time_index</span><span class="o">.</span><span class="n">month</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">design_matrix1</span> <span class="o">=</span> <span class="n">gen_fourier_basis</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<a class="sphinx-codeautolink-a" href="https://numpy.org/doc/stable/reference/generated/numpy.testing.assert_array_almost_equal.html#numpy.testing.assert_array_almost_equal" title="numpy.testing.assert_array_almost_equal"><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_array_almost_equal</span></a><span class="p">(</span><span class="n">design_matrix0</span><span class="p">,</span> <span class="n">design_matrix1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>What if we are observing the data daily? How would you change the Code
Block <a class="reference internal" href="#exercise-chap4-e6"><span class="std std-ref">exercise_chap4_e6</span></a> to:</p>
<ul class="simple">
<li><p>Make <code class="docutils literal notranslate"><span class="pre">time_index</span></code> represent day of the year instead of month of the
year.</p></li>
<li><p>Modify the function signature to <code class="docutils literal notranslate"><span class="pre">gen_fourier_basis</span></code> in line 8 and
10 so that the resulting design matrices coded for the month of the
year effect.</p></li>
<li><p>How does the new <code class="docutils literal notranslate"><span class="pre">design_matrix0</span></code> and <code class="docutils literal notranslate"><span class="pre">design_matrix1</span></code> differ? How
is the differences would impact the model fitting? Hint: validate
your reasoning by multiplying them with the same random regression
coefficient.</p></li>
</ul>
<p><strong>6E7.</strong> In Section <a class="reference internal" href="#chap4-ar"><span class="std std-ref">Autoregressive Models</span></a> we introduced the backshift
operator <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>. You might have already noticed that applying the
operation <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> on a time series is the same as performing a
matrix multiplication. We can generate a matrix <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> explicitly
in Python. Modify Code Block
<a class="reference internal" href="#ar1-without-forloop"><span class="std std-ref">ar1_without_forloop</span></a> to use an
explicit <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> constructed in NumPy or TensorFlow.</p>
<p><strong>6E8.</strong> The step linear function as defined in Equation
<a class="reference internal" href="#equation-eq-step-linear-function">(6.3)</a> and Code Block
<a class="reference internal" href="#step-linear-function-for-trend"><span class="std std-ref">step_linear_function_for_trend</span></a>
rely on a key regression coefficient <span class="math notranslate nohighlight">\(\delta\)</span>. Rewrite the definition so
that it has a similar form compare to other linear regression:</p>
<div class="math notranslate nohighlight" id="equation-eq-step-linear">
<span class="eqno">(6.26)<a class="headerlink" href="#equation-eq-step-linear" title="Permalink to this equation">#</a></span>\[g(t) = \mathbf{A}^\prime \delta^\prime\]</div>
<p>Find the appropriate expression of design matrix <span class="math notranslate nohighlight">\(\mathbf{A}^\prime\)</span> and
coefficient <span class="math notranslate nohighlight">\(\delta^\prime\)</span>.</p>
<p><strong>6E9.</strong> As we have seen in past chapters, a great way to
understand your data generating process is to write it down. In this
exercise we will generate synthetic data which will reinforce the
mapping of “real world” ideas to code. Assume we start with a linear
trend that is <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">2x,</span> <span class="pre">x</span> <span class="pre">=</span> <span class="pre">np.arange(90)</span></code>, and iid noise at each time
point draw from a <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span>. Assume that this time series
starts on Sunday June 6 2021. Generate 4 synthetic datasets that
include:</p>
<ol class="arabic simple">
<li><p>An additive weekend effect where weekends have 2x more volume than
weekdays.</p></li>
<li><p>An additive sinusoidal effect of sin(2x).</p></li>
<li><p>An additive AR(1) latent process with autoregressive coefficient of
your choice and a noise scale <span class="math notranslate nohighlight">\(\sigma = 0.2\)</span>.</p></li>
<li><p>A time series with weekend and sinusoidal effect from (1) and (2),
and an AR(1) process on the mean of the time series with the same
autoregressive coefficient as in (3)</p></li>
</ol>
<p><strong>6E10.</strong> Adapt the model in Code Block
<a class="reference internal" href="#gam-with-ar-likelihood"><span class="std std-ref">gam_with_ar_likelihood</span></a> to model
the generated time series in <strong>6E9</strong> (4).</p>
<p><strong>6E11.</strong> Inspection of the inference result (MCMC trace and
diagnostic) of models in this chapter using <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code>. For example, look
at:</p>
<ul class="simple">
<li><p>Trace plot</p></li>
<li><p>Rank plot</p></li>
<li><p>Summary of posterior sample</p></li>
</ul>
<p>Which model contains problematic chains (divergence, low ESS, large
<span class="math notranslate nohighlight">\(\hat R\)</span>)? Could you find ways to improve the inference for those
models?</p>
<p><strong>6M12.</strong> Generate a sinusoidal time series with 200 time
points in Python, and fit it with a AR(2) model. Do that in TFP by
modifying Code Block
<a class="reference internal" href="#ar1-without-forloop"><span class="std std-ref">ar1_without_forloop</span></a> and in PyMC3
with <code class="docutils literal notranslate"><span class="pre">pm.AR</span></code> API.</p>
<p><strong>6M13.</strong> This is an exercise of posterior predictive check
for AR models. Generate the prediction distribution at each time step
<span class="math notranslate nohighlight">\(t\)</span> for the AR2 model in Exercise <strong>6M11</strong>. Note that for
each time step <span class="math notranslate nohighlight">\(t\)</span> you need to condition on all the observations up to
time step <span class="math notranslate nohighlight">\(t-1\)</span>. Does the one-step-ahead predictive distribution match
the observed time series?</p>
<p><strong>6M14.</strong> Make forecast for 50 time steps using the AR2
models from Exercise <strong>6M11</strong>. Does the forecast also look
like a sinusoidal signal?</p>
<p><strong>6H15.</strong> Implement the generative process for the
<span class="math notranslate nohighlight">\(\text{SARIMA}(1, 1, 1)(1, 1, 1)_{12}\)</span> model, and make forecast.</p>
<p><strong>6M16.</strong> Implement and inference a <span class="math notranslate nohighlight">\(ARIMAX(1,1,1)X[4]\)</span> model
for the monthly birth dataset in this chapter, with the design matrix
generated from a Fourier basis functions with <span class="math notranslate nohighlight">\(N=2\)</span>.</p>
<p><strong>6H17.</strong> Derive the Kalman filter equations. Hint: first
work out the joint distribution of <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(X_{t-1}\)</span>, and then follow
with the joint distribution of <span class="math notranslate nohighlight">\(Y_t\)</span> and <span class="math notranslate nohighlight">\(X_t\)</span>. If you are still stuck
take at look at Chapter 4 in Särkkä’s book <span id="id51">[<a class="reference internal" href="references.html#id129" title="S. Särkkä. Bayesian Filtering and Smoothing. Bayesian Filtering and Smoothing. Cambridge University Press, 2013. ISBN 9781107030657.">60</a>]</span>.</p>
<p><strong>6M18.</strong> Inspect the output of
<code class="docutils literal notranslate"><span class="pre">linear_growth_model.forward_filter</span></code> by indexing to a given time step:</p>
<ul class="simple">
<li><p>Identify the input and output of one Kalman filter step;</p></li>
<li><p>Compute one step of the Kalman filter predict and update step using
the input;</p></li>
<li><p>Assert that your computation is the same as the indexed output.</p></li>
</ul>
<p><strong>6M19.</strong> Study the documentation and implementation of
<code class="docutils literal notranslate"><span class="pre">tfp.sts.Seasonal</span></code>, and answer the following questions:</p>
<ul class="simple">
<li><p>How many hyperparameters does a seasonal SSM contains?</p></li>
<li><p>How does it parameterized the latent states and what kind of
regularization effect does the prior has? Hint: draw connection to
the Gaussian Random Walk prior in Chapter <a class="reference internal" href="chp_05.html#chap3-5"><span class="std std-ref">5</span></a>.</p></li>
</ul>
<p><strong>6M20.</strong> Study the documentation and implementation of
<code class="docutils literal notranslate"><span class="pre">tfp.sts.LinearRegression</span></code> and <code class="docutils literal notranslate"><span class="pre">tfp.sts.Seasonal</span></code>, and reason about the
differences of SSM they represent when modeling a day of the week
pattern:</p>
<ul class="simple">
<li><p>How is the day of the week coefficient represented? Are they part of
the latent states?</p></li>
<li><p>How is the model fit different between the two SSMs? Validate your
reasoning with simulations.</p></li>
</ul>
<hr class="footnotes docutils"/>
<aside class="footnote brackets" id="id52" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id1" role="doc-backlink">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://quoteinvestigator.com/2013/10/20/no-predict/">https://quoteinvestigator.com/2013/10/20/no-predict/</a></p>
</aside>
<aside class="footnote brackets" id="id53" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id3" role="doc-backlink">2</a><span class="fn-bracket">]</span></span>
<p>There is also a subtlety that not all periodic patterns in the
time series should be considered seasonal. A useful distinction to
make is between cyclic and seasonal behavior. You can find a nice
summary in <a class="reference external" href="https://robjhyndman.com/hyndsight/cyclicts/">https://robjhyndman.com/hyndsight/cyclicts/</a>.</p>
</aside>
<aside class="footnote brackets" id="id54" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id4" role="doc-backlink">3</a><span class="fn-bracket">]</span></span>
<p>This makes the observation not iid and not exchangeable. You can
also see in Chapter <a class="reference internal" href="chp_04.html#chap3"><span class="std std-ref">4</span></a> where we define residuals</p>
</aside>
<aside class="footnote brackets" id="id55" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id7" role="doc-backlink">4</a><span class="fn-bracket">]</span></span>
<p>Which, it is unfortunate for our model and for our planet.</p>
</aside>
<aside class="footnote brackets" id="id56" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id9" role="doc-backlink">5</a><span class="fn-bracket">]</span></span>
<p>A series is stationary if its characteristic properties such as
means and covariances remain invariant across time.</p>
</aside>
<aside class="footnote brackets" id="id57" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id10" role="doc-backlink">6</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://facebook.github.io/prophet/">https://facebook.github.io/prophet/</a>.</p>
</aside>
<aside class="footnote brackets" id="id58" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id15" role="doc-backlink">7</a><span class="fn-bracket">]</span></span>
<p>A demo of the design matrix used in Facebook Prophet could be
found in <a class="reference external" href="http://prophet.mbrouns.com">http://prophet.mbrouns.com</a> from a PyMCon 2020
presentation.</p>
</aside>
<aside class="footnote brackets" id="id59" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id17" role="doc-backlink">8</a><span class="fn-bracket">]</span></span>
<p>That is why is called autoregressive, it applies a linear
regression to itself. Hence the similar naming to the
autocorrelation diagnostic introduced in Section <a class="reference internal" href="chp_02.html#autocorr-plot"><span class="std std-ref">Autocorrelation Plots</span></a>.</p>
</aside>
<aside class="footnote brackets" id="id60" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id20" role="doc-backlink">9</a><span class="fn-bracket">]</span></span>
<p>Actually, the AR example in this section <em>is</em> a Gaussian Process.</p>
</aside>
<aside class="footnote brackets" id="id61" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id24" role="doc-backlink">10</a><span class="fn-bracket">]</span></span>
<p>The Stan implementation of SARIMA can be found in e.g.
<a class="github reference external" href="https://github.com/asael697/bayesforecast">asael697/bayesforecast</a>.</p>
</aside>
<aside class="footnote brackets" id="id62" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id25" role="doc-backlink">11</a><span class="fn-bracket">]</span></span>
<p>For brevity, we omitted the MCMC sampling code here. You can find
the details in the accompanying Jupyter Notebook.</p>
</aside>
<aside class="footnote brackets" id="id63" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id27" role="doc-backlink">12</a><span class="fn-bracket">]</span></span>
<p>It might be useful to first consider “space” here being some
multi-dimensional Euclidean spaces, so <span class="math notranslate nohighlight">\(X_t\)</span> and <span class="math notranslate nohighlight">\(Y_t\)</span> is some
multi-dimensional array/tensor when we do computations in Python.</p>
</aside>
<aside class="footnote brackets" id="id64" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id30" role="doc-backlink">13</a><span class="fn-bracket">]</span></span>
<p>This also gives a nice example of a non-stationary observation
matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>.</p>
</aside>
<aside class="footnote brackets" id="id65" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id32" role="doc-backlink">14</a><span class="fn-bracket">]</span></span>
<p>Note that this is not the only way to express ARMA model in a
state-space form, for more detail see lecture note
<a class="reference external" href="http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf">http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf</a>.</p>
</aside>
<aside class="footnote brackets" id="id66" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id41" role="doc-backlink">15</a><span class="fn-bracket">]</span></span>
<p>Nothing more puts George E. P. Box’s famous quote: “All models
are wrong, but some are useful” into perspective, than reading
through his seminal book and working on forecasting problems.</p>
</aside>
<aside class="footnote brackets" id="id67" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id44" role="doc-backlink">16</a><span class="fn-bracket">]</span></span>
<p>For a demonstration see
<a class="reference external" href="https://mc-stan.org/loo/articles/loo2-lfo.html">https://mc-stan.org/loo/articles/loo2-lfo.html</a>.</p>
</aside>
<aside class="footnote brackets" id="id68" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id48" role="doc-backlink">17</a><span class="fn-bracket">]</span></span>
<p>Note that in practice we usually parameterize Equation
<a class="reference internal" href="#equation-eq-horse-shoe">(6.25)</a> a little bit differently.</p>
</aside>
<aside class="footnote brackets" id="id69" role="note">
<span class="label"><span class="fn-bracket">[</span><a href="#id50" role="doc-backlink">18</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Federal_holidays_in_the_United_States#List_of_federal_holidays">https://en.wikipedia.org/wiki/Federal_holidays_in_the_United_States#List_of_federal_holidays</a></p>
</aside>
</section>
</section>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</article>
<footer class="prev-next-footer">
<div class="prev-next-area">
<a class="left-prev" href="chp_05.html" title="previous page">
<i class="fa-solid fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title"><span class="section-number">5. </span>Splines</p>
</div>
</a>
<a class="right-next" href="chp_07.html" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title"><span class="section-number">7. </span>Bayesian Additive Regression Trees</p>
</div>
<i class="fa-solid fa-angle-right"></i>
</a>
</div>
</footer>
</div>
<div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">
<div class="sidebar-secondary-item">
<div class="page-toc tocsection onthispage">
<i class="fa-solid fa-list"></i> Contents
  </div>
<nav class="bd-toc-nav page-toc">
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-overview-of-time-series-problems">6.1. An Overview of Time Series Problems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-series-analysis-as-a-regression-problem">6.2. Time Series Analysis as a Regression Problem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#design-matrices-for-time-series">6.2.1. Design Matrices for Time Series</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-functions-and-generalized-additive-model">6.2.2. Basis Functions and Generalized Additive Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-models">6.3. Autoregressive Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#latent-ar-process-and-smoothing">6.3.1. Latent AR Process and Smoothing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#s-ar-i-ma-x">6.3.2. (S)AR(I)MA(X)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space-models">6.4. State Space Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-gaussian-state-space-models-and-kalman-filter">6.4.1. Linear Gaussian State Space Models and Kalman filter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arima-expressed-as-a-state-space-model">6.4.2. ARIMA, Expressed as a State Space Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-structural-time-series">6.4.3. Bayesian Structural Time Series</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-time-series-models">6.5. Other Time Series Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-criticism-and-choosing-priors">6.6. Model Criticism and Choosing Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#priors-for-time-series-models">6.6.1. Priors for Time Series Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">6.7. Exercises</a></li>
</ul>
</nav></div>
</div></div>
</div>
<footer class="bd-footer-content">
<div class="bd-footer-content__inner container">
<div class="footer-item">
<p class="component-author">
By Martin, Kumar, Lao
</p>
</div>
<div class="footer-item">
<p class="copyright">
    
      © Copyright 2022.
      <br/>
</p>
</div>
<div class="footer-item">
</div>
<div class="footer-item">
</div>
</div>
</footer>
</main>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>
<footer class="bd-footer">
</footer>
</body>
</html>