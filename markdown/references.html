
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>13. References &#8212; Bayesian Modeling and Computation in Python</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-codeautolink.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Code 1: Bayesian Inference" href="../notebooks/chp_01.html" />
    <link rel="prev" title="12. Glossary" href="glossary.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-702QMHG8ST"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-702QMHG8ST');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Bayesian Modeling and Computation in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 0
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dedication.html">
   Dedication
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="foreword.html">
   Foreword
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="symbollist.html">
   Symbols
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chp_01.html">
   1. Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_02.html">
   2. Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_03.html">
   3. Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_04.html">
   4. Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_05.html">
   5. Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_06.html">
   6. Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_07.html">
   7. Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_08.html">
   8. Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_09.html">
   9. End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_10.html">
   10. Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chp_11.html">
   11. Appendiceal Topics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   12. Glossary
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   13. References
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Part 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_01.html">
   Code 1: Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_02.html">
   Code 2: Exploratory Analysis of Bayesian Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_03.html">
   Code 3: Linear Models and Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_04.html">
   Code 4: Extending Linear Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_05.html">
   Code 5: Splines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_06.html">
   Code 6: Time Series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_07.html">
   Code 7: Bayesian Additive Regression Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_08.html">
   Code 8: Approximate Bayesian Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_09.html">
   Code 9: End to End Bayesian Workflows
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_10.html">
   Code 10: Probabilistic Programming Languages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks/chp_11.html">
   Code 11: Appendiceal Topics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/BayesianModelingandComputationInPython/BookCode_Edition1/issues/new?title=Issue%20on%20page%20%2Fmarkdown/references.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="references">
<span id="id1"></span><h1><span class="section-number">13. </span>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p id="id2"><dl class="citation">
<dt class="label" id="id124"><span class="brackets">1</span></dt>
<dd><p>John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programming in python using pymc3. <em>PeerJ Computer Science</em>, 2:e55, 2016.</p>
</dd>
<dt class="label" id="id141"><span class="brackets">2</span></dt>
<dd><p>Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. <em>arXiv preprint arXiv:1711.10604</em>, 2017.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">3</span></dt>
<dd><p>Ravin Kumar, Colin Carroll, Ari Hartikainen, and Osvaldo Martin. Arviz a unified library for exploratory analysis of bayesian models in python. <em>Journal of Open Source Software</em>, 4(33):1143, 2019.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">4</span></dt>
<dd><p>P. Westfall and K.S.S. Henning. <em>Understanding Advanced Statistical Methods</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2013. ISBN 9781466512108.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">5</span></dt>
<dd><p>J.K. Blitzstein and J. Hwang. <em>Introduction to Probability, Second Edition</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2019. ISBN 9780429766732.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">6</span></dt>
<dd><p>Wikipedia contributors. Conceptual model — Wikipedia, the free encyclopedia. Page Version ID: 952394363. URL: <a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Conceptual_model&amp;oldid=952394363">https://en.wikipedia.org/w/index.php?title=Conceptual_model&amp;oldid=952394363</a>.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">7</span></dt>
<dd><p>Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller. Equation of state calculations by fast computing machines. <em>The journal of chemical physics</em>, 21(6):1087–1092, 1953.</p>
</dd>
<dt class="label" id="id107"><span class="brackets">8</span></dt>
<dd><p>WK HASTINGS. Monte carlo sampling methods using markov chains and their applications. <em>Biometrika</em>, 57(1):97–109, 1970.</p>
</dd>
<dt class="label" id="id108"><span class="brackets">9</span></dt>
<dd><p>Marshall N Rosenbluth. Genesis of the monte carlo algorithm for statistical mechanics. In <em>AIP Conference Proceedings</em>, volume 690, 22–30. American Institute of Physics, 2003.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">10</span></dt>
<dd><p>R. McElreath. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2020. ISBN 9781482253481.</p>
</dd>
<dt class="label" id="id99"><span class="brackets">11</span></dt>
<dd><p>Daniel Lakens, Federico G. Adolfi, Casper J. Albers, Farid Anvari, Matthew A. J. Apps, Shlomo E. Argamon, Thom Baguley, Raymond B. Becker, Stephen D. Benning, Daniel E. Bradford, Erin M. Buchanan, Aaron R. Caldwell, Ben Van Calster, Rickard Carlsson, Sau-Chin Chen, Bryan Chung, Lincoln J. Colling, Gary S. Collins, Zander Crook, Emily S. Cross, Sameera Daniels, Henrik Danielsson, Lisa DeBruine, Daniel J. Dunleavy, Brian D. Earp, Michele I. Feist, Jason D. Ferrell, James G. Field, Nicholas W. Fox, Amanda Friesen, Caio Gomes, Monica Gonzalez-Marquez, James A. Grange, Andrew P. Grieve, Robert Guggenberger, James Grist, Anne-Laura van Harmelen, Fred Hasselman, Kevin D. Hochard, Mark R. Hoffarth, Nicholas P. Holmes, Michael Ingre, Peder M. Isager, Hanna K. Isotalus, Christer Johansson, Konrad Juszczyk, David A. Kenny, Ahmed A. Khalil, Barbara Konat, Junpeng Lao, Erik Gahner Larsen, Gerine M. A. Lodder, Jiří Lukavský, Christopher R. Madan, David Manheim, Stephen R. Martin, Andrea E. Martin, Deborah G. Mayo, Randy J. McCarthy, Kevin McConway, Colin McFarland, Amanda Q. X. Nio, Gustav Nilsonne, Cilene Lino de Oliveira, Jean-Jacques Orban de Xivry, Sam Parsons, Gerit Pfuhl, Kimberly A. Quinn, John J. Sakon, S. Adil Saribay, Iris K. Schneider, Manojkumar Selvaraju, Zsuzsika Sjoerds, Samuel G. Smith, Tim Smits, Jeffrey R. Spies, Vishnu Sreekumar, Crystal N. Steltenpohl, Neil Stenhouse, Wojciech Swiatkowski, Miguel A. Vadillo, Marcel A. L. M. Van Assen, Matt N. Williams, Samantha E. Williams, Donald R. Williams, Tal Yarkoni, Ignazio Ziano, and Rolf A. Zwaan. Justify your alpha. <em>Nature Human Behaviour</em>, 2(3):168–171, 2018.</p>
</dd>
<dt class="label" id="id100"><span class="brackets">12</span></dt>
<dd><p>David Deming. Do extraordinary claims require extraordinary evidence? <em>Philosophia</em>, 44(4):1319–1331, 2016.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">13</span></dt>
<dd><p>Andrew Gelman, Daniel Simpson, and Michael Betancourt. The prior can often only be understood in the context of the likelihood. <em>Entropy</em>, 19(10):555, 2017.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">14</span></dt>
<dd><p>John W. Tukey. <em>Exploratory Data Analysis</em>. Addison-Wesley, 1977.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">15</span></dt>
<dd><p>Persi Diaconis. <em>Theories of Data Analysis: From Magical Thinking Through Classical Statistics</em>, chapter 1, pages 1–36. John Wiley &amp; Sons, Ltd, 2006.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">16</span></dt>
<dd><p>Jonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. Visualization in bayesian workflow. <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, 182(2):389–402, 2019.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">17</span></dt>
<dd><p>Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, and Martin Modrák. Bayesian workflow. <em>arXiv preprint arXiv:2011.01808</em>, 2020.</p>
</dd>
<dt class="label" id="id3"><span class="brackets">18</span></dt>
<dd><p>A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. <em>Bayesian Data Analysis, Third Edition</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2013. ISBN 9781439840955.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">19</span></dt>
<dd><p>Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. Rank-Normalization, Folding, and Localization: An Improved $\widehat R$ for Assessing Convergence of MCMC. <em>Bayesian Analysis</em>, pages 1 – 38, 2021. URL: <a class="reference external" href="https://doi.org/10.1214/20-BA1221">https://doi.org/10.1214/20-BA1221</a>, <a class="reference external" href="https://doi.org/10.1214/20-BA1221">doi:10.1214/20-BA1221</a>.</p>
</dd>
<dt class="label" id="id171"><span class="brackets">20</span></dt>
<dd><p>Stephan Hoyer and Joe Hamman. Xarray: nd labeled arrays and datasets in python. <em>Journal of Open Research Software</em>, 2017.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">21</span></dt>
<dd><p>Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. <em>Journal of the American statistical Association</em>, 102(477):359–378, 2007.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">22</span></dt>
<dd><p>Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry. Pareto smoothed importance sampling. <em>arXiv preprint arXiv:1507.02646</em>, 2021.</p>
</dd>
<dt class="label" id="id170"><span class="brackets">23</span></dt>
<dd><p>Aki. Vehtari and Jonah. Gabry. Loo glossary. https://mc-stan.org/loo/reference/loo-glossary.html.</p>
</dd>
<dt class="label" id="id147"><span class="brackets">24</span></dt>
<dd><p>Topi Paananen, Juho Piironen, Paul-Christian Bürkner, and Aki Vehtari. Implicitly adaptive importance sampling. <em>Statistics and Computing</em>, 31(2):1–19, 2021.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">25</span></dt>
<dd><p>Jennifer A Hoeting, David Madigan, Adrian E Raftery, and Chris T Volinsky. Bayesian model averaging: a tutorial (with comments by m. clyde, david draper and ei george, and a rejoinder by the authors. <em>Statistical science</em>, 14(4):382–417, 1999.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">26</span></dt>
<dd><p>Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Using stacking to average bayesian predictive distributions (with discussion). <em>Bayesian Analysis</em>, 13(3):917–1007, 2018.</p>
</dd>
<dt class="label" id="id115"><span class="brackets">27</span></dt>
<dd><p>Donald B Rubin. Estimation in parallel randomized experiments. <em>Journal of Educational Statistics</em>, 6(4):377–401, 1981.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">28</span></dt>
<dd><p>Allison Marie Horst, Alison Presmanes Hill, and Kristen B Gorman. <em>palmerpenguins: Palmer Archipelago (Antarctica) penguin data</em>. 2020. R package version 0.1.0. URL: <a class="reference external" href="https://allisonhorst.github.io/palmerpenguins/">https://allisonhorst.github.io/palmerpenguins/</a>, <a class="reference external" href="https://doi.org/10.5281/zenodo.3960218">doi:10.5281/zenodo.3960218</a>.</p>
</dd>
<dt class="label" id="id128"><span class="brackets">29</span></dt>
<dd><p>Dan Piponi, Dave Moore, and Joshua V. Dillon. Joint distributions for tensorflow probability. <em>arXiv preprint arXiv:2001.11819</em>, 2020.</p>
</dd>
<dt class="label" id="id133"><span class="brackets">30</span></dt>
<dd><p>Junpeng Lao, Christopher Suter, Ian Langmore, Cyril Chimisov, Ashish Saxena, Pavel Sountsov, Dave Moore, Rif A Saurous, Matthew D Hoffman, and Joshua V. Dillon. Tfp.mcmc: modern markov chain monte carlo tools built for modern hardware. <em>arXiv preprint arXiv:2002.01184</em>, 2020.</p>
</dd>
<dt class="label" id="id60"><span class="brackets">31</span></dt>
<dd><p>J. Fox. <em>Applied Regression Analysis and Generalized Linear Models</em>. SAGE Publications, 2015. ISBN 9781483321318.</p>
</dd>
<dt class="label" id="id65"><span class="brackets">32</span></dt>
<dd><p>Kristen B Gorman, Tony D Williams, and William R Fraser. Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). <em>PloS one</em>, 9(3):e90081, 2014.</p>
</dd>
<dt class="label" id="id118"><span class="brackets">33</span></dt>
<dd><p>Tomás Capretto, Camen Piho, Ravin Kumar, Jacob Westfall, Tal Yarkoni, and Osvaldo A Martin. Bambi: a simple interface for fitting bayesian linear models in python. <em>arXiv preprint arXiv:2012.10754</em>, 2020.</p>
</dd>
<dt class="label" id="id82"><span class="brackets">34</span></dt>
<dd><p>Douglas Bates, Martin Mächler, Ben Bolker, and Steve Walker. Fitting linear mixed-effects models using lme4. <em>arXiv preprint arXiv:1406.5823</em>, 2014.</p>
</dd>
<dt class="label" id="id81"><span class="brackets">35</span></dt>
<dd><p>Jose Pinheiro, Douglas Bates, Saikat DebRoy, Deepayan Sarkar, and R Core Team. <em>nlme: Linear and Nonlinear Mixed Effects Models</em>. 2020. R package version 3.1-151. URL: <a class="reference external" href="https://CRAN.R-project.org/package=nlme">https://CRAN.R-project.org/package=nlme</a>.</p>
</dd>
<dt class="label" id="id83"><span class="brackets">36</span></dt>
<dd><p>Jonah Gabry and Ben Goodrich. Estimating generalized (non-)linear models with group-specific terms with rstanarm. 6 2020. URL: <a class="reference external" href="https://mc-stan.org/rstanarm/articles/glmer.html">https://mc-stan.org/rstanarm/articles/glmer.html</a>.</p>
</dd>
<dt class="label" id="id84"><span class="brackets">37</span></dt>
<dd><p>Paul-Christian Bürkner. Brms: an r package for bayesian multilevel models using stan. <em>Journal of statistical software</em>, 80(1):1–28, 2017.</p>
</dd>
<dt class="label" id="id79"><span class="brackets">38</span></dt>
<dd><p>C. Davidson-Pilon. <em>Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Inference</em>. Addison-Wesley Data &amp; Analytics Series. Pearson Education, 2015. ISBN 9780133902921.</p>
</dd>
<dt class="label" id="id172"><span class="brackets">39</span></dt>
<dd><p>Brian Greenhill, Michael D Ward, and Audrey Sacks. The separation plot: a new visual method for evaluating the fit of binary models. <em>American Journal of Political Science</em>, 55(4):991–1002, 2011.</p>
</dd>
<dt class="label" id="id62"><span class="brackets">40</span></dt>
<dd><p>A. Gelman, J. Hill, and A. Vehtari. <em>Regression and Other Stories</em>. Analytical Methods for Social Research. Cambridge University Press, 2020. ISBN 9781107023987.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">41</span></dt>
<dd><p>O. Martin. <em>Bayesian Analysis with Python: Introduction to Statistical Modeling and Probabilistic Programming Using PyMC3 and ArviZ, 2nd Edition</em>. Packt Publishing, 2018. ISBN 9781789341652.</p>
</dd>
<dt class="label" id="id119"><span class="brackets">42</span></dt>
<dd><p>Frank E Grubbs. Procedures for detecting outlying observations in samples. <em>Technometrics</em>, 11(1):1–21, 1969.</p>
</dd>
<dt class="label" id="id135"><span class="brackets">43</span></dt>
<dd><p>Michael Betancourt. Towards a principled bayesian workflow. <span><a class="reference external" href="#"></a></span>https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html, 4 2020.</p>
</dd>
<dt class="label" id="id80"><span class="brackets">44</span></dt>
<dd><p>Andrew Gelman. Analysis of variance—why it is more important than ever. <em>The annals of statistics</em>, 33(1):1–53, 2005.</p>
</dd>
<dt class="label" id="id86"><span class="brackets">45</span></dt>
<dd><p>Radford M Neal. Slice sampling. <em>The annals of statistics</em>, 31(3):705–767, 2003.</p>
</dd>
<dt class="label" id="id173"><span class="brackets">46</span></dt>
<dd><p>Omiros Papaspiliopoulos, Gareth O Roberts, and Martin Sköld. A general framework for the parametrization of hierarchical models. <em>Statistical Science</em>, pages 59–73, 2007.</p>
</dd>
<dt class="label" id="id164"><span class="brackets">47</span></dt>
<dd><p>Michael Betancourt. Hierarchical modeling. <span><a class="reference external" href="#"></a></span>https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html, 11 2020.</p>
</dd>
<dt class="label" id="id122"><span class="brackets">48</span></dt>
<dd><p>Nathan P Lemoine. Moving beyond noninformative priors: why and how to choose weakly informative priors in bayesian analyses. <em>Oikos</em>, 128(7):912–928, 2019.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">49</span></dt>
<dd><p>S.N. Wood. <em>Generalized Additive Models: An Introduction with R, Second Edition</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2017. ISBN 9781498728379.</p>
</dd>
<dt class="label" id="id120"><span class="brackets">50</span></dt>
<dd><p>Catherine Potvin, Martin J Lechowicz, and Serge Tardif. The statistical analysis of ecophysiological response curves obtained from experiments involving repeated measures. <em>Ecology</em>, 71(4):1389–1400, 1990.</p>
</dd>
<dt class="label" id="id121"><span class="brackets">51</span></dt>
<dd><p>Eric J Pedersen, David L Miller, Gavin L Simpson, and Noam Ross. Hierarchical generalized additive models in ecology: an introduction with mgcv. <em>PeerJ</em>, 7:e6876, 2019.</p>
</dd>
<dt class="label" id="id91"><span class="brackets">52</span></dt>
<dd><p>Carl Edward Rasmussen and Christopher K. I. Williams. <em>Gaussian Processes for Machine Learning</em>. The MIT Press, Cambridge, Mass, 2005. ISBN 978-0-262-18253-9.</p>
</dd>
<dt class="label" id="id125"><span class="brackets">53</span></dt>
<dd><p>Sean J Taylor and Benjamin Letham. Forecasting at scale. <em>The American Statistician</em>, 72(1):37–45, 2018.</p>
</dd>
<dt class="label" id="id126"><span class="brackets">54</span></dt>
<dd><p>Ryan Prescott Adams and David JC MacKay. Bayesian online changepoint detection. <em>arXiv preprint arXiv:0710.3742</em>, 2007.</p>
</dd>
<dt class="label" id="id160"><span class="brackets">55</span></dt>
<dd><p>G. Strang. <em>Introduction to Linear Algebra</em>. Wellesley-Cambridge Press, 2009. ISBN 9780980232714.</p>
</dd>
<dt class="label" id="id127"><span class="brackets">56</span></dt>
<dd><p>Andrew C Harvey and Neil Shephard. Structural time series models. <em>Handbook of Statistics,(edited by GS Maddala, CR Rao and HD Vinod)</em>, 11:261–302, 1993.</p>
</dd>
<dt class="label" id="id130"><span class="brackets">57</span></dt>
<dd><p>G.E.P. Box, G.M. Jenkins, and G.C. Reinsel. <em>Time Series Analysis: Forecasting and Control</em>. Wiley Series in Probability and Statistics. Wiley, 2008. ISBN 9780470272848.</p>
</dd>
<dt class="label" id="id132"><span class="brackets">58</span></dt>
<dd><p>R. Shumway and D. Stoffer. <em>Time Series: A Data Analysis Approach Using R</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press, 2019. ISBN 9781000001563.</p>
</dd>
<dt class="label" id="id134"><span class="brackets">59</span></dt>
<dd><p>M. West and J. Harrison. <em>Bayesian Forecasting and Dynamic Models</em>. Springer Series in Statistics. Springer New York, 2013. ISBN 9781475793659.</p>
</dd>
<dt class="label" id="id129"><span class="brackets">60</span></dt>
<dd><p>S. Särkkä. <em>Bayesian Filtering and Smoothing</em>. Bayesian Filtering and Smoothing. Cambridge University Press, 2013. ISBN 9781107030657.</p>
</dd>
<dt class="label" id="id131"><span class="brackets">61</span></dt>
<dd><p>James Durbin and Siem Jan Koopman. <em>Time series analysis by state space methods</em>. Oxford university press, 2012.</p>
</dd>
<dt class="label" id="id161"><span class="brackets">62</span></dt>
<dd><p>Mohinder S Grewal and Angus P Andrews. <em>Kalman filtering: Theory and Practice with MATLAB</em>. John Wiley &amp; Sons, 2014.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">63</span></dt>
<dd><p>N. Chopin and O. Papaspiliopoulos. <em>An Introduction to Sequential Monte Carlo</em>. Springer Series in Statistics. Springer International Publishing, 2020. ISBN 9783030478445.</p>
</dd>
<dt class="label" id="id123"><span class="brackets">64</span></dt>
<dd><p>Paul-Christian Bürkner, Jonah Gabry, and Aki Vehtari. Approximate leave-future-out cross-validation for bayesian time series models. <em>Journal of Statistical Computation and Simulation</em>, 90(14):2499–2523, 2020.</p>
</dd>
<dt class="label" id="id166"><span class="brackets">65</span></dt>
<dd><p>Carlos M. Carvalho, Nicholas G. Polson, and James G. Scott. The horseshoe estimator for sparse signals. <em>Biometrika</em>, 97(2):465–480, 2010.</p>
</dd>
<dt class="label" id="id168"><span class="brackets">66</span></dt>
<dd><p>Juho Piironen, Aki Vehtari, and others. Sparsity information and regularization in the horseshoe and other shrinkage priors. <em>Electronic Journal of Statistics</em>, 11(2):5018–5051, 2017.</p>
</dd>
<dt class="label" id="id167"><span class="brackets">67</span></dt>
<dd><p>Juho Piironen and Aki Vehtari. On the hyperprior choice for the global shrinkage parameter in the horseshoe prior. In <em>Artificial Intelligence and Statistics</em>, 905–913. PMLR, 2017.</p>
</dd>
<dt class="label" id="id169"><span class="brackets">68</span></dt>
<dd><p>Gabriel Riutort-Mayol, Paul-Christian Bürkner, Michael R Andersen, Arno Solin, and Aki Vehtari. Practical hilbert space approximate bayesian gaussian processes for probabilistic programming. <em>arXiv preprint arXiv:2004.11408</em>, 2020.</p>
</dd>
<dt class="label" id="id148"><span class="brackets">69</span></dt>
<dd><p>Leo Breiman. Statistical modeling: the two cultures (with comments and a rejoinder by the author). <em>Statistical science</em>, 16(3):199–231, 2001.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">70</span></dt>
<dd><p>Z.H. Zhou. <em>Ensemble Methods: Foundations and Algorithms</em>. Chapman &amp; Hall/CRC data mining and knowledge discovery series. CRC Press, 2012. ISBN 9781439830055.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">71</span></dt>
<dd><p>Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. Bart: bayesian additive regression trees. <em>The Annals of Applied Statistics</em>, 4(1):266–298, 2010.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">72</span></dt>
<dd><p>Veronika Ročková and Enakshi Saha. On theory for bart. In <em>The 22nd International Conference on Artificial Intelligence and Statistics</em>, 2839–2848. PMLR, 2019.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">73</span></dt>
<dd><p>Balaji Lakshminarayanan, Daniel Roy, and Yee Whye Teh. Particle gibbs for bayesian additive regression trees. In <em>Artificial Intelligence and Statistics</em>, 553–561. PMLR, 2015.</p>
</dd>
<dt class="label" id="id75"><span class="brackets">74</span></dt>
<dd><p>C. Molnar. <em>Interpretable Machine Learning</em>. Lulu.com, 2020. ISBN 9780244768522.</p>
</dd>
<dt class="label" id="id74"><span class="brackets">75</span></dt>
<dd><p>Christoph Molnar, Giuseppe Casalicchio, and Bernd Bischl. Interpretable machine learning–a brief history, state-of-the-art and challenges. In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 417–431. Springer, 2020.</p>
</dd>
<dt class="label" id="id72"><span class="brackets">76</span></dt>
<dd><p>Jerome H Friedman. Greedy function approximation: a gradient boosting machine. <em>Annals of statistics</em>, pages 1189–1232, 2001.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">77</span></dt>
<dd><p>Alex Goldstein, Adam Kapelner, Justin Bleich, and Emil Pitkin. Peeking inside the black box: visualizing statistical learning with plots of individual conditional expectation. <em>journal of Computational and Graphical Statistics</em>, 24(1):44–65, 2015.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">78</span></dt>
<dd><p>Yi Liu, Veronika Ročková, and Yuexi Wang. Variable selection with abc bayesian forests. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 2019.</p>
</dd>
<dt class="label" id="id71"><span class="brackets">79</span></dt>
<dd><p>Colin J. Carlson. Embarcadero: species distribution modelling with bayesian additive regression trees in r. <em>Methods in Ecology and Evolution</em>, 11(7):850–858, 2020.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">80</span></dt>
<dd><p>Justin Bleich, Adam Kapelner, Edward I George, and Shane T Jensen. Variable selection for bart: an application to gene regulation. <em>The Annals of Applied Statistics</em>, pages 1750–1781, 2014.</p>
</dd>
<dt class="label" id="id149"><span class="brackets">81</span></dt>
<dd><p>Leo Breiman. Random forests. <em>Machine learning</em>, 45(1):5–32, 2001.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">82</span></dt>
<dd><p>Matej Balog and Yee Whye Teh. The mondrian process for machine learning. <em>arXiv preprint arXiv:1507.05181</em>, 2015.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">83</span></dt>
<dd><p>Daniel M Roy and Yee Whye Teh. The mondrian process. In <em>Proceedings of the 21st International Conference on Neural Information Processing Systems</em>, 1377–1384. 2008.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">84</span></dt>
<dd><p>Mikael Sunnåker, Alberto Giovanni Busetto, Elina Numminen, Jukka Corander, Matthieu Foll, and Christophe Dessimoz. Approximate bayesian computation. <em>PLoS computational biology</em>, 9(1):e1002803, 2013.</p>
</dd>
<dt class="label" id="id56"><span class="brackets">85</span></dt>
<dd><p>Ritabrata Dutta, Marcel Schoengens, Jukka-Pekka Onnela, and Antonietta Mira. Abcpy: a user-friendly, extensible, and parallel library for approximate bayesian computation. In <em>Proceedings of the platform for advanced scientific computing conference</em>, 1–9. 2017.</p>
</dd>
<dt class="label" id="id55"><span class="brackets">86</span></dt>
<dd><p>Jarno Lintusaari, Henri Vuollekoski, Antti Kangasraasio, Kusti Skytén, Marko Jarvenpaa, Pekka Marttinen, Michael U Gutmann, Aki Vehtari, Jukka Corander, and Samuel Kaski. Elfi: engine for likelihood-free inference. <em>Journal of Machine Learning Research</em>, 19(16):1–7, 2018.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">87</span></dt>
<dd><p>Emmanuel Klinger, Dennis Rickert, and Jan Hasenauer. Pyabc: distributed, likelihood-free inference. <em>Bioinformatics</em>, 34(20):3591–3593, 2018.</p>
</dd>
<dt class="label" id="id153"><span class="brackets">88</span></dt>
<dd><p>Georges Darmois. Sur les lois de probabilitéa estimation exhaustive. <em>CR Acad. Sci. Paris</em>, 260(1265):85, 1935.</p>
</dd>
<dt class="label" id="id152"><span class="brackets">89</span></dt>
<dd><p>Bernard Osgood Koopman. On distributions admitting a sufficient statistic. <em>Transactions of the American Mathematical society</em>, 39(3):399–409, 1936.</p>
</dd>
<dt class="label" id="id151"><span class="brackets">90</span></dt>
<dd><p>Edwin James George Pitman. Sufficient statistics and intrinsic accuracy. In <em>Mathematical Proceedings of the cambridge Philosophical society</em>, volume 32, 567–579. Cambridge University Press, 1936.</p>
</dd>
<dt class="label" id="id150"><span class="brackets">91</span></dt>
<dd><p>Erling Bernhard Andersen. Sufficiency and exponential families for discrete sample spaces. <em>Journal of the American Statistical Association</em>, 65(331):1248–1255, 1970.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">92</span></dt>
<dd><p>Fernando Pérez-Cruz. Kullback-leibler divergence estimation of continuous distributions. In <em>2008 IEEE international symposium on information theory</em>, 1666–1670. IEEE, 2008.</p>
</dd>
<dt class="label" id="id47"><span class="brackets">93</span></dt>
<dd><p>Bai Jiang. Approximate bayesian computation with kullback-leibler divergence as data discrepancy. In <em>International conference on artificial intelligence and statistics</em>, 1711–1721. PMLR, 2018.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">94</span></dt>
<dd><p>Espen Bernton, Pierre E Jacob, Mathieu Gerber, and Christian P Robert. Approximate bayesian computation with the wasserstein distance. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 81(2):235–269, 2019.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">95</span></dt>
<dd><p>Jon Louis Bentley. Multidimensional binary search trees used for associative searching. <em>Communications of the ACM</em>, 18(9):509–517, 1975.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">96</span></dt>
<dd><p>S.A. Sisson, Y. Fan, and M. Beaumont. <em>Handbook of Approximate Bayesian Computation</em>. Chapman &amp; Hall/CRC Handbooks of Modern Statistical Methods. CRC Press, 2018. ISBN 9781439881514.</p>
</dd>
<dt class="label" id="id53"><span class="brackets">97</span></dt>
<dd><p>Mark A. Beaumont, Wenyang Zhang, and David J Balding. Approximate bayesian computation in population genetics. <em>Genetics</em>, 162(4):2025–2035, 2002.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">98</span></dt>
<dd><p>Mark A. Beaumont. Approximate bayesian computation in evolution and ecology. <em>Annual review of ecology, evolution, and systematics</em>, 41:379–406, 2010.</p>
</dd>
<dt class="label" id="id51"><span class="brackets">99</span></dt>
<dd><p>Pierre Pudlo, Jean-Michel Marin, Arnaud Estoup, Jean-Marie Cornuet, Mathieu Gautier, and Christian P Robert. Reliable abc model choice via random forests. <em>Bioinformatics</em>, 32(6):859–866, 2016.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">100</span></dt>
<dd><p>John W. Tukey. Modern Techniques in Data Analysis. In <em>proceesings of the Sponsored Regional Research Conference</em>. 1977.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">101</span></dt>
<dd><p>Glen D Rayner and Helen L MacGillivray. Numerical maximum likelihood estimation for the g-and-k and generalized g-and-h distributions. <em>Statistics and Computing</em>, 12(1):57–75, 2002.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">102</span></dt>
<dd><p>Dennis Prangle. Gk: an r package for the g-and-k and generalised g-and-h distributions. <em>arXiv preprint arXiv:1706.06889</em>, 2017.</p>
</dd>
<dt class="label" id="id42"><span class="brackets">103</span></dt>
<dd><p>Christopher C Drovandi and Anthony N Pettitt. Likelihood-free bayesian estimation of multivariate quantile distributions. <em>Computational Statistics &amp; Data Analysis</em>, 55(9):2541–2556, 2011.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">104</span></dt>
<dd><p>A.L. Bowley. <em>Elements of Statistics</em>. Number v. 2 in Elements of Statistics. P.S. King, 1920.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">105</span></dt>
<dd><p>JJA Moors. A quantile alternative for kurtosis. <em>Journal of the Royal Statistical Society: Series D (The Statistician)</em>, 37(1):25–32, 1988.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">106</span></dt>
<dd><p>Jean-Michel Marin, Pierre Pudlo, Christian P Robert, and Robin J Ryder. Approximate bayesian computational methods. <em>Statistics and Computing</em>, 22(6):1167–1180, 2012.</p>
</dd>
<dt class="label" id="id52"><span class="brackets">107</span></dt>
<dd><p>Mark A. Beaumont. Approximate bayesian computation. <em>Annual review of statistics and its application</em>, 6:379–403, 2019.</p>
</dd>
<dt class="label" id="id61"><span class="brackets">108</span></dt>
<dd><p>Christian P Robert, Jean-Marie Cornuet, Jean-Michel Marin, and Natesh S Pillai. Lack of confidence in approximate bayesian computation model choice. <em>Proceedings of the National Academy of Sciences</em>, 108(37):15112–15117, 2011.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">109</span></dt>
<dd><p>François-David Collin, Arnaud Estoup, Jean-Michel Marin, and Louis Raynal. Bringing abc inference to the machine learning realm: abcranger, an optimized random forests library for abc. In <em>JOBIM 2020</em>, volume 2020. 2020.</p>
</dd>
<dt class="label" id="id143"><span class="brackets">110</span></dt>
<dd><p>Peter Bickel, Bo Li, Thomas Bengtsson, and others. Sharp failure rates for the bootstrap particle filter in high dimensions. In <em>Pushing the limits of contemporary statistics: Contributions in honor of Jayanta K. Ghosh</em>, pages 318–329. Institute of Mathematical Statistics, 2008.</p>
</dd>
<dt class="label" id="id174"><span class="brackets">111</span></dt>
<dd><p>S.P. Otto and T. Day. <em>A Biologist's Guide to Mathematical Modeling in Ecology and Evolution</em>. Princeton University Press, 2011. ISBN 9781400840915.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">112</span></dt>
<dd><p>D. R. Cox. <em>Principles of statistical inference</em>. Cambridge University Press, 2006. ISBN 978-0521685672.</p>
</dd>
<dt class="label" id="id175"><span class="brackets">113</span></dt>
<dd><p>Andrew Gelman. The folk theorem of statistical computing. <span><a class="reference external" href="#"></a></span>https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/, 5 2008.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">114</span></dt>
<dd><p>John K Kruschke. Bayesian estimation supersedes the t test. <em>Journal of Experimental Psychology: General</em>, 142(2):573, 2013.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">115</span></dt>
<dd><p>Matthew PA. Clark and Brian D. Westerberg. How random is the toss of a coin? <em>Cmaj</em>, 181(12):E306–E308, 2009.</p>
</dd>
<dt class="label" id="id155"><span class="brackets">116</span></dt>
<dd><p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs. 2018. URL: <a class="reference external" href="http://github.com/google/jax">http://github.com/google/jax</a>.</p>
</dd>
<dt class="label" id="id113"><span class="brackets">117</span></dt>
<dd><p>Wally R Gilks, Andrew Thomas, and David J Spiegelhalter. A language and program for complex bayesian modelling. <em>Journal of the Royal Statistical Society: Series D (The Statistician)</em>, 43(1):169–177, 1994.</p>
</dd>
<dt class="label" id="id114"><span class="brackets">118</span></dt>
<dd><p>Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: a probabilistic programming language. <em>Journal of statistical software</em>, 76(1):1–32, 2017.</p>
</dd>
<dt class="label" id="id87"><span class="brackets">119</span></dt>
<dd><p>Maria I Gorinova, Andrew D Gordon, and Charles Sutton. Probabilistic programming with densities in slicstan: efficient, flexible, and deterministic. <em>Proceedings of the ACM on Programming Languages</em>, 3(POPL):1–30, 2019.</p>
</dd>
<dt class="label" id="id165"><span class="brackets">120</span></dt>
<dd><p>Max Kochurov, Colin Carroll, Thomas Wiecki, and Junpeng Lao. Pymc4: exploiting coroutines for implementing a probabilistic programming framework. Program Transformations for ML Workshop at NeurIPS, 2019. URL: <a class="reference external" href="https://openreview.net/forum?id=rkgzj5Za8H">https://openreview.net/forum?id=rkgzj5Za8H</a>.</p>
</dd>
<dt class="label" id="id142"><span class="brackets">121</span></dt>
<dd><p>George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. <em>arXiv preprint arXiv:1912.02762</em>, 2019.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">122</span></dt>
<dd><p>Brandon T Willard. Minikanren as a tool for symbolic computation in python. <em>arXiv preprint arXiv:2005.11644</em>, 2020.</p>
</dd>
<dt class="label" id="id136"><span class="brackets">123</span></dt>
<dd><p>Ohad Kammar, Sam Lindley, and Nicolas Oury. Handlers in action. <em>ACM SIGPLAN Notices</em>, 48(9):145–158, 2013.</p>
</dd>
<dt class="label" id="id144"><span class="brackets">124</span></dt>
<dd><p>Frank Wood, Jan Willem Meent, and Vikash Mansinghka. A new approach to probabilistic programming inference. In <em>Artificial Intelligence and Statistics</em>, 1024–1032. PMLR, 2014.</p>
</dd>
<dt class="label" id="id146"><span class="brackets">125</span></dt>
<dd><p>David Tolpin, Jan-Willem van de Meent, Hongseok Yang, and Frank Wood. Design and implementation of probabilistic programming language anglican. In <em>Proceedings of the 28th Symposium on the Implementation and Application of Functional programming Languages</em>, 1–12. 2016.</p>
</dd>
<dt class="label" id="id137"><span class="brackets">126</span></dt>
<dd><p>Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman. Pyro: deep universal probabilistic programming. <em>The Journal of Machine Learning Research</em>, 20(1):973–978, 2019.</p>
</dd>
<dt class="label" id="id138"><span class="brackets">127</span></dt>
<dd><p>Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flexible and accelerated probabilistic programming in numpyro. <em>arXiv preprint arXiv:1912.11554</em>, 2019.</p>
</dd>
<dt class="label" id="id139"><span class="brackets">128</span></dt>
<dd><p>Dustin Tran, Matthew Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, Alexey Radul, Matthew Johnson, and Rif A Saurous. Simple, distributed, and accelerated probabilistic programming. <em>arXiv preprint arXiv:1811.02091</em>, 2018.</p>
</dd>
<dt class="label" id="id140"><span class="brackets">129</span></dt>
<dd><p>Dave Moore and Maria I Gorinova. Effect handling for composable program transformations in edward2. <em>arXiv preprint arXiv:1811.06150</em>, 2018.</p>
</dd>
<dt class="label" id="id176"><span class="brackets">130</span></dt>
<dd><p>Maria Gorinova, Dave Moore, and Matthew Hoffman. Automatic reparameterisation of probabilistic programs. In <em>International Conference on Machine Learning</em>, 3648–3657. PMLR, 2020.</p>
</dd>
<dt class="label" id="id145"><span class="brackets">131</span></dt>
<dd><p>Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to probabilistic programming. <em>arXiv preprint arXiv:1809.10756</em>, 2018.</p>
</dd>
<dt class="label" id="id92"><span class="brackets">132</span></dt>
<dd><p>Allen B. Downey. <em>Think Stats: Exploratory Data Analysis</em>. O'Reilly Media;, 2014.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">133</span></dt>
<dd><p>Peter H Westfall. Kurtosis as peakedness, 1905–2014. rip. <em>The American Statistician</em>, 68(3):191–195, 2014.</p>
</dd>
<dt class="label" id="id17"><span class="brackets">134</span></dt>
<dd><p>T.M. Cover and J.A. Thomas. <em>Elements of Information Theory</em>. Wiley, 2012. ISBN 9781118585771.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">135</span></dt>
<dd><p>Hirotogu Akaike. Information theory and an extension of the maximum likelihood principle. In <em>Selected papers of hirotugu akaike</em>, pages 199–213. Springer, 1998.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">136</span></dt>
<dd><p>Sumio Watanabe and Manfred Opper. Asymptotic equivalence of bayes cross validation and widely applicable information criterion in singular learning theory. <em>Journal of machine learning research</em>, 2010.</p>
</dd>
<dt class="label" id="id21"><span class="brackets">137</span></dt>
<dd><p>Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leave-one-out cross-validation and waic. <em>Statistics and computing</em>, 27(5):1413–1432, 2017.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">138</span></dt>
<dd><p>W.R. Gilks, S. Richardson, and D. Spiegelhalter. <em>Markov Chain Monte Carlo in Practice</em>. Chapman &amp; Hall/CRC Interdisciplinary Statistics. CRC Press, 1995. ISBN 9781482214970.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">139</span></dt>
<dd><p>Nial Friel and Jason Wyse. Estimating the evidence–a review. <em>Statistica Neerlandica</em>, 66(3):288–308, 2012.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">140</span></dt>
<dd><p>Radford M Neal. Contribution to the discussion of “approximate bayesian inference with the weighted likelihood bootstrap” by michael a. newton and adrian e. raftery. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, 56:41–42, 1994.</p>
</dd>
<dt class="label" id="id156"><span class="brackets">141</span></dt>
<dd><p>Quentin F Gronau, Alexandra Sarafoglou, Dora Matzke, Alexander Ly, Udo Boehm, Maarten Marsman, David S Leslie, Jonathan J Forster, Eric-Jan Wagenmakers, and Helen Steingroever. A tutorial on bridge sampling. <em>Journal of mathematical psychology</em>, 81:80–97, 2017.</p>
</dd>
<dt class="label" id="id158"><span class="brackets">142</span></dt>
<dd><p>Danielle Navarro. A personal essay on bayes factors. <em>PsyArXiv</em>, 2020.</p>
</dd>
<dt class="label" id="id157"><span class="brackets">143</span></dt>
<dd><p>Daniel J Schad, Bruno Nicenboim, Paul-Christian Bürkner, Michael Betancourt, and Shravan Vasishth. Workflow techniques for the robust use of bayes factors. <em>arXiv preprint arXiv:2103.08744</em>, 2021.</p>
</dd>
<dt class="label" id="id154"><span class="brackets">144</span></dt>
<dd><p>E.A. Abbott and R. Jann. <em>Flatland: A Romance of Many Dimensions</em>. Oxford World's Classics. OUP Oxford, 2008. ISBN 9780199537501.</p>
</dd>
<dt class="label" id="id116"><span class="brackets">145</span></dt>
<dd><p>Gael M Martin, David T Frazier, and Christian P Robert. Computing bayes: bayesian computation from 1763 to the 21st century. <em>arXiv preprint arXiv:2004.06425</em>, 2020.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">146</span></dt>
<dd><p>Heikki Haario, Eero Saksman, and Johanna Tamminen. An adaptive metropolis algorithm. <em>Bernoulli</em>, pages 223–242, 2001.</p>
</dd>
<dt class="label" id="id104"><span class="brackets">147</span></dt>
<dd><p>Christophe Andrieu and Johannes Thoms. A tutorial on adaptive mcmc. <em>Statistics and computing</em>, 18(4):343–373, 2008.</p>
</dd>
<dt class="label" id="id103"><span class="brackets">148</span></dt>
<dd><p>Gareth O Roberts and Jeffrey S Rosenthal. Examples of adaptive mcmc. <em>Journal of computational and graphical statistics</em>, 18(2):349–367, 2009.</p>
</dd>
<dt class="label" id="id102"><span class="brackets">149</span></dt>
<dd><p>Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, and Arthur Gretton. Kernel adaptive metropolis-hastings. In <em>International conference on machine learning</em>, 1665–1673. PMLR, 2014.</p>
</dd>
<dt class="label" id="id105"><span class="brackets">150</span></dt>
<dd><p>Andrew Gelman, Walter R Gilks, and Gareth O Roberts. Weak convergence and optimal scaling of random walk metropolis algorithms. <em>The annals of applied probability</em>, 7(1):110–120, 1997.</p>
</dd>
<dt class="label" id="id109"><span class="brackets">151</span></dt>
<dd><p>Gareth O Roberts and Jeffrey S Rosenthal. Optimal scaling for various metropolis-hastings algorithms. <em>Statistical science</em>, 16(4):351–367, 2001.</p>
</dd>
<dt class="label" id="id110"><span class="brackets">152</span></dt>
<dd><p>Mylene Bedard. Optimal acceptance rates for metropolis algorithms: moving beyond 0.234. <em>Stochastic Processes and their Applications</em>, 118(12):2198–2222, 2008.</p>
</dd>
<dt class="label" id="id111"><span class="brackets">153</span></dt>
<dd><p>Chris Sherlock. Optimal scaling of the random walk metropolis: general criteria for the 0.234 acceptance rule. <em>Journal of Applied Probability</em>, 50(1):1–15, 2013.</p>
</dd>
<dt class="label" id="id112"><span class="brackets">154</span></dt>
<dd><p>Christopher CJ Potter and Robert H Swendsen. 0.234: the myth of a universal acceptance ratio for monte carlo simulations. <em>Physics Procedia</em>, 68:120–124, 2015.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">155</span></dt>
<dd><p>Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. <em>Physics letters B</em>, 195(2):216–222, 1987.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">156</span></dt>
<dd><p>S. Brooks, A. Gelman, G. Jones, and X.L. Meng. <em>Handbook of Markov Chain Monte Carlo</em>. Chapman &amp; Hall/CRC Handbooks of Modern Statistical Methods. CRC Press, 2011. ISBN 9781420079425.</p>
</dd>
<dt class="label" id="id97"><span class="brackets">157</span></dt>
<dd><p>Michael Betancourt. A conceptual introduction to hamiltonian monte carlo. <em>arXiv preprint arXiv:1701.02434</em>, 2017.</p>
</dd>
<dt class="label" id="id98"><span class="brackets">158</span></dt>
<dd><p>Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. <em>Journal of Machine Learning Research</em>, 15(47):1593–1623, 2014.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">159</span></dt>
<dd><p>Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential monte carlo samplers. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 68(3):411–436, 2006.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">160</span></dt>
<dd><p>Jianye Ching and Yi-Chu Chen. Transitional markov chain monte carlo method for bayesian model updating, model class selection, and model averaging. <em>Journal of engineering mechanics</em>, 133(7):816–832, 2007.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">161</span></dt>
<dd><p>Christian A Naesseth, Fredrik Lindsten, and Thomas B Schön. Elements of sequential monte carlo. <em>arXiv preprint arXiv:1903.04797</em>, 2019.</p>
</dd>
<dt class="label" id="id162"><span class="brackets">162</span></dt>
<dd><p>Yuling Yao, Aki Vehtari, Daniel Simpson, and Andrew Gelman. Yes, but did it work?: evaluating variational inference. In <em>International Conference on Machine Learning</em>, 5581–5590. PMLR, 2018.</p>
</dd>
<dt class="label" id="id159"><span class="brackets">163</span></dt>
<dd><p>David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: a review for statisticians. <em>Journal of the American statistical Association</em>, 112(518):859–877, 2017.</p>
</dd>
<dt class="label" id="id163"><span class="brackets">164</span></dt>
<dd><p>Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. Automatic differentiation variational inference. <em>The Journal of Machine Learning Research</em>, 18(1):430–474, 2017.</p>
</dd>
<dt class="label" id="id93"><span class="brackets">165</span></dt>
<dd><p>J.K. Kruschke. <em>Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan</em>. Academic Press. Academic Press, 2015. ISBN 9780124058880.</p>
</dd>
</dl>
</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./markdown"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="glossary.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">12. </span>Glossary</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../notebooks/chp_01.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Code 1: Bayesian Inference</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Martin, Kumar, Lao<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>